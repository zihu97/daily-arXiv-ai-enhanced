{"id": "2512.09300", "categories": ["cs.OS", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09300", "abs": "https://arxiv.org/abs/2512.09300", "authors": ["Guangxian Zou", "Isaac Zhang", "Ryan Zarick", "Kelvin Wong", "Thomas Kim", "Daniel L. -K. Wong", "Saeid Yazdinejad", "Dan Boneh"], "title": "ZeroOS: A Universal Modular Library OS for zkVMs", "comment": null, "summary": "zkVMs promise general-purpose verifiable computation through ISA-level compatibility with modern programs and toolchains. However, compatibility extends further than just the ISA; modern programs often cannot run or even compile without an operating system and libc. zkVMs attempt to address this by maintaining forks of language-specific runtimes and statically linking them into applications to create self-contained unikernels, but this ad-hoc approach leads to version hell and burdens verifiable applications (vApps) with an unnecessarily large trusted computing base. We solve this problem with ZeroOS, a modular library operating system (libOS) for vApp unikernels; vApp developers can use off-the-shelf toolchains to compile and link only the exact subset of the Linux ABI their vApp needs. Any zkVM team can easily leverage the ZeroOS ecosystem by writing a ZeroOS bootloader for their platform, resulting in a reduced maintainence burden and unifying the entire zkVM ecosystem with consolidated development and audit resources. ZeroOS is free and open-sourced at https://github.com/LayerZero-Labs/ZeroOS.", "AI": {"tldr": "ZeroOS \u662f\u4e00\u4e2a\u4e3a\u53ef\u9a8c\u8bc1\u5e94\u7528\uff08vApp\uff09\u8bbe\u8ba1\u7684\u6a21\u5757\u5316\u5e93\u64cd\u4f5c\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3 zkVM \u4e2d\u56e0\u9759\u6001\u94fe\u63a5\u8fd0\u884c\u65f6\u5bfc\u81f4\u7684\u7248\u672c\u6df7\u4e71\u548c\u53ef\u4fe1\u8ba1\u7b97\u57fa\u8fc7\u5927\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u7a0b\u5e8f\u4f9d\u8d56\u64cd\u4f5c\u7cfb\u7edf\u548c libc\uff0c\u800c zkVM \u901a\u8fc7\u7ef4\u62a4\u8bed\u8a00\u7279\u5b9a\u8fd0\u884c\u65f6\u7684\u5206\u652f\u5e76\u9759\u6001\u94fe\u63a5\uff0c\u5bfc\u81f4\u7248\u672c\u7ba1\u7406\u56f0\u96be\u548c\u53ef\u4fe1\u8ba1\u7b97\u57fa\u81a8\u80c0\u3002", "method": "\u5f00\u53d1 ZeroOS\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u4ec5\u94fe\u63a5\u5176 vApp \u6240\u9700\u7684 Linux ABI \u5b50\u96c6\uff0c\u5e76\u901a\u8fc7\u7f16\u5199\u5f15\u5bfc\u52a0\u8f7d\u7a0b\u5e8f\u4f7f zkVM \u56e2\u961f\u8f7b\u677e\u63a5\u5165 ZeroOS \u751f\u6001\u3002", "result": "\u964d\u4f4e zkVM \u7ef4\u62a4\u8d1f\u62c5\uff0c\u7edf\u4e00 zkVM \u751f\u6001\u7cfb\u7edf\uff0c\u96c6\u4e2d\u5f00\u53d1\u4e0e\u5ba1\u8ba1\u8d44\u6e90\u3002", "conclusion": "ZeroOS \u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u8f7b\u91cf\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8 zkVM \u751f\u6001\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.09312", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.09312", "abs": "https://arxiv.org/abs/2512.09312", "authors": ["Ziheng Yang", "Kun Qiu", "Zhe Chen", "Wenjun Zhu", "Yue Gao"], "title": "Tyche: A Hybrid Computation Framework of Illumination Pattern for Satellite Beam Hopping", "comment": null, "summary": "High-Throughput Satellites (HTS) use beam hopping to handle non-uniform and time-varying ground traffic demand. A significant technical challenge in beam hopping is the computation of effective illumination patterns. Traditional algorithms, like the genetic algorithm, require over 300 seconds to compute a single illumination pattern for just 37 cells, whereas modern HTS typically covers over 300 cells, rendering current methods impractical for real-world applications. Advanced approaches, such as multi-agent deep reinforcement learning, face convergence issues when the number of cells exceeds 40. In this paper, we introduce Tyche, a hybrid computation framework designed to address this challenge. Tyche incorporates a Monte Carlo Tree Search Beam Hopping (MCTS-BH) algorithm for computing illumination patterns and employs sliding window and pruning techniques to significantly reduce computation time. Specifically, MCTS-BH can compute one illumination pattern for 37 cells in just 12 seconds. To ensure real-time computation, we use a Greedy Beam Hopping (G-BH) algorithm, which provides a provisional solution while MCTS-BH completes its computation in the background. Our evaluation results show that MCTS-BH can increase throughput by up to 98.76%, demonstrating substantial improvements over existing solutions.", "AI": {"tldr": "Tyche\u6846\u67b6\u7ed3\u5408MCTS-BH\u4e0eG-BH\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u901a\u91cf\u536b\u661f\u6ce2\u675f\u8df3\u53d8\u7684\u8ba1\u7b97\u6548\u7387\u4e0e\u541e\u5410\u91cf\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6ce2\u675f\u7167\u660e\u6a21\u5f0f\u8017\u65f6\u8fc7\u957f\uff0c\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u9ad8\u901a\u91cf\u536b\u661f\u8986\u76d6\u6570\u767e\u5c0f\u533a\u7684\u5b9e\u65f6\u9700\u6c42\u3002", "method": "\u63d0\u51faTyche\u6df7\u5408\u6846\u67b6\uff0c\u91c7\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6ce2\u675f\u8df3\u53d8\u7b97\u6cd5\uff08MCTS-BH\uff09\u5e76\u8f85\u4ee5\u6ed1\u52a8\u7a97\u53e3\u4e0e\u526a\u679d\u6280\u672f\uff0c\u540c\u65f6\u7528\u8d2a\u5fc3\u7b97\u6cd5\uff08G-BH\uff09\u63d0\u4f9b\u5b9e\u65f6\u4e34\u65f6\u89e3\u3002", "result": "MCTS-BH\u572837\u5c0f\u533a\u573a\u666f\u4e0b\u4ec5\u970012\u79d2\u5b8c\u6210\u8ba1\u7b97\uff0c\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u534798.76%\u3002", "conclusion": "Tyche\u6709\u6548\u89e3\u51b3\u6ce2\u675f\u8df3\u53d8\u8ba1\u7b97\u6548\u7387\u74f6\u9888\uff0c\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2512.09304", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.09304", "abs": "https://arxiv.org/abs/2512.09304", "authors": ["Siyuan Ma", "Jiajun Hu", "Jeeho Ryoo", "Aman Arora", "Lizy Kurian John"], "title": "RACAM: Enhancing DRAM with Reuse-Aware Computation and Automated Mapping for ML Inference", "comment": null, "summary": "In-DRAM Processing-In-Memory (DRAM-PIM) has emerged as a promising approach to accelerate memory-intensive workloads by mitigating data transfer overhead between DRAM and the host processor. Bit-serial DRAM-PIM architectures, further enhance efficiency by supporting runtime variable data precision, which is critical for emerging workloads, such as large language model (LLM) inference. However, existing works still have major limitations: lack of data reuse, significant amounts of redundant data transfer, and insufficient support for workload mapping. To address these issues, we propose RACAM, the first in-DRAM bit-serial architecture which uses dedicated locality buffers, bit-serial PEs, popcount reduction units and broadcast units to enable data reuse and alleviate redundant data transfers. Furthermore, a workload mapping mechanism is proposed to fully explore the massive parallelism of DRAM architecture and identify the best mapping scheme of a given workload. We evaluate RACAM against GPUs and the state-of-the-art, in-DRAM PIM system, Proteus, across end-to-end LLM inferences. RACAM achieves 9x to 102x speedup over GPUs and 233x higher performance per mm2 compared to Proteus in case of GPT3.", "AI": {"tldr": "RACAM\u662f\u4e00\u79cd\u65b0\u578b\u7684DRAM\u5185\u4f4d\u4e32\u884c\u67b6\u6784\uff0c\u901a\u8fc7\u5c40\u90e8\u6027\u7f13\u51b2\u533a\u548c\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709DRAM-PIM\u65b9\u6848\u7f3a\u4e4f\u6570\u636e\u590d\u7528\u3001\u5b58\u5728\u5197\u4f59\u6570\u636e\u4f20\u8f93\u4e14\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u652f\u6301\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u4e13\u7528\u5c40\u90e8\u7f13\u51b2\u533a\u3001\u4f4d\u4e32\u884c\u5904\u7406\u5355\u5143\u3001popcount\u7f29\u51cf\u5355\u5143\u4e0e\u5e7f\u64ad\u5355\u5143\uff0c\u5e76\u8bbe\u8ba1\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u673a\u5236\u3002", "result": "\u76f8\u6bd4GPU\u63d0\u901f9-102\u500d\uff0c\u76f8\u6bd4Proteus\u5728GPT3\u4e0a\u5b9e\u73b0233\u500d/mm\u00b2\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "RACAM\u6709\u6548\u89e3\u51b3DRAM-PIM\u73b0\u5b58\u74f6\u9888\uff0c\u5927\u5e45\u63d0\u5347\u80fd\u6548\u4e0e\u5e76\u884c\u6548\u7387\u3002"}}
{"id": "2512.09187", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09187", "abs": "https://arxiv.org/abs/2512.09187", "authors": ["Mrinal Agarwal", "Saad Rana", "Theo Sundoro", "Hermela Berhe", "Spencer Kim", "Vasu Sharma", "Sean O'Brien", "Kevin Zhu"], "title": "WOLF: Werewolf-based Observations for LLM Deception and Falsehoods", "comment": "Spotlight Multi-Turn Interactions in Large Language Models (MTI-LLM) Workshop at NeurIPS 2025", "summary": "Deception is a fundamental challenge for multi-agent reasoning: effective systems must strategically conceal information while detecting misleading behavior in others. Yet most evaluations reduce deception to static classification, ignoring the interactive, adversarial, and longitudinal nature of real deceptive dynamics. Large language models (LLMs) can deceive convincingly but remain weak at detecting deception in peers. We present WOLF, a multi-agent social deduction benchmark based on Werewolf that enables separable measurement of deception production and detection. WOLF embeds role-grounded agents (Villager, Werewolf, Seer, Doctor) in a programmable LangGraph state machine with strict night-day cycles, debate turns, and majority voting. Every statement is a distinct analysis unit, with self-assessed honesty from speakers and peer-rated deceptiveness from others. Deception is categorized via a standardized taxonomy (omission, distortion, fabrication, misdirection), while suspicion scores are longitudinally smoothed to capture both immediate judgments and evolving trust dynamics. Structured logs preserve prompts, outputs, and state transitions for full reproducibility. Across 7,320 statements and 100 runs, Werewolves produce deceptive statements in 31% of turns, while peer detection achieves 71-73% precision with ~52% overall accuracy. Precision is higher for identifying Werewolves, though false positives occur against Villagers. Suspicion toward Werewolves rises from ~52% to over 60% across rounds, while suspicion toward Villagers and the Doctor stabilizes near 44-46%. This divergence shows that extended interaction improves recall against liars without compounding errors against truthful roles. WOLF moves deception evaluation beyond static datasets, offering a dynamic, controlled testbed for measuring deceptive and detective capacity in adversarial multi-agent interaction.", "AI": {"tldr": "WOLF\u662f\u4e00\u4e2a\u57fa\u4e8e\u72fc\u4eba\u6740\u7684\u591a\u667a\u80fd\u4f53\u793e\u4ea4\u63a8\u7406\u57fa\u51c6\uff0c\u7528\u4e8e\u5206\u522b\u8861\u91cf\u6b3a\u9a97\u751f\u6210\u4e0e\u68c0\u6d4b\u80fd\u529b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u65e5\u5fd7\u548c\u52a8\u6001\u4ea4\u4e92\u63d0\u5347\u8bc4\u4f30\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u6b3a\u9a97\u8bc4\u4f30\u591a\u4e3a\u9759\u6001\u5206\u7c7b\uff0c\u5ffd\u89c6\u4e86\u771f\u5b9e\u573a\u666f\u4e2d\u5bf9\u6297\u6027\u3001\u4ea4\u4e92\u6027\u548c\u957f\u671f\u6027\u7684\u6b3a\u9a97\u52a8\u6001\uff0c\u9700\u6784\u5efa\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u6d4b\u8bd5\u73af\u5883\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8eLangGraph\u72b6\u6001\u673a\u7684WOLF\u57fa\u51c6\uff0c\u5d4c\u5165\u89d2\u8272\u5316\u667a\u80fd\u4f53\uff0c\u8bbe\u5b9a\u663c\u591c\u5faa\u73af\u3001\u8fa9\u8bba\u56de\u5408\u4e0e\u6295\u7968\u673a\u5236\uff0c\u5bf9\u6bcf\u53e5\u8bdd\u6807\u6ce8\u8bda\u5b9e\u5ea6\u4e0e\u6b3a\u9a97\u6027\uff0c\u5e76\u6309\u6807\u51c6\u5206\u7c7b\u6b3a\u9a97\u7c7b\u578b\u3002", "result": "\u72fc\u4eba31%\u56de\u5408\u5b9e\u65bd\u6b3a\u9a97\uff0c\u540c\u4f34\u68c0\u6d4b\u7cbe\u5ea671-73%\uff0c\u6574\u4f53\u51c6\u786e\u7387\u7ea652%\uff1b\u968f\u8f6e\u6b21\u589e\u52a0\uff0c\u5bf9\u72fc\u4eba\u6000\u7591\u5ea6\u4ece52%\u5347\u81f360%\uff0c\u5bf9\u6751\u6c11\u548c\u533b\u751f\u7a33\u5b9a\u572844-46%\u3002", "conclusion": "WOLF\u63d0\u4f9b\u4e86\u4e00\u4e2a\u52a8\u6001\u53ef\u63a7\u7684\u5bf9\u6297\u6027\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u6d4b\u8bd5\u5e73\u53f0\uff0c\u80fd\u6709\u6548\u5206\u79bb\u5e76\u6d4b\u91cf\u6b3a\u9a97\u4e0e\u8bc6\u9a97\u80fd\u529b\uff0c\u8d85\u8d8a\u9759\u6001\u6570\u636e\u96c6\u8bc4\u4f30\u5c40\u9650\u3002"}}
{"id": "2512.09006", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09006", "abs": "https://arxiv.org/abs/2512.09006", "authors": ["Dyna Soumhane Ouchebara", "St\u00e9phane Dupont"], "title": "Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning", "comment": "20 pages, Accepted at ESORICS 2025", "summary": "The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6e90\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u53cc\u5fae\u8c03\u65b9\u6cd5\u5e76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u540c\u65f6\u63a2\u8ba8\u63d0\u793a\u5de5\u7a0b\u4e0eRAG\u6280\u672f\u7684\u8868\u73b0\u3002", "motivation": "\u8f6f\u4ef6\u5f00\u53d1\u5468\u671f\u52a0\u901f\u5bfc\u81f4\u6f0f\u6d1e\u6fc0\u589e\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u68c0\u6d4b\u65b9\u6cd5\uff0cAI\u5c24\u5176\u662fLLM\u6210\u4e3a\u65b0\u5174\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eLlama-3.1 8B\u6a21\u578b\uff0c\u91c7\u7528BigVul\u548cPrimeVul\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u591a\u79cd\u5fae\u8c03\u7b56\u7565\uff08\u542b\u65b0\u9896\u7684Double Fine-tuning\u548cTest-Time\u5fae\u8c03\uff09\u53ca\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u3002", "result": "\u5fae\u8c03\u5bf9\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u53cc\u5fae\u8c03\u8868\u73b0\u4f18\u5f02\uff0cLlama\u6a21\u578b\u5177\u6f5c\u529b\uff1b\u63d0\u793a\u6548\u679c\u4e0d\u4f73\uff0c\u4f46RAG\u4f5c\u4e3a\u793a\u4f8b\u9009\u62e9\u6280\u672f\u8868\u73b0\u5c1a\u53ef\u3002", "conclusion": "\u90e8\u5206\u7814\u7a76\u95ee\u9898\u5df2\u89e3\u7b54\uff0c\u4ecd\u5b58\u5927\u91cf\u5f85\u63a2\u7d22\u65b9\u5411\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2512.09277", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.09277", "abs": "https://arxiv.org/abs/2512.09277", "authors": ["Yanpeng Yu", "Haiyue Ma", "Krish Agarwal", "Nicolai Oswald", "Qijing Huang", "Hugo Linsenmaier", "Chunhui Mei", "Ritchie Zhao", "Ritika Borkar", "Bita Darvish Rouhani", "David Nellans", "Ronny Krashinsky", "Anurag Khandelwal"], "title": "Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens", "comment": null, "summary": "Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.\n  We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.", "AI": {"tldr": "METRO\u662f\u4e00\u79cd\u9488\u5bf9\u5185\u5b58\u53d7\u9650\u573a\u666f\u7684\u4e13\u5bb6\u5e76\u884cMoE\u670d\u52a1\u65b0\u8def\u7531\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u6bcfGPU\u6fc0\u6d3b\u4e13\u5bb6\u6570\u800c\u975e\u4ee4\u724c\u6570\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u5347\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5185\u5b58\u53d7\u9650\u65f6\u901a\u8fc7\u5e73\u8861\u5404GPU\u5904\u7406\u4ee4\u724c\u6570\u53cd\u800c\u52a0\u5267\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u589e\u52a0\u6fc0\u6d3b\u4e13\u5bb6\u6570\u5bfc\u81f4\u5185\u5b58\u538b\u529b\u589e\u5927\u3002", "method": "\u63d0\u51faMETRO\u7b97\u6cd5\uff0c\u4f18\u5316\u6fc0\u6d3b\u4e13\u5bb6\u6570\u5e73\u8861\uff0c\u7ed3\u5408\u9ad8\u6548\u7b97\u6cd5\u4e0eGPU\u5e76\u884c\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u4f4e\u5f00\u9500allGather\u83b7\u53d6\u5168\u5c40top-k\u4fe1\u606f\u3002", "result": "\u5728A100\u548cB200\u7cfb\u7edf\u4e0a\uff0c\u76f8\u6bd4EPLB\uff0cMETRO\u964d\u4f4e\u89e3\u7801\u5ef6\u8fdf11-22%\uff0c\u603b\u541e\u5410\u91cf\u63d0\u53473-21%\uff0c\u56fa\u5b9aSLO\u4e0b\u89e3\u7801\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53474.11\u500d\u3002", "conclusion": "METRO\u5728\u5185\u5b58\u53d7\u9650\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\uff0c\u4e3a\u4e13\u5bb6\u5e76\u884cMoE\u90e8\u7f72\u63d0\u4f9b\u66f4\u4f18\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09427", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09427", "abs": "https://arxiv.org/abs/2512.09427", "authors": ["Guoqiang Zou", "Wanyu Wang", "Hao Zheng", "Longxiang Yin", "Yinhe Han"], "title": "ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators", "comment": "10 pages, 5 figures", "summary": "Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.", "AI": {"tldr": "ODMA\u662f\u4e00\u79cd\u9488\u5bf9\u968f\u673a\u8bbf\u95ee\u53d7\u9650\u5185\u5b58\uff08RACM\uff09\u52a0\u901f\u5668\u7684\u6309\u9700\u5185\u5b58\u5206\u914d\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347LLM\u670d\u52a1\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5185\u5b58\u7ba1\u7406\u65b9\u6cd5\u5728RACM\u8bbe\u5907\u4e0a\u6548\u7387\u4f4e\u4e0b\uff0c\u65e0\u6cd5\u517c\u987e\u5185\u5b58\u5229\u7528\u7387\u4e0e\u8bbf\u95ee\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u8f7b\u91cf\u7ea7\u957f\u5ea6\u9884\u6d4b\u3001\u52a8\u6001\u6876\u5206\u533a\u548c\u5927\u6876\u4fdd\u62a4\u673a\u5236\uff0c\u5468\u671f\u6027\u6839\u636e\u8fd0\u884c\u8f68\u8ff9\u66f4\u65b0\u8fb9\u754c\u4ee5\u4f18\u5316\u5185\u5b58\u5229\u7528\u3002", "result": "\u5728Cambricon MLU370-X4\u4e0a\u8fd0\u884cDeepSeek-R1-Distill-Qwen-7B\uff0c\u5185\u5b58\u5229\u7528\u7387\u4ece55.05%\u63d0\u5347\u81f372.45%\uff0cRPS\u548cTPS\u5206\u522b\u63d0\u9ad829%\u548c27%\u3002", "conclusion": "\u786c\u4ef6\u611f\u77e5\u7684\u5185\u5b58\u5206\u914d\u7b56\u7565\u80fd\u6709\u6548\u91ca\u653eRACM\u5e73\u53f0\u4e0a\u7684LLM\u670d\u52a1\u6f5c\u529b\u3002"}}
{"id": "2512.09396", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09396", "abs": "https://arxiv.org/abs/2512.09396", "authors": ["Zishu Wei", "Qixiang Ma", "Xavier Hu", "Yuhang Liu", "Hui Zang", "Yudong Zhao", "Tao Wang", "Shengyu Zhang", "Fei Wu"], "title": "GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection", "comment": null, "summary": "Building AI systems for GUI automation task has attracted remarkable research efforts, where MLLMs are leveraged for processing user requirements and give operations. However, GUI automation includes a wide range of tasks, from document processing to online shopping, from CAD to video editing. Diversity between particular tasks requires MLLMs for GUI automation to have heterogeneous capabilities and master multidimensional expertise, raising problems on constructing such a model. To address such challenge, we propose GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection, a novel MLLM-based GUI automation agent framework designed for integrating knowledge and combining capabilities from heterogeneous models to build GUI automation agent systems with higher performance. Since different GUI-specific MLLMs are trained on different dataset and thus have different strengths, GAIR introduced a general-purpose MLLM for jointly processing the information from multiple GUI-specific models, further enhancing performance of the agent framework. The general-purpose MLLM also serves as decision maker, trying to execute a reasonable operation based on previously gathered information. When the general-purpose model thinks that there isn't sufficient information for a reasonable decision, GAIR would transit into group reflection status, where the general-purpose model would provide GUI-specific models with different instructions and hints based on their strengths and weaknesses, driving them to gather information with more significance and accuracy that can support deeper reasoning and decision. We evaluated the effectiveness and reliability of GAIR through extensive experiments on GUI benchmarks.", "AI": {"tldr": "GAIR\u6846\u67b6\u901a\u8fc7\u4fe1\u606f\u8054\u5408\u63a8\u7406\u4e0e\u7fa4\u4f53\u53cd\u601d\uff0c\u6574\u5408\u5f02\u6784\u6a21\u578b\u80fd\u529b\u4ee5\u63d0\u5347GUI\u81ea\u52a8\u5316\u6027\u80fd\u3002", "motivation": "GUI\u81ea\u52a8\u5316\u4efb\u52a1\u591a\u6837\u4e14\u590d\u6742\uff0c\u9700\u8981MLLM\u5177\u5907\u591a\u7ef4\u80fd\u529b\uff0c\u4f46\u6784\u5efa\u6b64\u7c7b\u6a21\u578b\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51faGAIR\u6846\u67b6\uff0c\u5229\u7528\u901a\u7528MLLM\u6574\u5408\u591a\u4e2aGUI\u4e13\u7528\u6a21\u578b\u7684\u4fe1\u606f\uff0c\u5e76\u5728\u4fe1\u606f\u4e0d\u8db3\u65f6\u89e6\u53d1\u7fa4\u4f53\u53cd\u601d\u673a\u5236\u4ee5\u4f18\u5316\u51b3\u7b56\u3002", "result": "\u5728GUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGAIR\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "conclusion": "GAIR\u6709\u6548\u6574\u5408\u5f02\u6784\u6a21\u578b\u77e5\u8bc6\u4e0e\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347GUI\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u51b3\u7b56\u8d28\u91cf\u4e0e\u6267\u884c\u6548\u679c\u3002"}}
{"id": "2512.09309", "categories": ["cs.DC", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09309", "abs": "https://arxiv.org/abs/2512.09309", "authors": ["Zihao Ding", "Mufeng Zhu", "Zhongze Tang", "Sheng Wei", "Yao Liu"], "title": "A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge", "comment": "16 pages, 7 figures. Published in the Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing (SEC '25), Dec 3-6, 2025, Washington, D.C., USA", "summary": "Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5206\u5e03\u5f0f\u5206\u5c42\u5378\u8f7d\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u667a\u80fd\u5de5\u5177\u5728\u4e91\u7aef\u5904\u7406\u65f6\u5e26\u6765\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u672c\u5730\u53ef\u4fe1\u8fb9\u7f18\u8bbe\u5907\u5c06\u56fe\u50cf\u5206\u7247\u5e76\u5206\u5e03\u5230\u591a\u4e2a\u4e91\u670d\u52a1\u5668\uff0c\u6700\u7ec8\u5728\u8fb9\u7f18\u8bbe\u5907\u805a\u5408\u7ed3\u679c\u3002", "result": "\u5728\u4fdd\u6301\u63a5\u8fd1\u57fa\u51c6\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5185\u5bb9\u91cd\u5efa\u548c\u6570\u636e\u66b4\u9732\u98ce\u9669\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8fb9\u7f18-\u4e91\u8fde\u7eed\u4f53\u4e2d\u7684\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09196", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09196", "abs": "https://arxiv.org/abs/2512.09196", "authors": ["Haonan Li", "Keyu Man", "Partha Kanuparthy", "Hanning Chen", "Wei Sun", "Sreen Tallam", "Chenguang Zhu", "Kevin Zhu", "Zhiyun Qian"], "title": "TritonForge: Profiling-Guided Framework for Automated Triton Kernel Optimization", "comment": null, "summary": "High-performance GPU kernel optimization remains a critical yet labor-intensive task in modern machine learning workloads. Although Triton, a domain-specific language for GPU programming, enables developers to write efficient kernels with concise code, achieving expert-level performance still requires deep understanding of GPU architectures and low-level performance trade-offs. We present TritonForge, a profiling-guided framework for automated Triton kernel optimization. TritonForge integrates kernel analysis, runtime profiling, and iterative code transformation to streamline the optimization process. By incorporating data-driven feedback from profiling results, the system identifies performance bottlenecks, proposes targeted code modifications, and evaluates their impact automatically. While our prototype leverages large language models (LLMs) to assist in code reasoning and transformation, the framework remains modular and model-agnostic. Across diverse kernel types and GPU architectures, TritonForge achieves up to 5x performance improvement over baseline implementations and on average 1.76x of the cases are successful, providing a foundation for future research in automated GPU performance optimization.", "AI": {"tldr": "TritonForge\u662f\u4e00\u4e2a\u57fa\u4e8e\u6027\u80fd\u5206\u6790\u7684\u81ea\u52a8\u5316Triton\u5185\u6838\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5185\u6838\u5206\u6790\u3001\u8fd0\u884c\u65f6\u5256\u6790\u548c\u8fed\u4ee3\u4ee3\u7801\u8f6c\u6362\uff0c\u5728\u591a\u79cdGPU\u67b6\u6784\u4e0a\u5e73\u5747\u5b9e\u73b01.76\u500d\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u9ad8\u6027\u80fdGPU\u5185\u6838\u4f18\u5316\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\u4e14\u8017\u65f6\u8d39\u529b\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u5de5\u5177\u964d\u4f4e\u95e8\u69db\u5e76\u63d0\u5347\u6548\u7387\u3002", "method": "\u7ed3\u5408\u8fd0\u884c\u65f6\u6027\u80fd\u5256\u6790\u6570\u636e\uff0c\u81ea\u52a8\u8bc6\u522b\u74f6\u9888\u3001\u63d0\u51fa\u4ee3\u7801\u4fee\u6539\u5efa\u8bae\u5e76\u8bc4\u4f30\u6548\u679c\uff1b\u5f53\u524d\u539f\u578b\u4f7f\u7528LLM\u8f85\u52a9\u63a8\u7406\uff0c\u4f46\u6846\u67b6\u672c\u8eab\u6a21\u5757\u5316\u4e14\u6a21\u578b\u65e0\u5173\u3002", "result": "\u5728\u591a\u79cd\u5185\u6838\u7c7b\u578b\u548cGPU\u67b6\u6784\u4e0a\u6700\u9ad8\u8fbe5\u500d\u52a0\u901f\uff0c\u5e73\u57471.76\u500d\u6210\u529f\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "TritonForge\u4e3a\u81ea\u52a8\u5316GPU\u6027\u80fd\u4f18\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2512.09331", "categories": ["cs.DC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.09331", "abs": "https://arxiv.org/abs/2512.09331", "authors": ["Nam Anh Dang", "Ben Landrum", "Ken Birman"], "title": "Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN", "comment": "12 pages, 14 figures, submitted to VLDB 2026", "summary": "Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.", "AI": {"tldr": "BatANN \u662f\u9996\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u78c1\u76d8\u5411\u91cf\u68c0\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u5168\u5c40\u56fe\u7ed3\u6784\u4e0e\u72b6\u6001\u8fc1\u79fb\u5b9e\u73b0\u9ad8\u541e\u5410\u4e0e\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u5e94\u5bf9\u672a\u6765\u8d85\u5927\u89c4\u6a21\u5411\u91cf\u6570\u636e\u96c6\u65e0\u6cd5\u5728\u5355\u673a\u5b58\u50a8\u4e0e\u68c0\u7d22\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u5728\u8de8\u673a\u5668\u8bbf\u95ee\u90bb\u5c45\u8282\u70b9\u65f6\u8fc1\u79fb\u5b8c\u6574\u67e5\u8be2\u72b6\u6001\u4ee5\u63d0\u5347\u5c40\u90e8\u6027\uff0c\u5e76\u57fa\u4e8e\u6807\u51c6TCP\u901a\u4fe1\u3002", "result": "\u572810\u53f0\u670d\u52a1\u5668\u30010.95\u53ec\u56de\u7387\u4e0b\uff0c\u76f8\u6bd4scatter-gather\u57fa\u7ebf\uff0c1\u4ebf\u4e0e10\u4ebf\u6570\u636e\u96c6\u5206\u522b\u63d0\u53476.21-6.49\u500d\u548c2.5-5.10\u500d\u541e\u5410\uff0c\u5e73\u5747\u5ef6\u8fdf\u4f4e\u4e8e6ms\u3002", "conclusion": "BatANN \u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5355\u4e00\u5168\u5c40\u56fe\u7684\u5206\u5e03\u5f0f\u78c1\u76d8\u5411\u91cf\u641c\u7d22\uff0c\u517c\u5177\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2512.09216", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09216", "abs": "https://arxiv.org/abs/2512.09216", "authors": ["Guangzong Cai", "Zengyang Li", "Peng Liang", "Ran Mo", "Hui Liu", "Yutao Ma"], "title": "Bug Priority Change Prediction: An Exploratory Study on Apache Software", "comment": "Preprint accepted for publication in ACM Transactions on Software Engineering and Methodology (TOSEM), 2025", "summary": "Bug fixing is a critical activity in the software development process. In issue tracking systems such as JIRA, each bug report is assigned a priority level to indicate the urgency and importance level of the bug. The priority may change during the bug fixing process, indicating that the urgency and importance level of the bug will change with the bug fixing. However, manually evaluating priority changes for bugs is a tedious process that heavily relies on the subjective judgment of developers and project managers, leading to incorrect priority changes and thus hindering timely bug fixes. Given the lack of research on bug priority change prediction, we propose a novel two-phase bug report priority change prediction method based on bug fixing evolution features and class imbalance handling strategy. Specifically, we divided the bug lifecycle into two phases: bug reporting and bug fixing, and constructed bug priority change prediction models for each phase. To evaluate the performance of our method, we conducted experiments on a bug dataset constructed from 32 non-trivial Apache projects. The experimental results show that our proposed bug fixing evolution features and the adopted class imbalance handling strategy can effectively improve the performance of prediction models. The F1-score of the prediction model constructed for the bug reporting phase reached 0.798, while the F1-weighted and F1-macro of the prediction model constructed for the bug fixing phase were 0.712 and 0.613, respectively. Furthermore, we explored the cross-project applicability of our prediction models and their performance at different priority levels. The findings indicate large variations in model performance across different projects, although the overall scores remain decent. Meanwhile, the predictive performance across various priority levels remained relatively consistently high.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f3a\u9677\u4fee\u590d\u6f14\u5316\u7279\u5f81\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5904\u7406\u7b56\u7565\u7684\u4e24\u9636\u6bb5\u7f3a\u9677\u62a5\u544a\u4f18\u5148\u7ea7\u53d8\u66f4\u9884\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u624b\u52a8\u8bc4\u4f30\u7f3a\u9677\u4f18\u5148\u7ea7\u53d8\u66f4\u4f9d\u8d56\u4e3b\u89c2\u5224\u65ad\uff0c\u6613\u51fa\u9519\u4e14\u6548\u7387\u4f4e\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u5c06\u7f3a\u9677\u751f\u547d\u5468\u671f\u5206\u4e3a\u62a5\u544a\u4e0e\u4fee\u590d\u4e24\u9636\u6bb5\uff0c\u5206\u522b\u6784\u5efa\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u6f14\u5316\u7279\u5f81\u4e0e\u7c7b\u522b\u4e0d\u5e73\u8861\u5904\u7406\u7b56\u7565\u3002", "result": "\u5728Apache\u9879\u76ee\u6570\u636e\u96c6\u4e0a\uff0c\u62a5\u544a\u9636\u6bb5F1-score\u8fbe0.798\uff0c\u4fee\u590d\u9636\u6bb5F1-weighted\u4e3a0.712\u3001F1-macro\u4e3a0.613\uff1b\u8de8\u9879\u76ee\u8868\u73b0\u5dee\u5f02\u5927\u4f46\u6574\u4f53\u826f\u597d\uff0c\u5404\u4f18\u5148\u7ea7\u9884\u6d4b\u6027\u80fd\u7a33\u5b9a\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u7f3a\u9677\u4f18\u5148\u7ea7\u53d8\u66f4\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u7f3a\u9677\u7ba1\u7406\u6d41\u7a0b\u3002"}}
{"id": "2512.09472", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09472", "abs": "https://arxiv.org/abs/2512.09472", "authors": ["Chiheng Lou", "Sheng Qi", "Rui Kang", "Yong Zhang", "Chen Sun", "Pengcheng Wang", "Bingyang Liu", "Xuanzhe Liu", "Xin Jin"], "title": "WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving", "comment": null, "summary": "Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.\n  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\\times$ more requests compared to the GPU-sharing system.", "AI": {"tldr": "WarmServe\u901a\u8fc7\u9884\u77e5\u672a\u6765\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4f18\u5316\u591aLLM\u670d\u52a1\u7684GPU\u9884\u70ed\u4e0e\u5185\u5b58\u7ba1\u7406\uff0c\u663e\u8457\u964d\u4f4e\u9996token\u5ef6\u8fdf\u5e76\u63d0\u5347\u8bf7\u6c42\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u4e3a\u63d0\u5347GPU\u5229\u7528\u7387\u727a\u7272\u63a8\u7406\u6027\u80fd\uff0c\u5c24\u5176\u9996token\u5ef6\u8fdf\uff0c\u800c\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u5177\u6709\u5468\u671f\u6027\u4e0e\u53ef\u9884\u6d4b\u6027\u3002", "method": "\u8bbe\u8ba1\u901a\u7528GPU\u5de5\u4f5c\u8005\u3001\u91c7\u7528\u9a71\u9010\u611f\u77e5\u6a21\u578b\u653e\u7f6e\u3001\u4e3b\u52a8\u9884\u70ed\u53ca\u96f6\u5f00\u9500\u5185\u5b58\u5207\u6362\u673a\u5236\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\uff0cTTFT\u964d\u4f4e\u8fbe50.8\u500d\uff0c\u8bf7\u6c42\u5904\u7406\u91cf\u63d0\u53472.5\u500d\u3002", "conclusion": "WarmServe\u6709\u6548\u5e73\u8861\u8d44\u6e90\u6548\u7387\u4e0e\u63a8\u7406\u6027\u80fd\uff0c\u9002\u5408\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u591aLLM\u90e8\u7f72\u3002"}}
{"id": "2512.09562", "categories": ["cs.SE", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.09562", "abs": "https://arxiv.org/abs/2512.09562", "authors": ["Radoslaw Klimek", "Jakub Blazowski"], "title": "Explainable Verification of Hierarchical Workflows Mined from Event Logs with Shapley Values", "comment": "This manuscript has been submitted to Rank A/A* conference", "summary": "Workflow mining discovers hierarchical process trees from event logs, but it remains unclear why such models satisfy or violate logical properties, or how individual elements contribute to overall behavior. We propose to translate mined workflows into logical specifications and analyze properties such as satisfiability, liveness, and safety with automated theorem provers. On this basis, we adapt Shapley values from cooperative game theory to attribute outcomes to workflow elements and quantify their contributions. Experiments on benchmark datasets show that this combination identifies critical nodes, reveals redundancies, and exposes harmful structures. This outlines a novel direction for explainable workflow analysis with direct relevance to software engineering practice, supporting compliance checks, process optimization, redundancy reduction, and the design of next-generation process mining tools.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u7ed3\u5408\u5de5\u4f5c\u6d41\u6316\u6398\u4e0e\u903b\u8f91\u5206\u6790\uff0c\u5229\u7528Shapley\u503c\u91cf\u5316\u6d41\u7a0b\u5143\u7d20\u8d21\u732e\uff0c\u4ee5\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5de5\u4f5c\u6d41\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u6d41\u6a21\u578b\u7f3a\u4e4f\u5bf9\u903b\u8f91\u5c5e\u6027\u6ee1\u8db3\u6216\u8fdd\u53cd\u539f\u56e0\u7684\u89e3\u91ca\uff0c\u4e5f\u672a\u80fd\u91cf\u5316\u5404\u5143\u7d20\u5bf9\u6574\u4f53\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u5c06\u6316\u6398\u51fa\u7684\u5de5\u4f5c\u6d41\u8f6c\u6362\u4e3a\u903b\u8f91\u89c4\u8303\uff0c\u4f7f\u7528\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u5668\u5206\u6790\u5c5e\u6027\uff0c\u5e76\u5f15\u5165Shapley\u503c\u91cf\u5316\u5143\u7d20\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u8bc6\u522b\u5173\u952e\u8282\u70b9\u3001\u63ed\u793a\u5197\u4f59\u5e76\u66b4\u9732\u6709\u5bb3\u7ed3\u6784\uff0c\u63d0\u5347\u5de5\u4f5c\u6d41\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u89e3\u91ca\u5de5\u4f5c\u6d41\u5206\u6790\u5f00\u8f9f\u65b0\u65b9\u5411\uff0c\u6709\u52a9\u4e8e\u5408\u89c4\u68c0\u67e5\u3001\u6d41\u7a0b\u4f18\u5316\u4e0e\u4e0b\u4e00\u4ee3\u6d41\u7a0b\u6316\u6398\u5de5\u5177\u8bbe\u8ba1\u3002"}}
{"id": "2512.09568", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09568", "abs": "https://arxiv.org/abs/2512.09568", "authors": ["Zhi Zhao", "Hang Xiao", "Wei Rang"], "title": "PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing", "comment": "24 pages,5 figures", "summary": "Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.", "AI": {"tldr": "\u63d0\u51faPHWSOA\u7b97\u6cd5\uff0c\u7ed3\u5408\u9cb8\u9c7c\u4e0e\u6d77\u9e25\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e15\u7d2f\u6258\u591a\u76ee\u6807\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e91\u4efb\u52a1\u8c03\u5ea6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8c03\u5ea6\u65b9\u6cd5\u591a\u5173\u6ce8\u5355\u4e00\u6307\u6807\uff0c\u7f3a\u4e4f\u5bf9\u5b8c\u5de5\u65f6\u95f4\u3001\u8d1f\u8f7d\u5747\u8861\u548c\u6210\u672c\u7684\u7efc\u5408\u4f18\u5316\u3002", "method": "\u878d\u5408WOA\u4e0eSOA\uff0c\u5f15\u5165Halton\u5e8f\u5217\u521d\u59cb\u5316\u3001\u5e15\u7d2f\u6258\u5f15\u5bfc\u53d8\u5f02\u673a\u5236\u3001\u5e76\u884c\u5904\u7406\u53ca\u52a8\u6001\u8d1f\u8f7d\u91cd\u5206\u914d\u7b56\u7565\u3002", "result": "\u5728CloudSim\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u57fa\u7ebf\u7b97\u6cd5\uff0cPHWSOA\u4f7f\u5b8c\u5de5\u65f6\u95f4\u51cf\u5c1172.1%\uff0c\u8d1f\u8f7d\u5747\u8861\u63d0\u534736.8%\uff0c\u6210\u672c\u964d\u4f4e23.5%\u3002", "conclusion": "PHWSOA\u5728\u591a\u76ee\u6807\u4e91\u4efb\u52a1\u8c03\u5ea6\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2512.09596", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09596", "abs": "https://arxiv.org/abs/2512.09596", "authors": ["Arkadiusz Ry\u015b", "Lucas Lima", "Joeri Exelmans", "Dennis Janssens", "Hans Vangheluwe"], "title": "Model management to support systems engineering workflows using ontology-based knowledge graphs", "comment": null, "summary": "System engineering has been shifting from document-centric to model-based approaches, where assets are becoming more and more digital. Although digitisation conveys several benefits, it also brings several concerns (e.g., storage and access) and opportunities. In the context of Cyber- Physical Systems (CPS), we have experts from various domains executing complex workflows and manipulating models in a plethora of different formalisms, each with their own methods, techniques and tools. Storing knowledge on these workflows can reduce considerable effort during system development not only to allow their repeatability and replicability but also to access and reason on data generated by their execution. In this work, we propose a framework to manage modelling artefacts generated from workflow executions. The basic workflow concepts, related formalisms and artefacts are formally defined in an ontology specified in OML (Ontology Modelling Language). This ontology enables the construction of a knowledge graph that contains system engineering data to which we can apply reasoning. We also developed several tools to support system engineering during the design of workflows, their enactment, and artefact storage, considering versioning, querying and reasoning on the stored data. These tools also hide the complexity of manipulating the knowledge graph directly. Finally, we have applied our proposed framework in a real-world system development scenario of a drivetrain smart sensor system. Results show that our proposal not only helped the system engineer with fundamental difficulties like storage and versioning but also reduced the time needed to access relevant information and new knowledge that can be inferred from the knowledge graph.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u672c\u4f53\u7684\u77e5\u8bc6\u56fe\u8c31\u6846\u67b6\uff0c\u7528\u4e8e\u7ba1\u7406CPS\u7cfb\u7edf\u5de5\u7a0b\u4e2d\u7684\u5efa\u6a21\u4ea7\u7269\uff0c\u652f\u6301\u5b58\u50a8\u3001\u7248\u672c\u63a7\u5236\u3001\u67e5\u8be2\u4e0e\u63a8\u7406\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "motivation": "\u5e94\u5bf9\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\u4e2d\u591a\u9886\u57df\u5de5\u4f5c\u6d41\u4ea7\u751f\u7684\u5f02\u6784\u5efa\u6a21\u4ea7\u7269\u7684\u5b58\u50a8\u3001\u8bbf\u95ee\u4e0e\u590d\u7528\u96be\u9898\uff0c\u63d0\u5347\u7cfb\u7edf\u5f00\u53d1\u7684\u53ef\u91cd\u590d\u6027\u4e0e\u77e5\u8bc6\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efaOML\u672c\u4f53\u5b9a\u4e49\u5de5\u4f5c\u6d41\u6982\u5ff5\u4e0e\u4ea7\u7269\uff0c\u5efa\u7acb\u77e5\u8bc6\u56fe\u8c31\u5e76\u5f00\u53d1\u914d\u5957\u5de5\u5177\u94fe\uff0c\u652f\u6301\u8bbe\u8ba1\u3001\u6267\u884c\u3001\u5b58\u50a8\u3001\u67e5\u8be2\u4e0e\u63a8\u7406\uff0c\u5c4f\u853d\u5e95\u5c42\u590d\u6742\u6027\u3002", "result": "\u5728\u4f20\u52a8\u7cfb\u667a\u80fd\u4f20\u611f\u5668\u7cfb\u7edf\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff0c\u663e\u8457\u6539\u5584\u5b58\u50a8\u4e0e\u7248\u672c\u7ba1\u7406\uff0c\u7f29\u77ed\u4fe1\u606f\u83b7\u53d6\u65f6\u95f4\uff0c\u5e76\u652f\u6301\u4ece\u56fe\u8c31\u4e2d\u63a8\u5bfc\u65b0\u77e5\u8bc6\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u6574\u5408\u5f02\u6784\u5efa\u6a21\u8d44\u4ea7\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u4e0e\u5de5\u5177\u94fe\u589e\u5f3a\u7cfb\u7edf\u5de5\u7a0b\u6548\u7387\u4e0e\u667a\u80fd\u5316\u6c34\u5e73\uff0c\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2512.09664", "categories": ["cs.DC", "cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.09664", "abs": "https://arxiv.org/abs/2512.09664", "authors": ["Antonio Terpin", "Alan Bonomi", "Francesco Banelli", "Raffaello D'Andrea"], "title": "SynthPix: A lightspeed PIV images generator", "comment": "Code: https://github.com/antonioterpin/synthpix", "summary": "We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.", "AI": {"tldr": "SynthPix\u662f\u4e00\u4e2a\u57fa\u4e8eJAX\u7684\u9ad8\u6027\u80fd\u5e76\u884c\u5408\u6210\u56fe\u50cf\u751f\u6210\u5668\uff0c\u4e13\u4e3a\u7c92\u5b50\u56fe\u50cf\u6d4b\u901f\uff08PIV\uff09\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u5bf9\u751f\u6210\u901f\u5ea6\uff0c\u52a9\u529b\u5f3a\u5316\u5b66\u4e60\u4e0e\u5b9e\u65f6\u6d41\u573a\u4f30\u8ba1\u7814\u7a76\u3002", "motivation": "\u52a0\u901f\u6570\u636e\u5bc6\u96c6\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\uff0c\u7f29\u77ed\u5b9e\u65f6PIV\u53cd\u9988\u63a7\u5236\u4e2d\u6d41\u573a\u4f30\u8ba1\u7b97\u6cd5\u7684\u5f00\u53d1\u8fed\u4ee3\u65f6\u95f4\u3002", "method": "\u91c7\u7528JAX\u5b9e\u73b0\u9ad8\u6027\u80fd\u5e76\u884c\u5316\u67b6\u6784\uff0c\u652f\u6301\u4e0e\u73b0\u6709\u5de5\u5177\u76f8\u540c\u7684\u914d\u7f6e\u53c2\u6570\u3002", "result": "\u56fe\u50cf\u5bf9\u751f\u6210\u541e\u5410\u91cf\u6bd4\u73b0\u6709\u5de5\u5177\u9ad8\u51fa\u6570\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "SynthPix\u6709\u671b\u5e7f\u6cdb\u670d\u52a1\u4e8e\u6d41\u4f53\u529b\u5b66\u7814\u7a76\u793e\u533a\uff0c\u63a8\u52a8\u5feb\u901f\u6d41\u573a\u4f30\u8ba1\u4e0e\u4e3b\u52a8\u6d41\u4f53\u63a7\u5236\u53d1\u5c55\u3002"}}
{"id": "2512.09627", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09627", "abs": "https://arxiv.org/abs/2512.09627", "authors": ["Jingwei Ye", "Zhi Wang", "Chenbin Su", "Jieshuai Yang", "Jiayi Ding", "Chunbo Liu", "Ge Chu"], "title": "LogICL: Distilling LLM Reasoning to Bridge the Semantic Gap in Cross-Domain Log Anomaly Detection", "comment": null, "summary": "Effective log anomaly detection is critical to sustaining reliability in large-scale IT infrastructures. Transformer-based models require substantial resources and labeled data, exacerbating the cold-start problem in target domains where logs are scarce. Existing cross-domain methods leverage source logs but struggle with generalization due to reliance on surface lexical similarity, failing to capture latent semantic equivalence amid structural divergences. To address this, we propose LogICL, a framework distilling Large Language Model (LLM) reasoning into a lightweight encoder for cross-domain anomaly detection. During training, LogICL constructs a delta matrix measuring the utility of demonstrations selected via Maximal Marginal Relevance relative to zero-shot inference. The encoder is optimized via a multi-objective loss comprising an ICL-Guided term that aligns representations based on reasoning assistance utility, maximum mean discrepancy for domain alignment, and supervised contrastive loss. At inference, the optimized encoder retrieves reasoning-aware demonstrations using semantic similarity and delta scores, enabling frozen-LLM in-context learning with Chain-of-Thought for accurate and interpretable detection. Experiments on few-shot and zero-shot cross-domain benchmarks confirm LogICL achieves state-of-the-art performance across heterogeneous systems. Further analysis via visualizations and case studies confirms LogICL bridges the semantic gap beyond surface lexical similarity, effectively capturing latent semantic equivalence for rapid deployment.", "AI": {"tldr": "LogICL\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7f16\u7801\u5668\u5b9e\u73b0\u8de8\u57df\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\uff0c\u5728\u8d44\u6e90\u6709\u9650\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8868\u5c42\u8bcd\u6c47\u76f8\u4f3c\u6027\uff0c\u96be\u4ee5\u5e94\u5bf9\u7ed3\u6784\u5dee\u5f02\u4e0b\u7684\u8bed\u4e49\u7b49\u4ef7\u95ee\u9898\uff0c\u4e14\u51b7\u542f\u52a8\u65f6\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u6784\u5efadelta\u77e9\u9635\u8861\u91cf\u793a\u4f8b\u6548\u7528\uff0c\u7ed3\u5408\u591a\u76ee\u6807\u635f\u5931\u4f18\u5316\u7f16\u7801\u5668\uff0c\u63a8\u7406\u65f6\u5229\u7528\u8bed\u4e49\u76f8\u4f3c\u4e0edelta\u5206\u6570\u68c0\u7d22\u793a\u4f8b\uff0c\u9a71\u52a8\u51bb\u7ed3LLM\u8fdb\u884c\u601d\u7ef4\u94fe\u63a8\u7406\u3002", "result": "\u5728\u5c11\u6837\u672c\u4e0e\u96f6\u6837\u672c\u8de8\u57df\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u53ef\u89c6\u5316\u4e0e\u6848\u4f8b\u5206\u6790\u9a8c\u8bc1\u5176\u6709\u6548\u5f25\u5408\u8bed\u4e49\u9e3f\u6c9f\u3002", "conclusion": "LogICL\u663e\u8457\u63d0\u5347\u8de8\u57df\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u5feb\u901f\u90e8\u7f72\u3002"}}
{"id": "2512.09685", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09685", "abs": "https://arxiv.org/abs/2512.09685", "authors": ["Zeyu Zhang", "Haiying Shen"], "title": "Straggler Tolerant and Resilient DL Training on Homogeneous GPUs", "comment": null, "summary": "Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.", "AI": {"tldr": "STAR\u7cfb\u7edf\u901a\u8fc7\u4f18\u5316\u540c\u6b65\u6a21\u5f0f\u548c\u8d44\u6e90\u5206\u914d\uff0c\u6709\u6548\u964d\u4f4e\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684TTA\u5e76\u907f\u514d\u62d6\u5c3e\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9GPU\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e2d\u62d6\u5c3e\u95ee\u9898\u7684\u6210\u56e0\u53ca\u7f13\u89e3\u65b9\u6cd5\u6548\u679c\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002", "method": "\u63d0\u51faSTAR\u7cfb\u7edf\uff0c\u5305\u542b\u65b0\u7684\u540c\u6b65\u6a21\u5f0f\u3001\u542f\u53d1\u5f0f\u4e0e\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9009\u62e9\u6700\u4f18\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u8d44\u6e90\u91cd\u5206\u914d\u907f\u514dCPU\u4e0e\u5e26\u5bbd\u8fc7\u8f7d\u3002", "result": "\u5728AWS\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cSTAR\u5728PS\u548call-reduce\u67b6\u6784\u4e0b\u5206\u522b\u964d\u4f4eTTA 48-84%\u548c51-70%\uff0c\u540c\u65f6\u4fdd\u6301\u6536\u655b\u7cbe\u5ea6\u3002", "conclusion": "STAR\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u4e14\u5f00\u6e90\uff0c\u4e3a\u89e3\u51b3\u62d6\u5c3e\u95ee\u9898\u63d0\u4f9b\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2512.09679", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09679", "abs": "https://arxiv.org/abs/2512.09679", "authors": ["Naizhu Jin", "Zhong Li", "Guang Yang", "Tian Zhang", "Qingkai Zeng"], "title": "Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis", "comment": null, "summary": "Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \\emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86Chain-of-Thought\u63d0\u793a\u5728\u795e\u7ecf\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u7ed3\u6784\u5316CoT\u65b9\u6cd5\u5728\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u6a21\u578b\u89c4\u6a21\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u63a8\u7406\u8d28\u91cf\u662f\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u63a2\u7d22CoT\u63d0\u793a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7801\u751f\u6210\u6027\u80fd\u7684\u5185\u5728\u673a\u5236\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u7b56\u7565\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u4fe1\u606f\u8bba\u89c6\u89d2\uff08\u6761\u4ef6\u4e92\u4fe1\u606f\uff09\u8bc4\u4f30\u4e94\u79cdCoT\u8303\u5f0f\uff0c\u5728\u516d\u79cdPython\u57fa\u51c6\u300112\u79cd\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u53ca7B\u81f3480B\u53c2\u6570\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5916\u90e8\u5f15\u5bfc\u7684CoT\u4f18\u4e8e\u76f4\u63a5\u751f\u6210\uff0c\u7ed3\u6784\u5316\u65b9\u6cd5\u5e73\u5747\u63d0\u5347Pass@1\u8fbe5-12%\uff0c\u4e14\u63a8\u7406\u8d28\u91cf\u663e\u8457\u5f71\u54cd\u6548\u679c\uff0c\u8f7b\u91cf\u7ea7\u6a21\u677f\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u5e94\u6839\u636e\u6a21\u578b\u5bb9\u91cf\u3001\u8bed\u8a00\u7279\u6027\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u9009\u62e9\u5408\u9002\u7684CoT\u7b56\u7565\uff0c\u9ad8\u8d28\u91cf\u7ed3\u6784\u5316\u63a8\u7406\u6700\u5177\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.09710", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09710", "abs": "https://arxiv.org/abs/2512.09710", "authors": ["Hagit Attiya", "Panagiota Fatourou", "Eleftherios Kosmas", "Yuanhao Wei"], "title": "Recoverable Lock-Free Locks", "comment": null, "summary": "This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u540c\u65f6\u5b9e\u73b0\u65e0\u9501\u6027\u548c\u53ef\u6062\u590d\u6027\u7684\u8f6c\u6362\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u89e3\u51b3\u4f20\u7edf\u9501\u673a\u5236\u5728\u6545\u969c\u6062\u590d\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "method": "\u4ece\u57fa\u4e8e\u9501\u7684\u5b9e\u73b0\u51fa\u53d1\uff0c\u66ff\u6362\u9501\u83b7\u53d6\u4e0e\u91ca\u653e\u64cd\u4f5c\uff0c\u652f\u6301\u5d4c\u5957\u9501\u5e76\u786e\u4fdd\u53ef\u6062\u590d\u6027\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9501\u4e14\u53ef\u6062\u590d\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u672a\u5f71\u54cd\u539f\u9501\u673a\u5236\u7684\u6b63\u786e\u6027\u3002", "conclusion": "\u8be5\u8f6c\u6362\u65b9\u6cd5\u517c\u987e\u6027\u80fd\u4e0e\u5bb9\u9519\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2512.09775", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09775", "abs": "https://arxiv.org/abs/2512.09775", "authors": ["Vladimir Balditsyn", "Philippe Lalanda", "German Vega", "St\u00e9phanie Chollet"], "title": "Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition", "comment": null, "summary": "The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7cfb\u7edf\u4e2d\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u8fd0\u884c\u65f6\u8bc4\u4f30\u6a21\u578b\u9884\u6d4b\u7684\u76f8\u5173\u6027\uff0c\u4ee5\u8f85\u52a9\u9886\u57df\u4e13\u5bb6\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u9a71\u52a8\u800c\u975e\u624b\u52a8\u7f16\u7801\uff0c\u5176\u8fd0\u884c\u8fb9\u754c\u4e0d\u786e\u5b9a\u4e14\u65e0\u6cd5\u4fdd\u8bc1\u65e0\u9519\u8bef\uff0c\u56e0\u6b64\u9700\u91cf\u5316\u5176\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u7ed3\u5408\u5e76\u9002\u914d\u4e00\u7ec4\u9009\u5b9a\u6280\u672f\uff0c\u5728\u8fd0\u884c\u65f6\u8bc4\u4f30\u6a21\u578b\u9884\u6d4b\u76f8\u5173\u6027\uff0c\u5e94\u7528\u4e8e\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u9886\u57df\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8be6\u7ec6\u8ba8\u8bba\u4e86\u5bf9\u9886\u57df\u4e13\u5bb6\u7684\u8f85\u52a9\u4f5c\u7528\u3002", "conclusion": "\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u6709\u52a9\u4e8e\u63d0\u5347ML\u7cfb\u7edf\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u3002"}}
