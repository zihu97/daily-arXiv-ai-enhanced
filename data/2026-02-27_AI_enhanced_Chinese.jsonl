{"id": "2602.22229", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.22229", "abs": "https://arxiv.org/abs/2602.22229", "authors": ["Lohit Daksha", "Seyda Guzelhan", "Kaustubh Shivdikar", "Carlos Agull\u00f3 Domingo", "\u00d3scar Vera Lopez", "Gilbert Jonatan", "Hubert Dymarkowski", "Aymane El Jerari", "Jos\u00e9 Cano", "Jos\u00e9 L. Abell\u00e1n", "John Kim", "David Kaeli", "Ajay Joshi"], "title": "FHECore: Rethinking GPU Microarchitecture for Fully Homomorphic Encryption", "comment": null, "summary": "Fully Homomorphic Encryption (FHE) enables computation directly on encrypted data but incurs massive computational and memory overheads, often exceeding plaintext execution by several orders of magnitude. While custom ASIC accelerators can mitigate these costs, their long time-to-market and the rapid evolution of FHE algorithms threaten their long-term relevance. GPUs, by contrast, offer scalability, programmability, and widespread availability, making them an attractive platform for FHE. However, modern GPUs are increasingly specialized for machine learning workloads, emphasizing low-precision datatypes (e.g., INT$8$, FP$8$) that are fundamentally mismatched to the wide-precision modulo arithmetic required by FHE. Essentially, while GPUs offer ample parallelism, their functional units, like Tensor Cores, are not suited for wide-integer modulo arithmetic required by FHE schemes such as CKKS. Despite this constraint, researchers have attempted to map FHE primitives on Tensor Cores by segmenting wide integers into low-precision (INT$8$) chunks.\n  To overcome these bottlenecks, we propose FHECore, a specialized functional unit integrated directly into the GPU's Streaming Multiprocessor. Our design is motivated by a key insight: the two dominant contributors to latency$-$Number Theoretic Transform and Base Conversion$-$can be formulated as modulo-linear transformations. This allows them to be mapped on a common hardware unit that natively supports wide-precision modulo-multiply-accumulate operations. Our simulations demonstrate that FHECore reduces dynamic instruction count by a geometric mean of $2.41\\times$ for CKKS primitives and $1.96\\times$ for end-to-end workloads. These reductions translate to performance speedups of $1.57\\times$ and $2.12\\times$, respectively$-$including a $50\\%$ reduction in bootstrapping latency$-$all while inuring a modest $2.4\\%$ area overhead.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.22276", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2602.22276", "abs": "https://arxiv.org/abs/2602.22276", "authors": ["Oliver Karras", "Amirreza Alasti", "Lena John", "Sushant Aggarwal", "Y\u00fccel Celik"], "title": "EmpiRE-Compass: A Neuro-Symbolic Dashboard for Sustainable and Dynamic Knowledge Exploration, Synthesis, and Reuse", "comment": "7 pages, 1 figure, Accepted at 32nd International Working Conference on Requirements Engineering: Foundations for Software Quality", "summary": "Software engineering (SE) and requirements engineering (RE) face a significant increase in secondary studies, particularly literature reviews (LRs), due to the ever-growing number of scientific publications. Generative artificial intelligence (GenAI) exacerbates this trend by producing LRs rapidly but often at the expense of quality, rigor, and transparency. At the same time, secondary studies often fail to share underlying data and artifacts, limiting replication and reuse. This paper introduces EmpiRE-Compass, a neuro-symbolic dashboard designed to lower barriers for accessing, replicating, and reusing LR data. Its overarching goal is to demonstrate how LRs can become more sustainable by semantically structuring their underlying data in research knowledge graphs (RKGs) and by leveraging large language models (LLMs) for easy and dynamic access, replication, and reuse. Building on two RE use cases, we developed EmpiRE-Compass with a modular system design and workflows for curated and custom competency questions. The dashboard is freely available online, accompanied by a demonstration video. To manage operational costs, a limit of 25 requests per IP address per day applies to the default LLM (GPT-4o mini). All source code and documentation are released as an open-source project to foster reuse, adoption, and extension. EmpiRE-Compass provides three core capabilities: (1) Exploratory visual analytics for curated competency questions; (2) Neuro-symbolic synthesis for custom competency questions; and (3) Reusable knowledge with all queries, analyses, and results openly available. By unifying RKGs and LLMs in a neuro-symbolic dashboard, EmpiRE-Compass advances sustainable LRs in RE, SE, and beyond. It lowers technical barriers, fosters transparency and reproducibility, and enables collaborative, continuously updated, and reusable LRs", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86EmpiRE-Compass\uff0c\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u4eea\u8868\u677f\uff0c\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u5316\u7ed3\u6784\u5316\u6587\u732e\u7efc\u8ff0\u5e95\u5c42\u6570\u636e\u4e8e\u7814\u7a76\u77e5\u8bc6\u56fe\u8c31\u4e2d\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4fbf\u6377\u7684\u8bbf\u95ee\u3001\u590d\u5236\u548c\u91cd\u7528\u529f\u80fd\uff0c\u4ece\u800c\u4fc3\u8fdb\u6587\u732e\u7efc\u8ff0\u7684\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u8f6f\u4ef6\u5de5\u7a0b\u548c\u9700\u6c42\u5de5\u7a0b\u9886\u57df\u7684\u4e8c\u7ea7\u7814\u7a76\uff08\u5c24\u5176\u662f\u6587\u732e\u7efc\u8ff0\uff09\u6570\u91cf\u6fc0\u589e\uff0c\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u867d\u80fd\u5feb\u901f\u4ea7\u51fa\u6587\u732e\u7efc\u8ff0\uff0c\u4f46\u5e38\u4ee5\u727a\u7272\u8d28\u91cf\u3001\u4e25\u8c28\u6027\u548c\u900f\u660e\u5ea6\u4e3a\u4ee3\u4ef7\u3002\u540c\u65f6\uff0c\u4e8c\u7ea7\u7814\u7a76\u5f80\u5f80\u672a\u80fd\u5171\u4eab\u5e95\u5c42\u6570\u636e\u548c\u5de5\u4ef6\uff0c\u9650\u5236\u4e86\u590d\u5236\u548c\u91cd\u7528\u3002", "method": "\u5f00\u53d1EmpiRE-Compass\u795e\u7ecf\u7b26\u53f7\u4eea\u8868\u677f\uff0c\u91c7\u7528\u6a21\u5757\u5316\u7cfb\u7edf\u8bbe\u8ba1\u548c\u5de5\u4f5c\u6d41\u7a0b\uff0c\u652f\u6301\u4e13\u95e8\u5b9a\u5236\u7684\u81ea\u5b9a\u4e49\u80fd\u529b\u95ee\u9898\u3002\u8be5\u5de5\u5177\u7ed3\u5408\u7814\u7a76\u77e5\u8bc6\u56fe\u8c31(RKGs)\u548c\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\uff0c\u63d0\u4f9b\u63a2\u7d22\u6027\u53ef\u89c6\u5206\u6790\u3001\u795e\u7ecf\u7b26\u53f7\u5408\u6210\u548c\u53ef\u91cd\u7528\u77e5\u8bc6\u4e09\u5927\u6838\u5fc3\u529f\u80fd\u3002", "result": "EmpiRE-Compass\u5df2\u5728\u7ebf\u514d\u8d39\u63d0\u4f9b\uff0c\u914d\u6709\u6f14\u793a\u89c6\u9891\uff0c\u6e90\u4ee3\u7801\u548c\u6587\u6863\u4f5c\u4e3a\u5f00\u6e90\u9879\u76ee\u53d1\u5e03\u3002\u9ed8\u8ba4LLM(GPT-4o mini)\u9650\u5236\u6bcf\u4e2aIP\u5730\u5740\u6bcf\u592925\u6b21\u8bf7\u6c42\u4ee5\u63a7\u5236\u8fd0\u8425\u6210\u672c\u3002\u8be5\u5de5\u5177\u901a\u8fc7\u7edf\u4e00RKGs\u548cLLMs\uff0c\u964d\u4f4e\u4e86\u6280\u672f\u58c1\u5792\uff0c\u4fc3\u8fdb\u4e86\u900f\u660e\u5ea6\u548c\u53ef\u590d\u73b0\u6027\u3002", " conclusion": "EmpiRE-Compass\u63a8\u8fdb\u4e86\u9700\u6c42\u5de5\u7a0b\u3001\u8f6f\u4ef6\u5de5\u7a0b\u7b49\u9886\u57df\u6587\u732e\u7efc\u8ff0\u7684\u53ef\u6301\u7eed\u6027\uff0c\u5b9e\u73b0\u4e86\u534f\u4f5c\u3001\u6301\u7eed\u66f4\u65b0\u548c\u53ef\u91cd\u7528\u7684\u6587\u732e\u7efc\u8ff0\uff0c\u4e3a\u4e8c\u7ea7\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.22350", "categories": ["cs.DC", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.22350", "abs": "https://arxiv.org/abs/2602.22350", "authors": ["Paul Borrill"], "title": "Engineered Simultaneity: The Physical Impossibility of Consolidated Price Discovery Across Spacelike-Separated Exchanges", "comment": "8 pages, 2 figures, 2 tables", "summary": "We introduce the concept of engineered simultaneity: a system design that (1) requires comparing events at spacelike-separated locations, (2) implements this comparison via an implicit simultaneity convention, and (3) represents the result as objective rather than conventional. The United States National Best Bid and Offer (NBBO), mandated by SEC Regulation NMS Rule 611, is shown to be an instance of engineered simultaneity. We prove that the NBBO is frame-dependent: its value depends on the reference frame in which \"current\" prices are defined. Since the exchanges that generate quote data are separated by distances of 43-1,180 km, light-travel times of 143-3,940 microseconds create unavoidable windows during which no frame-independent price ordering exists. High-frequency trading firms exploit this window by accessing exchange data via direct feeds (latency ~tens of microseconds) while the consolidated Securities Information Processor operates at ~1,128 microseconds -- a ratio exceeding 50:1. We demonstrate that this constitutes a category mistake in the sense of Ryle: the NBBO applies the concept of \"simultaneity\" in a domain where it has no frame-independent meaning. The resulting information asymmetry extracts approximately $5 billion annually from other market participants.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f15\u5165'\u5de5\u7a0b\u5316\u540c\u65f6\u6027'\u6982\u5ff5\uff0c\u6307\u51fa\u7f8e\u56fd\u5168\u56fd\u6700\u4f73\u4e70\u5356\u62a5\u4ef7(NBBO)\u662f\u5178\u578b\u6848\u4f8b\uff0c\u7531\u4e8e\u4ea4\u6613\u6240\u95f4\u8ddd\u79bb\u9020\u6210\u7684\u5149\u4f20\u64ad\u5ef6\u8fdf(143-3,940\u5fae\u79d2)\uff0cNBBO\u5177\u6709\u53c2\u8003\u7cfb\u4f9d\u8d56\u6027\uff0c\u9ad8\u9891\u4ea4\u6613\u516c\u53f8\u5229\u7528\u6570\u636e\u8bbf\u95ee\u65f6\u5dee(50:1\u4f18\u52bf)\u6bcf\u5e74\u83b7\u5229\u7ea650\u4ebf\u7f8e\u5143\u3002", "motivation": "\u7814\u7a76\u91d1\u878d\u5e02\u573a\u4e2d\u540c\u65f6\u6027\u6982\u5ff5\u7684\u5e94\u7528\u95ee\u9898\uff0c\u63ed\u793aNBBO\u7cfb\u7edf\u56e0\u5ffd\u7565\u76f8\u5bf9\u8bba\u6548\u5e94\u800c\u5b58\u5728\u7684\u7ed3\u6784\u7f3a\u9677\u3002", "method": "\u901a\u8fc7\u5206\u6790NBBO\u7cfb\u7edf\u7684\u7269\u7406\u7ea6\u675f(\u4ea4\u6613\u6240\u95f4\u8ddd\u3001\u5149\u4f20\u64ad\u5ef6\u8fdf)\uff0c\u6bd4\u8f83\u4e0d\u540c\u6570\u636e\u8bbf\u95ee\u6e20\u9053\u7684\u65f6\u5ef6\u5dee\u5f02\uff0c\u5e94\u7528\u8d56\u5c14\u7684\u8303\u7574\u9519\u8bef\u7406\u8bba\u8fdb\u884c\u6982\u5ff5\u5206\u6790\u3002", "result": "\u8bc1\u660eNBBO\u662f\u53c2\u8003\u7cfb\u4f9d\u8d56\u7684\uff0c\u4e0d\u5b58\u5728\u6846\u67b6\u65e0\u5173\u7684\u4ef7\u683c\u6392\u5e8f\uff0c\u9ad8\u9891\u4ea4\u6613\u516c\u53f8\u5229\u7528\u8fd9\u4e00\u73b0\u8c61\u521b\u9020\u4fe1\u606f\u4e0d\u5bf9\u79f0\uff0c\u5e74\u83b7\u5229\u7ea650\u4ebf\u7f8e\u5143\u3002", "conclusion": "NBBO\u7cfb\u7edf\u5728\u540c\u65f6\u6027\u6982\u5ff5\u7684\u5e94\u7528\u4e0a\u5b58\u5728\u8303\u7574\u9519\u8bef\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u7f3a\u9677\u5bfc\u81f4\u5e02\u573a\u7ed3\u6784\u4e0d\u5e73\u7b49\uff0c\u9700\u8981\u91cd\u65b0\u8003\u8651\u91d1\u878d\u5e02\u573a\u7684\u540c\u65f6\u6027\u5b9a\u4e49\u548c\u6570\u636e\u8bbf\u95ee\u89c4\u5219\u3002"}}
{"id": "2602.22292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.22292", "abs": "https://arxiv.org/abs/2602.22292", "authors": ["Oliver Karras"], "title": "The Ethos of the PEERfect REVIEWer: Scientific Care and Collegial Welfare", "comment": "13 pages, Accepted at the 32nd International Working Conference on Requirements Engineering: Foundations for Software Quality", "summary": "Peer review remains a cornerstone in academia, yet it frequently falls short in fostering joint progress and well-being. While peer review primarily emphasizes scientific rigor, it often lacks the empathy essential for supporting and encouraging all peers involved. In this experience report, I aim to highlight that peer review is a practice that demands both scientific care for quality and collegial welfare for the joint progress and well-being of all peers involved, including authors, co-reviewers, workshop or conference organizers, and journal editors. Drawing on my ten years of experience in academia, I propose the ethos of the PEERfect REVIEWer, grounded in the two core values: Scientific care and collegial welfare. Through reflection shaped by professional exchanges with colleagues, consideration of literature, and an examination of both self-authored and received reviews, I formulated an accompanying guideline with 16 practical recommendations to guide reviewers in their actions to achieve these two values. The ethos of the PEERfect REVIEWer and its accompanying guideline help reviewers in upholding high scientific standards and conducting peer review in a constructive, supportive, respectful, and timely manner. They demonstrate that scientific rigor and empathy are complementary forces that promote impactful peer review practice. By placing scientific care and collegial welfare at the core of peer review, this experience report reaffirms the importance of scientific rigor while also advocating for greater attention to empathy. It invites reviewers to reconsider their role not merely as gatekeepers but as partners in the academic journey of each peer involved. The PEERfect REVIEWer is both a caretaker of quality and a steward of joint progress and well-being - as truly impactful peer review practice requires scientific rigor and empathy in equal measure.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa'PEERfect REVIEWer'\u7406\u5ff5\uff0c\u5f3a\u8c03\u540c\u884c\u8bc4\u8bae\u5e94\u517c\u987e\u79d1\u5b66\u4e25\u8c28\u6027\u548c\u540c\u884c\u5173\u6000\uff0c\u901a\u8fc716\u6761\u5b9e\u7528\u6307\u5357\u5e2e\u52a9\u5ba1\u7a3f\u4eba\u4ee5\u5efa\u8bbe\u6027\u3001\u652f\u6301\u6027\u7684\u65b9\u5f0f\u8fdb\u884c\u8bc4\u8bae\u3002", "motivation": "\u4f20\u7edf\u540c\u884c\u8bc4\u8bae\u8fc7\u4e8e\u5f3a\u8c03\u79d1\u5b66\u4e25\u8c28\u6027\uff0c\u800c\u7f3a\u4e4f\u652f\u6301\u6240\u6709\u53c2\u4e0e\u8005\uff08\u4f5c\u8005\u3001\u5171\u540c\u5ba1\u7a3f\u4eba\u3001\u7ec4\u7ec7\u8005\u548c\u7f16\u8f91\uff09\u7684\u540c\u7406\u5fc3\uff0c\u4e0d\u5229\u4e8e\u5b66\u672f\u754c\u7684\u5171\u540c\u8fdb\u6b65\u548c\u798f\u7949\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u5341\u5e74\u5b66\u672f\u7ecf\u9a8c\u3001\u4e0e\u540c\u4e8b\u7684\u4e13\u4e1a\u4ea4\u6d41\u3001\u6587\u732e\u7814\u7a76\u4ee5\u53ca\u5bf9\u81ea\u8eab\u64b0\u5199\u548c\u6536\u5230\u7684\u8bc4\u8bae\u7684\u5206\u6790\uff0c\u63d0\u70bc\u51fa'PEERfect REVIEWer'\u7406\u5ff5\u53ca\u5176\u6307\u5bfc\u65b9\u9488\u3002", "result": "PEERfect REVIEWer\u7406\u5ff5\u5e2e\u52a9\u5ba1\u7a3f\u4eba\u4fdd\u6301\u9ad8\u79d1\u5b66\u6807\u51c6\uff0c\u540c\u65f6\u4ee5\u5efa\u8bbe\u6027\u3001\u652f\u6301\u6027\u3001\u5c0a\u91cd\u6027\u548c\u53ca\u65f6\u6027\u7684\u65b9\u5f0f\u8fdb\u884c\u540c\u884c\u8bc4\u8bae\uff0c\u8bc1\u660e\u79d1\u5b66\u4e25\u8c28\u6027\u548c\u540c\u7406\u5fc3\u662f\u4e92\u8865\u529b\u91cf\u3002", "conclusion": "\u540c\u884c\u8bc4\u8bae\u8005\u5e94\u91cd\u65b0\u5ba1\u89c6\u81ea\u5df1\u7684\u89d2\u8272\uff0c\u4e0d\u4ec5\u662f\u8d28\u91cf\u628a\u5173\u8005\uff0c\u66f4\u662f\u6bcf\u4f4d\u540c\u884c\u5b66\u672f\u65c5\u7a0b\u4e2d\u7684\u4f19\u4f34\uff0c\u79d1\u5b66\u4e25\u8c28\u6027\u548c\u540c\u7406\u5fc3\u540c\u7b49\u91cd\u8981\uff0c\u5171\u540c\u4fc3\u8fdb\u771f\u6b63\u6709\u5f71\u54cd\u529b\u7684\u540c\u884c\u8bc4\u8bae\u5b9e\u8df5\u3002"}}
{"id": "2602.22392", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.22392", "abs": "https://arxiv.org/abs/2602.22392", "authors": ["Md Hasanur Rashid", "Xinyi Li", "Youbiao He", "Forrest Sheng Bao", "Dong Dai"], "title": "DIAL: Decentralized I/O AutoTuning via Learned Client-side Local Metrics for Parallel File System", "comment": null, "summary": "Enabling efficient, high-performance data access in parallel file systems (PFS) is critical for today's high-performance computing systems. PFS client-side I/O heavily impacts the final I/O performance delivered to individual applications and the entire system. Autotuning the key client-side I/O behaviors has been extensively studied and shows promising results. However, existing work has heavily relied on extensive number of global runtime metrics to monitor and accurate modeling of applications' I/O patterns. Such heavy overheads significantly limit the ability to enable fine-grained, dynamic tuning in practical systems. In this study, we propose DIAL (Decentralized I/O AutoTuning via Learned Client-side Local Metrics) which takes a drastically different approach. Instead of trying to extract the global I/O patterns of applications, DIAL takes a decentralized approach, treating each I/O client as an independent unit and tuning configurations using only its locally observable metrics. With the help of machine learning models, DIAL enables multiple tunable units to make independent but collective decisions, reacting to what is happening in the global storage systems in a timely manner and achieving better I/O performance globally for the application.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDIAL\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u672c\u5730\u6307\u6807\u548c\u673a\u5668\u5b66\u4e60\uff0c\u5b9e\u73b0\u5e76\u884c\u6587\u4ef6\u7cfb\u7edf\u7684\u53bb\u4e2d\u5fc3\u5316I/O\u81ea\u52a8\u8c03\u4f18\uff0c\u907f\u514d\u4e86\u5168\u5c40\u76d1\u63a7\u7684\u5f00\u9500\uff0c\u5b9e\u73b0\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u52a8\u6001\u8c03\u4f18\u3002", "motivation": "\u73b0\u6709\u7684\u5e76\u884c\u6587\u4ef6\u7cfb\u7edfI/O\u8c03\u4f18\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u5168\u5c40\u8fd0\u884c\u65f6\u6307\u6807\u548c\u590d\u6742\u7684\u5e94\u7528I/O\u6a21\u5f0f\u5efa\u6a21\uff0c\u5bfc\u81f4\u5f00\u9500\u8fc7\u5927\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7ec6\u7c92\u5ea6\u52a8\u6001\u8c03\u4f18\u7684\u5b9e\u73b0\u3002", "method": "DIAL\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\uff0c\u5c06\u6bcf\u4e2aI/O\u5ba2\u6237\u7aef\u89c6\u4e3a\u72ec\u7acb\u5355\u5143\uff0c\u4ec5\u4f7f\u7528\u53ef\u89c2\u6d4b\u7684\u672c\u5730\u6307\u6807\u8fdb\u884c\u914d\u7f6e\u8c03\u4f18\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f7f\u591a\u4e2a\u53ef\u8c03\u5355\u5143\u80fd\u591f\u505a\u51fa\u72ec\u7acb\u4f46\u534f\u540c\u7684\u51b3\u7b56\uff0c\u53ca\u65f6\u54cd\u5e94\u5168\u5c40\u5b58\u50a8\u7cfb\u7edf\u72b6\u6001\u53d8\u5316\u3002", "result": "DIAL\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5168\u5c40\u6307\u6807\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u5e94\u7528\u7684\u5168\u5c40I/O\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u51cf\u5c11\u76d1\u63a7\u5f00\u9500\uff0c\u652f\u6301\u66f4\u7ec6\u7c92\u5ea6\u548c\u52a8\u6001\u7684\u8c03\u4f18\u3002", "conclusion": "DIAL\u4e3a\u5e76\u884c\u6587\u4ef6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684I/O\u8c03\u4f18\u65b9\u6cd5\uff0c\u901a\u8fc7\u672c\u5730\u6307\u6807\u548c\u673a\u5668\u5b66\u4e60\u5b9e\u73b0\u4e86\u5168\u5c40\u6027\u80fd\u4f18\u5316\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u8c03\u4f18\u65b9\u6cd5\u7684\u5f00\u9500\u95ee\u9898\u3002"}}
{"id": "2602.22368", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22368", "abs": "https://arxiv.org/abs/2602.22368", "authors": ["Jiahao Zhang", "Yifan Zhang", "Kevin Leach", "Yu Huang"], "title": "EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization", "comment": "Accepted at the 34th IEEE/ACM International Conference on Program Comprehension (ICPC 2026), April 12-13, 2026, Rio de Janeiro, Brazil", "summary": "Code summarization is the task of generating natural language descriptions of source code, which is critical for software comprehension and maintenance. While large language models (LLMs) have achieved remarkable progress on this task, an open question remains: can human expertise in code understanding further guide and enhance these models? We propose EyeLayer, a lightweight attention-augmentation module that incorporates human eye-gaze patterns, as a proxy of human expertise, into LLM-based code summarization. EyeLayer models human attention during code reading via a Multimodal Gaussian Mixture, redistributing token embeddings based on learned parameters (\u03bc_i, \u03c3_i^2) that capture where and how intensively developers focus. This design enables learning generalizable attention priors from eye-tracking data and incorporating them into LLMs seamlessly, without disturbing existing representations. We evaluate EyeLayer across diverse model families (i.e., LLaMA-3.2, Qwen3, and CodeBERT) covering different scales and architectures. EyeLayer consistently outperforms strong fine-tuning baselines across standard metrics, achieving gains of up to 13.17% on BLEU-4. These results demonstrate that human gaze patterns encode complementary attention signals that enhance the semantic focus of LLMs and transfer effectively across diverse models for code summarization.", "AI": {"tldr": "EyeLayer\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u878d\u5165\u4eba\u7c7b\u4e13\u5bb6\u7684\u773c\u52a8\u6a21\u5f0f\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u6458\u8981\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u4eba\u7c7b\u5728\u4ee3\u7801\u7406\u89e3\u65b9\u9762\u7684\u4e13\u4e1a\u77e5\u8bc6\u662f\u5426\u80fd\u591f\u6307\u5bfc\u548c\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "EyeLayer\u4f7f\u7528\u591a\u6a21\u6001\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5efa\u6a21\u4eba\u7c7b\u9605\u8bfb\u4ee3\u7801\u65f6\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u6839\u636e\u5b66\u4e60\u5230\u7684\u53c2\u6570(\u03bc_i, \u03c3_i^2)\u91cd\u65b0\u5206\u5e03token\u5d4c\u5165\uff0c\u6355\u83b7\u5f00\u53d1\u8005\u5173\u6ce8\u7684\u4f4d\u7f6e\u548c\u5f3a\u5ea6\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784(\u5982LLaMA-3.2\u3001Qwen3\u548cCodeBERT)\u4e0a\uff0cEyeLayer consistently\u8d85\u8d8a\u5f3a\u5fae\u8c03\u57fa\u7ebf\uff0c\u5728BLEU-4\u6307\u6807\u4e0a\u6700\u9ad8\u63d0\u534713.17%\u3002", "conclusion": "\u4eba\u7c7b\u773c\u52a8\u6a21\u5f0f\u7f16\u7801\u4e86\u4e92\u8865\u7684\u6ce8\u610f\u529b\u4fe1\u53f7\uff0c\u80fd\u591f\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u805a\u7126\u80fd\u529b\uff0c\u5e76\u6709\u6548\u8fc1\u79fb\u5230\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u7684\u4ee3\u7801\u6458\u8981\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2602.22409", "categories": ["cs.DC", "cs.PF", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.22409", "abs": "https://arxiv.org/abs/2602.22409", "authors": ["Md Hasanur Rashid", "Dong Dai"], "title": "AdapTBF: Decentralized Bandwidth Control via Adaptive Token Borrowing for HPC Storage", "comment": null, "summary": "Modern high-performance computing (HPC) applications run on compute resources but share global storage systems. This design can cause problems when applications consume a disproportionate amount of storage bandwidth relative to their allocated compute resources. For example, an application running on a single compute node can issue many small, random writes and consume excessive I/O bandwidth from a storage server. This can hinder larger jobs that write to the same storage server and are allocated many compute nodes, resulting in significant resource waste.\n  A straightforward solution is to limit each application's I/O bandwidth on storage servers in proportion to its allocated compute resources. This approach has been implemented in parallel file systems using Token Bucket Filter (TBF). However, strict proportional limits often reduce overall I/O efficiency because HPC applications generate short, bursty I/O. Limiting bandwidth can waste server capacity when applications are idle or prevent applications from temporarily using higher bandwidth during bursty phases.\n  We argue that I/O control should maximize per-application performance and overall storage efficiency while ensuring fairness (e.g., preventing small jobs from blocking large-scale ones). We propose AdapTBF, which builds on TBF in modern parallel file systems (e.g., Lustre) and introduces a decentralized bandwidth control approach using adaptive borrowing and lending. We detail the algorithm, implement AdapTBF in Lustre, and evaluate it using synthetic workloads modeled after real-world scenarios. Results show that AdapTBF manages I/O bandwidth effectively while maintaining high storage utilization, even under extreme conditions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faAdapTBF\uff0c\u4e00\u79cd\u5728HPC\u73af\u5883\u4e2d\u6539\u8fdbI/O\u5e26\u5bbd\u63a7\u5236\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u501f\u7528\u548c\u501f\u51fa\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u516c\u5e73\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u5b58\u50a8\u6548\u7387\u3002", "motivation": "\u73b0\u4ee3HPC\u5e94\u7528\u5171\u4eab\u5168\u5c40\u5b58\u50a8\u7cfb\u7edf\u65f6\uff0c\u67d0\u4e9b\u5e94\u7528\u53ef\u80fd\u6d88\u8017\u4e0e\u5176\u8ba1\u7b97\u8d44\u6e90\u4e0d\u6210\u6bd4\u4f8b\u7684\u5b58\u50a8\u5e26\u5bbd\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\uff0c\u800c\u4f20\u7edf\u7684\u56fa\u5b9a\u6bd4\u4f8b\u9650\u5236(TBF)\u65e0\u6cd5\u6709\u6548\u5904\u7406\u7a81\u53d1I/O\u6a21\u5f0f\u3002", "method": "\u57fa\u4e8eLustre\u5e76\u884c\u6587\u4ef6\u7cfb\u7edf\u5b9e\u73b0AdapTBF\uff0c\u91c7\u7528\u5206\u6563\u5f0f\u5e26\u5bbd\u63a7\u5236\u65b9\u6cd5\uff0c\u5141\u8bb8\u5e94\u7528\u6839\u636e\u9700\u6c42\u52a8\u6001\u501f\u7528\u548c\u501f\u51fa\u5e26\u5bbd\u8d44\u6e90\uff0c\u800c\u975e\u4e25\u683c\u6309\u6bd4\u4f8b\u9650\u5236\u3002", "result": "AdapTBF\u6709\u6548\u7ba1\u7406\u4e86I/O\u5e26\u5bbd\u5206\u914d\uff0c\u5373\u4f7f\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u4e5f\u80fd\u4fdd\u6301\u9ad8\u5b58\u50a8\u5229\u7528\u7387\uff0c\u540c\u65f6\u786e\u4fdd\u5e94\u7528\u95f4\u7684\u516c\u5e73\u6027\uff0c\u63d0\u9ad8\u4e86\u6574\u4f53\u7cfb\u7edf\u6548\u7387\u3002", "conclusion": "AdapTBF\u901a\u8fc7\u81ea\u9002\u5e94\u5e26\u5bbd\u63a7\u5236\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edfI/O\u9650\u5236\u65b9\u6cd5\u5728\u5904\u7406\u7a81\u53d1I/O\u65f6\u7684\u6548\u7387\u95ee\u9898\uff0c\u4e3aHPC\u73af\u5883\u4e2d\u7684\u5b58\u50a8\u8d44\u6e90\u5171\u4eab\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22402", "categories": ["cs.SE", "cs.AI", "cs.HC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2602.22402", "abs": "https://arxiv.org/abs/2602.22402", "authors": ["Cosmo Santoni"], "title": "Contextual Memory Virtualisation: DAG-Based State Management and Structurally Lossless Trimming for LLM Agents", "comment": "11 pages. 6 figures. Introduces a DAG-based state management system for LLM agents. Evaluation on 76 coding sessions shows up to 86% token reduction (mean 20%) while remaining economically viable under prompt caching. Includes reference implementation for Claude Code", "summary": "As large language models engage in extended reasoning tasks, they accumulate significant state -- architectural mappings, trade-off decisions, codebase conventions -- within the context window. This understanding is lost when sessions reach context limits and undergo lossy compaction. We propose Contextual Memory Virtualisation (CMV), a system that treats accumulated LLM understanding as version-controlled state. Borrowing from operating system virtual memory, CMV models session history as a Directed Acyclic Graph (DAG) with formally defined snapshot, branch, and trim primitives that enable context reuse across independent parallel sessions. We introduce a three-pass structurally lossless trimming algorithm that preserves every user message and assistant response verbatim while reducing token counts by a mean of 20% and up to 86% for sessions with significant overhead by stripping mechanical bloat such as raw tool outputs, base64 images, and metadata. A single-user case-study evaluation across 76 real-world coding sessions demonstrates that trimming remains economically viable under prompt caching, with the strongest gains in mixed tool-use sessions, which average 39% reduction and reach break-even within 10 turns. A reference implementation is available at https://github.com/CosmoNaught/claude-code-cmv.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCMV\u7cfb\u7edf\uff0c\u901a\u8fc7\u865a\u62df\u5316\u4e0a\u4e0b\u6587\u5185\u5b58\u5e76\u91c7\u7528\u4e09\u9636\u6bb5\u65e0\u635f\u4fee\u526a\u7b97\u6cd5\uff0c\u5c06LLM\u4f1a\u8bdd\u5efa\u6a21\u4e3aDAG\uff0c\u5728\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u7684\u540c\u65f6\u5e73\u5747\u51cf\u5c1120%\u7684token\u4f7f\u7528\u91cf\uff0c\u6700\u591a\u51cf\u5c1186%\uff0c\u663e\u8457\u63d0\u9ad8\u957f\u5bf9\u8bdd\u7684\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002", "motivation": "LLM\u5728\u957f\u63a8\u7406\u4efb\u52a1\u4e2d\u79ef\u7d2f\u7684\u91cd\u8981\u72b6\u6001\u4fe1\u606f\u56e0\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u800c\u5728\u6709\u635f\u538b\u7f29\u4e2d\u4e22\u5931\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u91cd\u7528\u548c\u7ba1\u7406\u8fd9\u4e9b\u79ef\u7d2f\u7684\u7406\u89e3\uff0c\u5f71\u54cd\u957f\u671f\u63a8\u7406\u4efb\u52a1\u7684\u6548\u679c\u548c\u6210\u672c\u3002", "method": "CMV\u7cfb\u7edf\u501f\u9274\u64cd\u4f5c\u7cfb\u7edf\u865a\u62df\u5185\u5b58\u6982\u5ff5\uff0c\u5c06\u4f1a\u8bdd\u5386\u53f2\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe(DAG)\uff0c\u5b9e\u73b0\u7248\u672c\u63a7\u5236\u7684\u72b6\u6001\u7ba1\u7406\u3002\u91c7\u7528\u4e09\u9636\u6bb5\u7ed3\u6784\u65e0\u635f\u4fee\u526a\u7b97\u6cd5\uff0c\u4fdd\u7559\u7528\u6237\u6d88\u606f\u548c\u52a9\u624b\u54cd\u5e94\u539f\u6587\uff0c\u540c\u65f6\u53bb\u9664\u673a\u68b0\u6027\u81a8\u80c0\u5185\u5bb9\u3002", "result": "\u572876\u4e2a\u771f\u5b9e\u7f16\u7801\u4f1a\u8bdd\u8bc4\u4f30\u4e2d\uff0c\u7b97\u6cd5\u5e73\u5747\u51cf\u5c1120%\u7684token\u4f7f\u7528\u91cf\uff0c\u6700\u591a\u51cf\u5c1186%\u3002\u6df7\u5408\u5de5\u5177\u4f7f\u7528\u4f1a\u8bdd\u4e2d\u6548\u679c\u6700\u4f73\uff0c\u5e73\u5747\u51cf\u5c1139%\uff0c10\u8f6e\u5185\u8fbe\u5230\u6536\u652f\u5e73\u8861\u3002\u5728\u63d0\u793a\u7f13\u5b58\u4e0b\u4fdd\u6301\u7ecf\u6d4e\u53ef\u884c\u6027\u3002", "conclusion": "CMV\u7cfb\u7edf\u901a\u8fc7\u865a\u62df\u5316\u4e0a\u4e0b\u6587\u5185\u5b58\u548c\u7ed3\u6784\u65e0\u635f\u4fee\u526a\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u957f\u4f1a\u8bdd\u4e2d\u7684\u72b6\u6001\u7ba1\u7406\u548ctoken\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6df7\u5408\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22423", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.22423", "abs": "https://arxiv.org/abs/2602.22423", "authors": ["Md Hasanur Rashid", "Nathan R. Tallent", "Forrest Sheng Bao", "Dong Dai"], "title": "CARAT: Client-Side Adaptive RPC and Cache Co-Tuning for Parallel File Systems", "comment": "to be published in 40th IEEE International Parallel & Distributed Processing Symposium (IPDPS), 2026", "summary": "Tuning parallel file system in High-Performance Computing (HPC) systems remains challenging due to the complex I/O paths, diverse I/O patterns, and dynamic system conditions. While existing autotuning frameworks have shown promising results in tuning PFS parameters based on applications' I/O patterns, they lack scalability, adaptivity, and the ability to operate online. In this work, focusing on scalable online tuning, we present CARAT, an ML-guided framework to co-tune client-side RPC and caching parameters of PFS, leveraging only locally observable metrics. Unlike global or pattern-dependent approaches, CARAT enables each client to make independent and intelligent tuning decisions online, responding to real-time changes in both application I/O behaviors and system states. We then prototyped CARAT using Lustre and evaluated it extensively across dynamic I/O patterns, real-world HPC workloads, and multi-client deployments. The results demonstrated that CARAT can achieve up to 3x performance improvement over the default or static configurations, validating the effectiveness and generality of our approach. Due to its scalability and lightweight, we believe CARAT has the potential to be widely deployed into existing PFS and benefit various data-intensive applications.", "AI": {"tldr": "CARAT\u662f\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5bf9\u5e76\u884c\u6587\u4ef6\u7cfb\u7edf\u53c2\u6570\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u5728\u7ebf\u8c03\u4f18\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe3\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u5e76\u884c\u6587\u4ef6\u7cfb\u7edf\u81ea\u52a8\u8c03\u4f18\u6846\u67b6\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u3001\u81ea\u9002\u5e94\u80fd\u529b\u548c\u5728\u7ebf\u64cd\u4f5c\u80fd\u529b\u3002", "method": "CARAT\u5229\u7528\u672c\u5730\u53ef\u89c2\u6d4b\u6307\u6807\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u534f\u540c\u8c03\u4f18\u5ba2\u6237\u7aefRPC\u548c\u7f13\u5b58\u53c2\u6570\uff0c\u4f7f\u6bcf\u4e2a\u5ba2\u6237\u7aef\u80fd\u591f\u72ec\u7acb\u505a\u51fa\u667a\u80fd\u8c03\u4f18\u51b3\u7b56\u3002", "result": "\u5728\u52a8\u6001I/O\u6a21\u5f0f\u3001\u771f\u5b9eHPC\u5de5\u4f5c\u8d1f\u8f7d\u548c\u591a\u5ba2\u6237\u7aef\u90e8\u7f72\u7684\u8bc4\u4f30\u4e2d\uff0cCARAT\u6bd4\u9ed8\u8ba4\u6216\u9759\u6001\u914d\u7f6e\u5b9e\u73b0\u4e86\u9ad8\u8fbe3\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CARAT\u7684\u53ef\u6269\u5c55\u6027\u548c\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u4f7f\u5176\u9002\u5408\u5728\u73b0\u6709\u5e76\u884c\u6587\u4ef6\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u90e8\u7f72\uff0c\u4e3a\u5404\u79cd\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\u7a0b\u5e8f\u5e26\u6765\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.22403", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.22403", "abs": "https://arxiv.org/abs/2602.22403", "authors": ["Saumendu Roy", "Banani Roy", "Chanchal Roy", "Richard Bassey"], "title": "XMENTOR: A Rank-Aware Aggregation Approach for Human-Centered Explainable AI in Just-in-Time Software Defect Prediction", "comment": "10 pages, 14 figures, conference", "summary": "Machine learning (ML)-based defect prediction models can improve software quality. However, their opaque reasoning creates an HCI challenge because developers struggle to trust models they cannot interpret. Explainable AI (XAI) methods such as LIME, SHAP, and BreakDown aim to provide transparency, but when used together, they often produce conflicting explanations that increase confusion, frustration, and cognitive load. To address this usability challenge, we introduce XMENTOR, a human-centered, rank-aware aggregation method implemented as a VS Code plugin. XMENTOR unifies multiple post-hoc explanations into a single, coherent view by applying adaptive thresholding, rank and sign agreement, and fallback strategies to preserve clarity without overwhelming users. In a user study, nearly 90% of the participants preferred aggregated explanations, citing reduced confusion and stronger support for daily tasks of debugging and review of defects. Our findings show how combining explanations and embedding them into developer workflows can enhance interpretability, usability, and trust.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86XMENTOR\uff0c\u4e00\u79cd\u7edf\u4e00\u591a\u79cdXAI\u65b9\u6cd5\u51b2\u7a81\u89e3\u91ca\u7684VS Code\u63d2\u4ef6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u3001\u6392\u540d\u548c\u7b26\u53f7\u4e00\u81f4\u6027\u7b56\u7565\u63d0\u4f9b\u5355\u4e00\u8fde\u8d2f\u89c6\u56fe\u3002\u7528\u6237\u7814\u7a76\u663e\u793a90%\u53c2\u4e0e\u8005\u504f\u597d\u805a\u5408\u89e3\u91ca\uff0c\u56e0\u5176\u51cf\u5c11\u4e86\u6df7\u6dc6\u5e76\u589e\u5f3a\u4e86\u8c03\u8bd5\u652f\u6301\u3002", "motivation": "ML\u7f3a\u9677\u9884\u6d4b\u6a21\u578b\u63d0\u9ad8\u8f6f\u4ef6\u8d28\u91cf\uff0c\u4f46 opaque reasoning \u521b\u9020HCI\u6311\u6218\uff0c\u5f00\u53d1\u8005\u96be\u4ee5\u4fe1\u4efb\u4e0d\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u3002\u73b0\u6709XAI\u65b9\u6cd5(LIME\u3001SHAP\u3001BreakDown)\u4e00\u8d77\u4f7f\u7528\u65f6\u4ea7\u751f\u51b2\u7a81\u89e3\u91ca\uff0c\u589e\u52a0\u6df7\u6dc6\u3001\u632b\u8d25\u611f\u548c\u8ba4\u77e5\u8d1f\u8377\u3002", "method": "\u63d0\u51faXMENTOR\uff0c\u4e00\u79cd\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u3001\u6392\u540d\u611f\u77e5\u7684\u805a\u5408\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e3aVS Code\u63d2\u4ef6\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u3001\u6392\u540d\u548c\u7b26\u53f7\u4e00\u81f4\u6027\u4ee5\u53ca\u5907\u7528\u7b56\u7565\uff0c\u5c06\u591a\u4e2a\u540e\u9a8c\u89e3\u91ca\u7edf\u4e00\u4e3a\u5355\u4e00\u8fde\u8d2f\u89c6\u56fe\uff0c\u4fdd\u6301\u6e05\u6670\u5ea6\u800c\u4e0d\u4f7f\u7528\u6237\u4e0d\u582a\u91cd\u8d1f\u3002", "result": "\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u8fd190%\u53c2\u4e0e\u8005\u66f4\u559c\u6b22\u805a\u5408\u89e3\u91ca\uff0c\u6307\u51fa\u5176\u51cf\u5c11\u6df7\u6dc6\uff0c\u5e76\u4e3a\u8c03\u8bd5\u548c\u7f3a\u9677\u5ba1\u67e5\u7684\u65e5\u5e38\u4efb\u52a1\u63d0\u4f9b\u66f4\u5f3a\u652f\u6301\u3002", "conclusion": "\u7ed3\u5408\u591a\u79cd\u89e3\u91ca\u5e76\u5c06\u5176\u5d4c\u5165\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u7a0b\u53ef\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u7528\u6027\u548c\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2602.22434", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22434", "abs": "https://arxiv.org/abs/2602.22434", "authors": ["Alex Aizman", "Abhishek Gaikwad", "Piotr \u017belasko"], "title": "GetBatch: Distributed Multi-Object Retrieval for ML Data Loading", "comment": "11 pages, 3 figures, 2 tables. Preprint", "summary": "Machine learning training pipelines consume data in batches. A single training step may require thousands of samples drawn from shards distributed across a storage cluster. Issuing thousands of individual GET requests incurs per-request overhead that often dominates data transfer time. To solve this problem, we introduce GetBatch - a new object store API that elevates batch retrieval to a first-class storage operation, replacing independent GET operations with a single deterministic, fault-tolerant streaming execution. GetBatch achieves up to 15x throughput improvement for small objects and, in a production training workload, reduces P95 batch retrieval latency by 2x and P99 per-object tail latency by 3.7x compared to individual GET requests.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGetBatch API\uff0c\u5c06\u6279\u91cf\u6570\u636e\u68c0\u7d22\u63d0\u5347\u4e3a\u5b58\u50a8\u4e00\u7ea7\u64cd\u4f5c\uff0c\u66ff\u4ee3\u591a\u4e2a\u72ec\u7acbGET\u8bf7\u6c42\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u6570\u636e\u83b7\u53d6\u6548\u7387\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u9700\u4ece\u5206\u5e03\u5f0f\u5b58\u50a8\u83b7\u53d6\u5927\u91cf\u6570\u636e\u6837\u672c\uff0c\u6570\u5343\u4e2a\u72ec\u7acbGET\u8bf7\u6c42\u7684\u8bf7\u6c42\u5f00\u9500\u4e3b\u5bfc\u4e86\u6570\u636e\u4f20\u8f93\u65f6\u95f4\uff0c\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002", "method": "\u8bbe\u8ba1GetBatch\u5bf9\u8c61\u5b58\u50a8API\uff0c\u5b9e\u73b0\u786e\u5b9a\u6027\u7684\u3001\u5bb9\u9519\u7684\u6d41\u5f0f\u6279\u91cf\u6570\u636e\u68c0\u7d22\u64cd\u4f5c\uff0c\u66ff\u4ee3\u4f20\u7edf\u591a\u4e2aGET\u8bf7\u6c42\u3002", "result": "\u5bf9\u5c0f\u5bf9\u8c61\u5b9e\u73b0\u9ad8\u8fbe15\u500d\u541e\u5410\u91cf\u63d0\u5347\uff1b\u751f\u4ea7\u73af\u5883\u4e2dP95\u6279\u91cf\u68c0\u7d22\u5ef6\u8fdf\u964d\u4f4e2\u500d\uff0cP99\u5c3e\u90e8\u5ef6\u8fdf\u964d\u4f4e3.7\u500d\u3002", "conclusion": "GetBatch\u901a\u8fc7\u6279\u91cf\u5904\u7406\u4f18\u5316\u6570\u636e\u68c0\u7d22\u6d41\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u7684\u6570\u636e\u83b7\u53d6\u5f00\u9500\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2602.22456", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22456", "abs": "https://arxiv.org/abs/2602.22456", "authors": ["Ikram Darif", "Feifei Niu", "Manel Abdellatif", "Lionel C. Briand", "Ramesh S.", "Arun Adiththan"], "title": "Automating the Detection of Requirement Dependencies Using Large Language Models", "comment": null, "summary": "Requirements are inherently interconnected through various types of dependencies. Identifying these dependencies is essential, as they underpin critical decisions and influence a range of activities throughout software development. However, this task is challenging, particularly in modern software systems, given the high volume of complex, coupled requirements. These challenges are further exacerbated by the ambiguity of Natural Language (NL) requirements and their constant change. Consequently, requirement dependency detection is often overlooked or performed manually. Large Language Models (LLMs) exhibit strong capabilities in NL processing, presenting a promising avenue for requirement-related tasks. While they have shown to enhance various requirements engineering tasks, their effectiveness in identifying requirement dependencies remains unexplored. In this paper, we introduce LEREDD, an LLM-based approach for automated detection of requirement dependencies that leverages Retrieval-Augmented Generation (RAG) and In-Context Learning (ICL). It is designed to identify diverse dependency types directly from NL requirements. We empirically evaluate LEREDD against two state-of-the-art baselines. The results show that LEREDD provides highly accurate classification of dependent and non-dependent requirements, achieving an accuracy of 0.93, and an F1 score of 0.84, with the latter averaging 0.96 for non-dependent cases. LEREDD outperforms zero-shot LLMs and baselines, particularly in detecting fine-grained dependency types, where it yields average relative gains of 94.87% and 105.41% in F1 scores for the Requires dependency over the baselines. We also provide an annotated dataset of requirement dependencies encompassing 813 requirement pairs across three distinct systems to support reproducibility and future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LEREDD\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u9700\u6c42\u4f9d\u8d56\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u6280\u672f\u3002\u8be5\u65b9\u6cd5\u5728\u9700\u6c42\u4f9d\u8d56\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387(0.93)\u548cF1\u5206\u6570(0.84)\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u7ec6\u7c92\u5ea6\u4f9d\u8d56\u7c7b\u578b\u68c0\u6d4b\u65b9\u9762\u3002\u4f5c\u8005\u8fd8\u63d0\u4f9b\u4e86\u5305\u542b813\u4e2a\u9700\u6c42\u5bf9\u7684\u6807\u6ce8\u6570\u636e\u96c6\u4ee5\u652f\u6301\u53ef\u590d\u73b0\u6027\u3002", "motivation": "\u8f6f\u4ef6\u9700\u6c42\u4e4b\u95f4\u901a\u8fc7\u5404\u79cd\u7c7b\u578b\u76f8\u4e92\u5173\u8054\uff0c\u8bc6\u522b\u8fd9\u4e9b\u4f9d\u8d56\u5173\u7cfb\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u5927\u91cf\u590d\u6742\u8026\u5408\u7684\u9700\u6c42\u3001\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u7684\u6a21\u7cca\u6027\u4ee5\u53ca\u9700\u6c42\u7684\u9891\u7e41\u53d8\u5316\uff0c\u4f7f\u5f97\u9700\u6c42\u4f9d\u8d56\u68c0\u6d4b\u5177\u6709\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u8fd9\u4e00\u4efb\u52a1\u5e38\u88ab\u5ffd\u89c6\u6216\u624b\u52a8\u6267\u884c\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u6613\u51fa\u9519\u3002", "method": "\u4f5c\u8005\u63d0\u51faLEREDD\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u9700\u6c42\u4f9d\u8d56\u81ea\u52a8\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u6280\u672f\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u4e2d\u8bc6\u522b\u591a\u79cd\u4f9d\u8d56\u7c7b\u578b\u3002\u8be5\u65b9\u6cd5\u8bbe\u8ba1\u7528\u4e8e\u533a\u5206\u4f9d\u8d56\u548c\u975e\u4f9d\u8d56\u9700\u6c42\uff0c\u5e76\u7279\u522b\u5173\u6ce8\u7ec6\u7c92\u5ea6\u4f9d\u8d56\u7c7b\u578b\u7684\u68c0\u6d4b\u3002", "result": "LEREDD\u5728\u4e24\u4e2a\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff1a\u9700\u6c42\u4f9d\u8d56\u548c\u975e\u4f9d\u8d56\u5206\u7c7b\u51c6\u786e\u7387\u8fbe0.93\uff0cF1\u5206\u6570\u4e3a0.84(\u975e\u4f9d\u8d56\u6848\u4f8b\u5e73\u5747\u4e3a0.96)\u3002\u5728\u68c0\u6d4b\u7ec6\u7c92\u5ea6\u4f9d\u8d56\u7c7b\u578b\u65b9\u9762\uff0cLEREDD\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5bf9\u4e8eRequires\u4f9d\u8d56\uff0cF1\u5206\u6570\u76f8\u5bf9\u63d0\u5347\u5206\u522b\u8fbe\u523094.87%\u548c105.41%\u3002", "conclusion": "LEREDD\u8bc1\u660e\u4e86\u7ed3\u5408RAG\u548cICL\u6280\u672f\u7684LLM\u5728\u9700\u6c42\u4f9d\u8d56\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9700\u6c42\u5de5\u7a0b\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u63d0\u4f9b\u7684\u5305\u542b\u4e09\u4e2a\u4e0d\u540c\u7cfb\u7edf813\u4e2a\u9700\u6c42\u5bf9\u7684\u6807\u6ce8\u6570\u636e\u96c6\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u53ef\u590d\u73b0\u6027\u548c\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2602.22437", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22437", "abs": "https://arxiv.org/abs/2602.22437", "authors": ["Zezhou Wang", "Youjie Li", "Zhiqi Lin", "Jiacheng Yang", "Cong Xie", "Guanyu Feng", "Zheng Zhong", "Ziyue Huang", "Hongyu Zhu", "Zhi Zhang", "Yanghua Peng", "Xin Liu"], "title": "veScale-FSDP: Flexible and High-Performance FSDP at Scale", "comment": null, "summary": "Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP's fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today's implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5~66% higher throughput and 16~30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.", "AI": {"tldr": "veScale-FSDP\u662f\u4e00\u6b3e\u91cd\u65b0\u8bbe\u8ba1\u7684FSDP\u7cfb\u7edf\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u5206\u7247\u683c\u5f0f\u548c\u7ed3\u6784\u611f\u77e5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u548c\u66f4\u4f4e\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u80fd\u591f\u6269\u5c55\u5230\u6570\u4e07GPU\u3002", "motivation": "\u73b0\u6709FSDP\u7cfb\u7edf\u5728\u7ed3\u6784\u5316\u8bad\u7ec3\u65b9\u6cd5\u548c\u975e\u5143\u7d20\u7ea7\u4f18\u5316\u5668\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56fa\u5b9a\u5206\u7247\u683c\u5f0f\u4e0e\u5757\u7ed3\u6784\u8ba1\u7b97\u51b2\u7a81\uff0c\u4e14\u901a\u4fe1\u548c\u5185\u5b58\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u6269\u5c55\u80fd\u529b\u3002", "method": "\u5f15\u5165RaggedShard\u7075\u6d3b\u5206\u7247\u683c\u5f0f\u548c\u7ed3\u6784\u611f\u77e5\u89c4\u5212\u7b97\u6cd5\uff0c\u4fdd\u6301FSDP\u6240\u9700\u7684\u9ad8\u6548\u6570\u636e\u5e03\u5c40\uff0c\u539f\u751f\u652f\u6301\u5757\u91cf\u5316\u8bad\u7ec3\u548c\u975e\u5143\u7d20\u7ea7\u4f18\u5316\u5668\u3002", "result": "\u76f8\u6bd4\u73b0\u6709FSDP\u7cfb\u7edf\uff0c\u541e\u5410\u91cf\u63d0\u9ad85-66%\uff0c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e16-30%\uff0c\u5e76\u80fd\u9ad8\u6548\u6269\u5c55\u5230\u6570\u4e07GPU\u3002", "conclusion": "veScale-FSDP\u5728\u4fdd\u6301\u7075\u6d3b\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2602.22445", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.22445", "abs": "https://arxiv.org/abs/2602.22445", "authors": ["Martin Kuettler", "Hermann Haertig"], "title": "Fault-tolerant Reduce and Allreduce operations based on correction", "comment": null, "summary": "Implementations of Broadcast based on some information dissemination algorithm -- e.g., gossip or tree-based communication -- followed by a correction algorithm has been proposed previously. This work describes an approach to apply a similar idea to Reduce. In it, a correction-like communication phase precedes a tree-based phase. This provides a Reduce algorithm which is tolerant to a number of failed processes. Semantics of the resulting algorithm are provided and proven.\n  Based on these results, Broadcast and Reduce are combined to provide Allreduce.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bb9\u9519\u7684Reduce\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u6811\u5f62\u9636\u6bb5\u524d\u6dfb\u52a0\u6821\u6b63\u901a\u4fe1\u9636\u6bb5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8fdb\u7a0b\u6545\u969c\u7684\u5bb9\u5fcd\uff0c\u5e76\u5c06\u6b64\u65b9\u6cd5\u4e0eBroadcast\u7ed3\u5408\u5f62\u6210Allreduce\u7b97\u6cd5\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u5904\u7406\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u8fdb\u7a0b\u6545\u969c\u7684\u5bb9\u9519\u96c6\u4f53\u64cd\u4f5c\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9Reduce\u64cd\u4f5c\u3002", "method": "\u91c7\u7528\u4e0eBroadcast\u76f8\u53cd\u7684\u987a\u5e8f\uff0c\u5728\u6811\u5f62\u901a\u4fe1\u9636\u6bb5\u524d\u5b9e\u73b0\u7c7b\u4f3c\u6821\u6b63\u7684\u901a\u4fe1\u9636\u6bb5\uff0c\u4ee5\u5b9e\u73b0\u5bb9\u9519Reduce\u64cd\u4f5c\u3002", "result": "\u63d0\u4f9b\u4e86\u5177\u6709\u5df2\u8bc1\u660e\u8bed\u4e49\u7684\u5bb9\u9519Reduce\u7b97\u6cd5\uff0c\u5e76\u5c06Broadcast\u4e0eReduce\u7ed3\u5408\u5f62\u6210\u4e86Allreduce\u7b97\u6cd5\u3002", "conclusion": "\u6210\u529f\u5c06\u5bb9\u9519\u6280\u672f\u4eceBroadcast\u6269\u5c55\u5230Reduce\u64cd\u4f5c\uff0c\u4f7f\u96c6\u4f53\u8ba1\u7b97\u5728\u53ef\u80fd\u5b58\u5728\u8fdb\u7a0b\u6545\u969c\u7684\u7cfb\u7edf\u4e2d\u66f4\u52a0\u53ef\u9760\u3002"}}
{"id": "2602.22729", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.22729", "abs": "https://arxiv.org/abs/2602.22729", "authors": ["Yuchong Xie", "Kaikai Zhang", "Yu Liu", "Rundong Yang", "Ping Chen", "Shuai Wang", "Dongdong She"], "title": "RandSet: Randomized Corpus Reduction for Fuzzing Seed Scheduling", "comment": "To Appear in ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA 2026)", "summary": "Seed explosion is a fundamental problem in fuzzing seed scheduling, where a fuzzer maintains a huge corpus and fails to choose promising seeds. Existing works focus on seed prioritization but still suffer from seed explosion since corpus size remains huge. We tackle this from a new perspective: corpus reduction, i.e., computing a seed corpus subset. However, corpus reduction could lead to poor seed diversity and large runtime overhead. Prior techniques like cull_queue, AFL-Cmin, and MinSet suffer from poor diversity or prohibitive overhead, making them unsuitable for high-frequency seed scheduling.\n  We propose RandSet, a novel randomized corpus reduction technique that reduces corpus size and yields diverse seed selection simultaneously with minimal overhead. Our key insight is introducing randomness into corpus reduction to enjoy two benefits of a randomized algorithm: randomized output (diverse seed selection) and low runtime cost. Specifically, we formulate corpus reduction as a set cover problem and compute a randomized subset covering all features of the entire corpus. We then schedule seeds from this small, randomized subset rather than the entire corpus, effectively mitigating seed explosion.\n  We implement RandSet on three popular fuzzers: AFL++, LibAFL, and Centipede, and evaluate it on standalone programs, FuzzBench, and Magma. Results show RandSet achieves significantly more diverse seed selection than other reduction techniques, with average subset ratios of 4.03% and 5.99% on standalone and FuzzBench programs. RandSet achieves a 16.58% coverage gain on standalone programs and up to 3.57% on FuzzBench in AFL++, triggers up to 7 more ground-truth bugs than the state-of-the-art on Magma, while introducing only 1.17%-3.93% overhead.", "AI": {"tldr": "RandSet\u662f\u4e00\u79cd\u968f\u673a\u5316\u8bed\u6599\u5e93\u7f29\u51cf\u6280\u672f\uff0c\u901a\u8fc7\u5c06\u8bed\u6599\u5e93\u7f29\u51cf\u95ee\u9898\u8f6c\u5316\u4e3a\u96c6\u5408\u8986\u76d6\u95ee\u9898\u6765\u540c\u65f6\u5b9e\u73b0\u8bed\u6599\u5e93\u5927\u5c0f\u7f29\u51cf\u548c\u591a\u6837\u5316\u79cd\u5b50\u9009\u62e9\uff0c\u6700\u5c0f\u5316\u8fd0\u884c\u65f6\u5f00\u9500\uff0c\u5728\u4e09\u4e2a\u6d41\u884c\u6a21\u7cca\u6d4b\u8bd5\u5668\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u7cca\u6d4b\u8bd5\u9762\u4e34\u79cd\u5b50\u7206\u70b8\u95ee\u9898\uff0c\u5373\u7ef4\u6301\u5de8\u5927\u8bed\u6599\u5e93\u65f6\u65e0\u6cd5\u6709\u6548\u9009\u62e9\u6709\u524d\u9014\u7684\u79cd\u5b50\u3002\u73b0\u6709\u5de5\u4f5c\u4e13\u6ce8\u4e8e\u79cd\u5b50\u4f18\u5148\u7ea7\u6392\u5e8f\u4f46\u65e0\u6cd5\u89e3\u51b3\u6839\u672c\u95ee\u9898\uff0c\u56e0\u4e3a\u8bed\u6599\u5e93\u89c4\u6a21\u4ecd\u7136\u5de8\u5927\u3002", "method": "RandSet\u5c06\u8bed\u6599\u5e93\u7f29\u51cf\u8868\u8ff0\u4e3a\u96c6\u5408\u8986\u76d6\u95ee\u9898\uff0c\u8ba1\u7b97\u4e00\u4e2a\u8986\u76d6\u6574\u4e2a\u8bed\u6599\u5e93\u6240\u6709\u7279\u5f81\u7684\u968f\u673a\u5316\u5b50\u96c6\uff0c\u4ece\u8be5\u5b50\u96c6\u800c\u975e\u6574\u4e2a\u8bed\u6599\u5e93\u4e2d\u8c03\u5ea6\u79cd\u5b50\uff0c\u901a\u8fc7\u5f15\u5165\u968f\u673a\u6027\u540c\u65f6\u5b9e\u73b0\u591a\u6837\u5316\u7684\u79cd\u5b50\u9009\u62e9\u548c\u4f4e\u8fd0\u884c\u6210\u672c\u3002", "result": "RandSet\u5728\u72ec\u7acb\u7a0b\u5e8f\u4e0a\u5b9e\u73b0\u4e8616.58%\u7684\u8986\u76d6\u7387\u63d0\u5347\uff0c\u5728FuzzBench\u4e0a\u6700\u9ad8\u63d0\u53473.57%\uff0c\u5728Magma\u4e0a\u6bd4\u6700\u5148\u8fdb\u6280\u672f\u89e6\u53d1\u591a\u8fbe7\u4e2a\u989d\u5916\u7684\u771f\u5b9e\u9519\u8bef\uff0c\u540c\u65f6\u4ec5\u5f15\u51651.17%-3.93%\u7684\u5f00\u9500\u3002\u5e73\u5747\u5b50\u96c6\u6bd4\u4f8b\u4e3a\u72ec\u7acb\u7a0b\u5e8f\u76844.03%\u548cFuzzBench\u76845.99%\u3002", "conclusion": "RandSet\u901a\u8fc7\u968f\u673a\u5316\u8bed\u6599\u5e93\u7f29\u51cf\u6280\u672f\u6709\u6548\u7f13\u89e3\u4e86\u79cd\u5b50\u7206\u70b8\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u591a\u6837\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u7cca\u6d4b\u8bd5\u6548\u7387\uff0c\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u8bed\u6599\u5e93\u7f29\u51cf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22764", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.22764", "abs": "https://arxiv.org/abs/2602.22764", "authors": ["Jiahong Xiang", "Wenxiao He", "Xihua Wang", "Hongliang Tian", "Yuqun Zhang"], "title": "Evaluating and Improving Automated Repository-Level Rust Issue Resolution with LLM-based Agents", "comment": "Accepted to the 48th International Conference on Software Engineering (ICSE 2026)", "summary": "The Rust programming language presents a steep learning curve and significant coding challenges, making the automation of issue resolution essential for its broader adoption. Recently, LLM-powered code agents have shown remarkable success in resolving complex software engineering tasks, yet their application to Rust has been limited by the absence of a large-scale, repository-level benchmark. To bridge this gap, we introduce Rust-SWE-bench, a benchmark comprising 500 real-world, repository-level software engineering tasks from 34 diverse and popular Rust repositories. We then perform a comprehensive study on Rust-SWE-bench with four representative agents and four state-of-the-art LLMs to establish a foundational understanding of their capabilities and limitations in the Rust ecosystem. Our extensive study reveals that while ReAct-style agents are promising, i.e., resolving up to 21.2% of issues, they are limited by two primary challenges: comprehending repository-wide code structure and complying with Rust's strict type and trait semantics. We also find that issue reproduction is rather critical for task resolution. Inspired by these findings, we propose RUSTFORGER, a novel agentic approach that integrates an automated test environment setup with a Rust metaprogramming-driven dynamic tracing strategy to facilitate reliable issue reproduction and dynamic analysis. The evaluation shows that RUSTFORGER using Claude-Sonnet-3.7 significantly outperforms all baselines, resolving 28.6% of tasks on Rust-SWE-bench, i.e., a 34.9% improvement over the strongest baseline, and, in aggregate, uniquely solves 46 tasks that no other agent could solve across all adopted advanced LLMs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.22580", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.22580", "abs": "https://arxiv.org/abs/2602.22580", "authors": ["Yuhao Lin", "Zhipeng Tang", "Jiayan Tong", "Junqing Xiao", "Bin Lu", "Yuhang Li", "Chao Li", "Zhiguo Zhang", "Junhua Wang", "Hao Luo", "James Cheng", "Chuang Hu", "Jiawei Jiang", "Xiao Yan"], "title": "FuxiShuffle: An Adaptive and Resilient Shuffle Service for Distributed Data Processing on Alibaba Cloud", "comment": "14 pages, 13 figures", "summary": "Shuffle exchanges intermediate results between upstream and downstream operators in distributed data processing and is usually the bottleneck due to factors such as small random I/Os and network contention. Several systems have been designed to improve shuffle efficiency, but from our experiences of running ultra-large clusters at Alibaba Cloud MaxCompute platform, we observe that they can not adapt to highly dynamic job characteristics and cluster resource conditions, and their fault tolerance mechanisms are passive and inefficient when failures are inevitable. To tackle their limitations, we design and implement FuxiShuffle as a general data shuffle service for the ultra-large production environment of MaxCompute, featuring good adaptability and efficient failure resilience. Specifically, to achieve good adaptability, FuxiShuffle dynamically selects the shuffle mode based on runtime information, conducts progress-aware scheduling for the downstream workers, and automatically determines the most suitable backup strategy for each shuffle data chunk. To make failure resilience efficient, FuxiShuffle actively ensures data availability with multi-replica failover, prevents memory overflow with careful memory management, and employs an incremental recovery mechanism that does not lose computation progress. Our experiments show that, compared to baseline systems, FuxiShuffle significantly reduces not only end-to-end job completion time but also aggregate resource consumption. Micro experiments suggest that our designs are effective in improving adaptability and failure resilience.", "AI": {"tldr": "FuxiShuffle\u662f\u4e00\u79cd\u4e3a\u8d85\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8ba1\u7b97\u73af\u5883\u8bbe\u8ba1\u7684\u6570\u636e\u6df7\u6d17\u670d\u52a1\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u548c\u9ad8\u6548\u6545\u969c\u6062\u590d\u673a\u5236\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u548c\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u6df7\u6d17\u7cfb\u7edf\u65e0\u6cd5\u9002\u5e94\u9ad8\u5ea6\u52a8\u6001\u7684\u4f5c\u4e1a\u7279\u6027\u548c\u96c6\u7fa4\u8d44\u6e90\u6761\u4ef6\uff0c\u4e14\u5728\u6545\u969c\u53d1\u751f\u65f6\u5bb9\u9519\u673a\u5236\u88ab\u52a8\u4f4e\u6548\u3002", "method": "FuxiShuffle\u5b9e\u73b0\u4e86\u57fa\u4e8e\u8fd0\u884c\u65f6\u4fe1\u606f\u7684\u52a8\u6001\u6df7\u6d17\u6a21\u5f0f\u9009\u62e9\u3001\u4e0b\u6e38\u5de5\u4f5c\u5668\u7684\u8fdb\u5ea6\u611f\u77e5\u8c03\u5ea6\u3001\u81ea\u52a8\u786e\u5b9a\u6bcf\u4e2a\u6570\u636e\u5757\u7684\u6700\u9002\u5907\u4efd\u7b56\u7565\u3001\u591a\u526f\u672c\u6545\u969c\u8f6c\u79fb\u3001\u7cbe\u7ec6\u5185\u5b58\u7ba1\u7406\u548c\u4e0d\u4e22\u5931\u8ba1\u7b97\u8fdb\u5ea6\u7684\u589e\u91cf\u6062\u590d\u673a\u5236\u3002", "result": "\u4e0e\u57fa\u7ebf\u7cfb\u7edf\u76f8\u6bd4\uff0cFuxiShuffle\u663e\u8457\u51cf\u5c11\u4e86\u7aef\u5230\u7aef\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u548c\u603b\u4f53\u8d44\u6e90\u6d88\u8017\uff0c\u5fae\u89c2\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u63d0\u9ad8\u9002\u5e94\u6027\u548c\u6545\u969c\u6062\u590d\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "FuxiShuffle\u4e3a\u8d85\u5927\u89c4\u6a21\u751f\u4ea7\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u548c\u5bb9\u9519\u7684\u6570\u636e\u6df7\u6d17\u670d\u52a1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22835", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.22835", "abs": "https://arxiv.org/abs/2602.22835", "authors": ["Elisabeth Mo", "Jefferson Seide Moll\u00e9ri", "Asle Fagerstr\u00f8m"], "title": "Productivity and Collaboration in Hybrid Agile Teams: An Interview Study", "comment": null, "summary": "Hybrid work has become a reality post-pandemic, transforming how Agile teams deliver value, collaborate, and adapt. This study investigate how hybrid settings influence productivity and collaboration through nine interviews with three Norwegian Agile teams. Our findings show that hybrid work reduces informal interaction, creates uneven participation, and increases reliance on digital tools. Agile ceremonies became alignment anchors, while trust, communication, and tool support mediate team effectiveness. Hybrid Agile work is an evolving field that requires tailored structures to support inclusion, team cohesion, and sustainable performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8bbf\u8c08\u4e09\u4e2a\u632a\u5a01\u654f\u6377\u56e2\u961f\uff0c\u53d1\u73b0\u6df7\u5408\u5de5\u4f5c\u51cf\u5c11\u4e86\u975e\u6b63\u5f0f\u4e92\u52a8\uff0c\u9020\u6210\u53c2\u4e0e\u4e0d\u5747\uff0c\u5e76\u589e\u52a0\u5bf9\u6570\u5b57\u5de5\u5177\u7684\u4f9d\u8d56\uff0c\u800c\u4fe1\u4efb\u3001\u6c9f\u901a\u548c\u5de5\u5177\u652f\u6301\u662f\u56e2\u961f\u6709\u6548\u6027\u7684\u4e2d\u4ecb\u56e0\u7d20\u3002", "motivation": "\u75ab\u60c5\u540e\u6df7\u5408\u5de5\u4f5c\u5df2\u6210\u4e3a\u73b0\u5b9e\uff0c\u6539\u53d8\u4e86\u654f\u6377\u56e2\u961f\u4ea4\u4ed8\u4ef7\u503c\u3001\u534f\u4f5c\u548c\u9002\u5e94\u7684\u65b9\u5f0f\uff0c\u9700\u8981\u4e86\u89e3\u6df7\u5408\u73af\u5883\u5982\u4f55\u5f71\u54cd\u654f\u6377\u56e2\u961f\u7684\u751f\u4ea7\u529b\u548c\u534f\u4f5c\u3002", "method": "\u901a\u8fc7\u5bf9\u4e09\u4e2a\u632a\u5a01\u654f\u6377\u56e2\u961f\u8fdb\u884c\u4e5d\u6b21\u8bbf\u8c08\uff0c\u91c7\u7528\u5b9a\u6027\u7814\u7a76\u65b9\u6cd5\u6536\u96c6\u6df7\u5408\u5de5\u4f5c\u73af\u5883\u4e2d\u7684\u56e2\u961f\u4f53\u9a8c\u6570\u636e\u3002", "result": "\u6df7\u5408\u5de5\u4f5c\u51cf\u5c11\u4e86\u975e\u6b63\u5f0f\u4e92\u52a8\uff0c\u9020\u6210\u53c2\u4e0e\u4e0d\u5747\uff0c\u589e\u52a0\u5bf9\u6570\u5b57\u5de5\u5177\u7684\u4f9d\u8d56\uff0c\u654f\u6377\u4eea\u5f0f\u6210\u4e3a\u56e2\u961f\u5b9a\u4f4d\u7684\u951a\u70b9\uff0c\u4fe1\u4efb\u3001\u6c9f\u901a\u548c\u5de5\u5177\u652f\u6301\u662f\u56e2\u961f\u6709\u6548\u6027\u7684\u4e2d\u4ecb\u56e0\u7d20\u3002", "conclusion": "\u6df7\u5408\u654f\u6377\u5de5\u4f5c\u662f\u4e00\u4e2a\u4e0d\u65ad\u53d1\u5c55\u9886\u57df\uff0c\u9700\u8981\u91cf\u8eab\u5b9a\u5236\u7684\u7ed3\u6784\u6765\u652f\u6301\u5305\u5bb9\u6027\u3001\u56e2\u961f\u51dd\u805a\u529b\u548c\u53ef\u6301\u7eed\u7ee9\u6548\u3002"}}
{"id": "2602.22593", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.22593", "abs": "https://arxiv.org/abs/2602.22593", "authors": ["Shouwei Gao", "Junqi Yin", "Feiyi Wang", "Wenqian Dong"], "title": "FLYING SERVING: On-the-Fly Parallelism Switching for Large Language Model Serving", "comment": null, "summary": "Production LLM serving must simultaneously deliver high throughput, low latency, and sufficient context capacity under non-stationary traffic and mixed request requirements. Data parallelism (DP) maximizes throughput by running independent replicas, while tensor parallelism (TP) reduces per-request latency and pools memory for long-context inference. However, existing serving stacks typically commit to a static parallelism configuration at deployment; adapting to bursts, priorities, or long-context requests is often disruptive and slow. We present Flying Serving, a vLLM-based system that enables online DP-TP switching without restarting engine workers. Flying Serving makes reconfiguration practical by virtualizing the state that would otherwise force data movement: (i) a zero-copy Model Weights Manager that exposes TP shard views on demand, (ii) a KV Cache Adaptor that preserves request KV state across DP/TP layouts, (iii) an eagerly initialized Communicator Pool to amortize collective setup, and (iv) a deadlock-free scheduler that coordinates safe transitions under execution skew. Across three popular LLMs and realistic serving scenarios, Flying Serving improves performance by up to $4.79\\times$ under high load and $3.47\\times$ under low load while supporting latency- and memory-driven requests.", "AI": {"tldr": "Flying Serving\u662f\u4e00\u79cdvLLM-based\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5728\u7ebf\u6570\u636e\u5e76\u884c(DP)\u4e0e\u5f20\u91cf\u5e76\u884c(TP)\u7684\u65e0\u7f1d\u5207\u6362\uff0c\u65e0\u9700\u91cd\u542f\u5f15\u64ce\u5de5\u4f5c\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u670d\u52a1\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u9759\u6001\u5e76\u884c\u914d\u7f6e\uff0c\u96be\u4ee5\u9002\u5e94\u975e\u5e73\u7a33\u6d41\u91cf\u548c\u6df7\u5408\u8bf7\u6c42\u9700\u6c42\uff0c\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u9ad8\u541e\u5410\u91cf\u3001\u4f4e\u5ef6\u8fdf\u548c\u5927\u4e0a\u4e0b\u6587\u5bb9\u91cf\u7684\u8981\u6c42\u3002", "method": "Flying Serving\u901a\u8fc7\u56db\u9879\u5173\u952e\u6280\u672f\u5b9e\u73b0DP-TP\u5728\u7ebf\u5207\u6362\uff1a\u96f6\u62f7\u8d1d\u6a21\u578b\u6743\u91cd\u7ba1\u7406\u5668\u3001KV\u7f13\u5b58\u9002\u914d\u5668\u3001\u9884\u521d\u59cb\u5316\u901a\u4fe1\u5668\u6c60\u548c\u65e0\u6b7b\u9501\u8c03\u5ea6\u5668\uff0c\u865a\u62df\u5316\u72b6\u6001\u4ee5\u907f\u514d\u6570\u636e\u8fc1\u79fb\u3002", "result": "\u5728\u4e09\u79cd\u6d41\u884cLLM\u548c\u5b9e\u9645\u670d\u52a1\u573a\u666f\u4e2d\uff0cFlying Serving\u5728\u9ad8\u8d1f\u8f7d\u4e0b\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe4.79\u500d\uff0c\u4f4e\u8d1f\u8f7d\u4e0b\u63d0\u53473.47\u500d\uff0c\u540c\u65f6\u652f\u6301\u5ef6\u8fdf\u654f\u611f\u548c\u5185\u5b58\u5bc6\u96c6\u578b\u8bf7\u6c42\u3002", "conclusion": "Flying Serving\u901a\u8fc7\u52a8\u6001\u5e76\u884c\u7b56\u7565\u5207\u6362\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u670d\u52a1\u7cfb\u7edf\u5728\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u4e0a\u4e0b\u6587\u5bb9\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u8d44\u6e90\u5229\u7528\u548c\u670d\u52a1\u8d28\u91cf\u3002"}}
{"id": "2602.23005", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23005", "abs": "https://arxiv.org/abs/2602.23005", "authors": ["Man Zhang", "Tao Yue", "Yihua He"], "title": "Managing Uncertainty in LLM-based Multi-Agent System Operation", "comment": null, "summary": "Applying LLM-based multi-agent software systems in safety-critical domains such as lifespan echocardiography introduces system-level risks that cannot be addressed by improving model accuracy alone. During system operation, beyond individual LLM behavior, uncertainty propagates through agent coordination, data pipelines, human-in-the-loop interaction, and runtime control logic. Yet existing work largely treats uncertainty at the model level rather than as a first-class software engineering concern. This paper approaches uncertainty from both system-level and runtime perspectives. We first differentiate epistemological and ontological uncertainties in the context of LLM-based multi-agent software system operation. Building on this foundation, we propose a lifecycle-based uncertainty management framework comprising four mechanisms: representation, identification, evolution, and adaptation. The uncertainty lifecycle governs how uncertainties emerge, transform, and are mitigated across architectural layers and execution phases, enabling structured runtime governance and controlled adaptation. We demonstrate the feasibility of the framework using a real-world LLM-based multi-agent echocardiographic software system developed in clinical collaboration, showing improved reliability and diagnosability in diagnostic reasoning. The proposed approach generalizes to other safety-critical LLM-based multi-agent software systems, supporting principled operational control and runtime assurance beyond model-centric methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u751f\u547d\u5468\u671f\u7684\u591a\u667a\u80fd\u4f53\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u4e2d\u7684\u7cfb\u7edf\u7ea7\u98ce\u9669\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u7ea7\u522b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u667a\u80fd\u4f53\u534f\u8c03\u3001\u6570\u636e\u7ba1\u9053\u3001\u4eba\u673a\u4ea4\u4e92\u548c\u8fd0\u884c\u65f6\u63a7\u5236\u903b\u8f91\u7b49\u7cfb\u7edf\u7ea7\u98ce\u9669\u4f20\u64ad\u95ee\u9898\u3002", "method": "\u533a\u5206\u8ba4\u8bc6\u8bba\u548c\u672c\u4f53\u8bba\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51fa\u5305\u542b\u8868\u793a\u3001\u8bc6\u522b\u3001\u6f14\u5316\u548c\u9002\u5e94\u56db\u4e2a\u673a\u5236\u7684\u5168\u751f\u547d\u5468\u671f\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\u6846\u67b6\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u8fd0\u884c\u65f6\u6cbb\u7406\u548c\u53d7\u63a7\u9002\u5e94\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u7684\u5fc3\u810f\u8d85\u58f0\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u8bca\u65ad\u63a8\u7406\u53ef\u9760\u6027\u548c\u53ef\u8bca\u65ad\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u4ee5\u6a21\u578b\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u4e3a\u5176\u4ed6\u5b89\u5168\u5173\u952e\u9886\u57df\u7684LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u539f\u5219\u7684\u8fd0\u884c\u63a7\u5236\u548c\u8fd0\u884c\u4fdd\u8bc1\u3002"}}
{"id": "2602.22760", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22760", "abs": "https://arxiv.org/abs/2602.22760", "authors": ["Philipp Wiesner", "Soeren Becker", "Brett Cornick", "Dominik Scheinert", "Alexander Acker", "Odej Kao"], "title": "Distributed LLM Pretraining During Renewable Curtailment Windows: A Feasibility Study", "comment": "Technical report", "summary": "Training large language models (LLMs) requires substantial compute and energy. At the same time, renewable energy sources regularly produce more electricity than the grid can absorb, leading to curtailment, the deliberate reduction of clean generation that would otherwise go to waste. These periods represent an opportunity: if training is aligned with curtailment windows, LLMs can be pretrained using electricity that is both clean and cheap. This technical report presents a system that performs full-parameter LLM training across geo-distributed GPU clusters during regional curtailment windows, elastically switching between local single-site training and federated multi-site synchronization as sites become available or unavailable. Our prototype trains a 561M-parameter transformer model across three clusters using the Flower federated learning framework, with curtailment periods derived from real-world marginal carbon intensity traces. Preliminary results show that curtailment-aware scheduling preserves training quality while reducing operational emissions to 5-12% of single-site baselines.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\uff0c\u5728\u53ef\u518d\u751f\u80fd\u6e90\u524a\u51cf\u671f\u95f4\u5229\u7528\u5206\u5e03\u5f0fGPU\u96c6\u7fa4\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u78b3\u6392\u653e\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u8d28\u91cf\u3002", "motivation": "\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u548c\u80fd\u6e90\uff0c\u540c\u65f6\u53ef\u518d\u751f\u80fd\u6e90\u7ecf\u5e38\u4ea7\u751f\u8d85\u8fc7\u7535\u7f51\u53ef\u5438\u6536\u7684\u7535\u529b\uff0c\u5bfc\u81f4\u524a\u51cf\u6d6a\u8d39\u3002\u5c06\u8bad\u7ec3\u4e0e\u524a\u51cf\u7a97\u53e3\u5bf9\u9f50\u53ef\u5229\u7528\u6e05\u6d01\u4e14\u5ec9\u4ef7\u7684\u7535\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5728\u5730\u7406\u4f4d\u7f6e\u5206\u5e03\u7684GPU\u96c6\u7fa4\u4e0a\u6267\u884c\u5168\u53c2\u6570LLM\u8bad\u7ec3\u7684\u7cfb\u7edf\uff0c\u5728\u533a\u57df\u524a\u51cf\u7a97\u53e3\u671f\u95f4\u8bad\u7ec3\uff0c\u5f39\u6027\u5207\u6362\u672c\u5730\u5355\u7ad9\u70b9\u8bad\u7ec3\u548c\u8054\u90a6\u591a\u7ad9\u70b9\u540c\u6b65\uff0c\u4f7f\u7528Flower\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u57fa\u4e8e\u771f\u5b9e\u8fb9\u9645\u78b3\u5f3a\u5ea6\u8f68\u8ff9\u786e\u5b9a\u524a\u51cf\u671f\u3002", "result": "\u5728\u4e09\u4e2a\u96c6\u7fa4\u4e0a\u6210\u529f\u8bad\u7ec3\u4e86561M\u53c2\u6570\u7684transformer\u6a21\u578b\uff0c\u524a\u51cf\u611f\u77e5\u8c03\u5ea6\u5728\u4fdd\u6301\u8bad\u7ec3\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5c06\u8fd0\u8425\u6392\u653e\u964d\u4f4e\u5230\u5355\u7ad9\u70b9\u57fa\u7ebf\u76845-12%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u8bc1\u660e\u4e86LLM\u8bad\u7ec3\u53ef\u4ee5\u4e0e\u53ef\u518d\u751f\u80fd\u6e90\u524a\u51cf\u671f\u5bf9\u9f50\uff0c\u663e\u8457\u964d\u4f4e\u78b3\u6392\u653e\uff0c\u5f39\u6027\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5141\u8bb8\u6839\u636e\u7ad9\u70b9\u53ef\u7528\u6027\u52a8\u6001\u8c03\u6574\u3002"}}
{"id": "2602.23047", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23047", "abs": "https://arxiv.org/abs/2602.23047", "authors": ["Haichuan Hu", "Ye Shang", "Guoqing Xie", "Congqing He", "Quanjun Zhang"], "title": "CL4SE: A Context Learning Benchmark For Software Engineering Tasks", "comment": "23 pages, 4 figures", "summary": "Context engineering has emerged as a pivotal paradigm for unlocking the potential of Large Language Models (LLMs) in Software Engineering (SE) tasks, enabling performance gains at test time without model fine-tuning. Despite its success, existing research lacks a systematic taxonomy of SE-specific context types and a dedicated benchmark to quantify the heterogeneous effects of different contexts across core SE workflows. To address this gap, we propose CL4SE (Context Learning for Software Engineering), a comprehensive benchmark featuring a fine-grained taxonomy of four SE-oriented context types (interpretable examples, project-specific context, procedural decision-making context, and positive & negative context), each mapped to a representative task (code generation, code summarization, code review, and patch correctness assessment). We construct high-quality datasets comprising over 13,000 samples from more than 30 open-source projects and evaluate five mainstream LLMs across nine metrics. Extensive experiments demonstrate that context learning yields an average performance improvement of 24.7% across all tasks. Specifically, procedural context boosts code review performance by up to 33% (Qwen3-Max), mixed positive-negative context improves patch assessment by 30% (DeepSeek-V3), project-specific context increases code summarization BLEU by 14.78% (GPT-Oss-120B), and interpretable examples enhance code generation PASS@1 by 5.72% (DeepSeek-V3). CL4SE establishes the first standardized evaluation framework for SE context learning, provides actionable empirical insights into task-specific context design, and releases a large-scale dataset to facilitate reproducible research in this domain.", "AI": {"tldr": "CL4SE\u662f\u9996\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u56db\u79cdSE\u7279\u5b9a\u4e0a\u4e0b\u6587\u7c7b\u578b\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u663e\u793a\u4e0a\u4e0b\u6587\u5b66\u4e60\u5e73\u5747\u63d0\u5347\u6027\u80fd24.7%\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u8f6f\u4ef6\u5de5\u7a0b\u7279\u5b9a\u4e0a\u4e0b\u6587\u7c7b\u578b\u7684\u7cfb\u7edf\u5206\u7c7b\u548c\u4e13\u7528\u57fa\u51c6\u6765\u91cf\u5316\u4e0d\u540c\u4e0a\u4e0b\u6587\u5728\u6838\u5fc3SE\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5f02\u6784\u6548\u5e94\u3002", "method": "\u63d0\u51faCL4SE\u57fa\u51c6\uff0c\u5305\u542b\u56db\u79cdSE\u5bfc\u5411\u4e0a\u4e0b\u6587\u7c7b\u578b(\u53ef\u89e3\u91ca\u793a\u4f8b\u3001\u9879\u76ee\u7279\u5b9a\u4e0a\u4e0b\u6587\u3001\u7a0b\u5e8f\u51b3\u7b56\u4e0a\u4e0b\u6587\u3001\u6b63\u8d1f\u4e0a\u4e0b\u6587)\uff0c\u6620\u5c04\u5230\u56db\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\uff0c\u6784\u5efa\u4e86\u6765\u81ea30\u591a\u4e2a\u5f00\u6e90\u9879\u76ee\u768413,000\u591a\u4e2a\u6837\u672c\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e94\u4e2a\u4e3b\u6d41LLM\u5728\u4e5d\u4e2a\u6307\u6807\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u534724.7%\uff0c\u7a0b\u5e8f\u4e0a\u4e0b\u6587\u5c06\u4ee3\u7801\u5ba1\u67e5\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe33%\uff0c\u6df7\u5408\u6b63\u8d1f\u4e0a\u4e0b\u6587\u5c06\u8865\u4e01\u8bc4\u4f30\u63d0\u9ad830%\uff0c\u9879\u76ee\u7279\u5b9a\u4e0a\u4e0b\u6587\u5c06\u4ee3\u7801\u6458\u8981BLEU\u63d0\u9ad814.78%\uff0c\u53ef\u89e3\u91ca\u793a\u4f8b\u5c06\u4ee3\u7801\u751f\u6210PASS@1\u63d0\u9ad85.72%\u3002", "conclusion": "CL4SE\u5efa\u7acb\u4e86SE\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u9996\u4e2a\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u4efb\u52a1\u7279\u5b9a\u4e0a\u4e0b\u6587\u8bbe\u8ba1\u7684\u53ef\u64cd\u4f5c\u7ecf\u9a8c\u89c1\u89e3\uff0c\u5e76\u53d1\u5e03\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2602.22780", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.22780", "abs": "https://arxiv.org/abs/2602.22780", "authors": ["Qingyuan Zhang"], "title": "An Artificial Intelligence Framework for Joint Structural-Temporal Load Forecasting in Cloud Native Platforms", "comment": null, "summary": "This study targets cloud native environments where microservice invocation relations are complex, load fluctuations are multi-scale and superimposed, and cross-service impacts are significant. We propose a structured temporal joint load prediction framework oriented to microservice topology. The method represents the system as a coupled entity of a time-evolving service invocation graph and multivariate load sequences. It constructs neighborhood-aggregated and global summarized views based on service level observations. This forms layered load representations across instance, service, and cluster levels. A unified sequence encoder models multi-scale historical context. To strengthen the expression of invocation dependencies, the framework introduces a lightweight structural prior into attention computation. This enables more effective capture of load propagation and accumulation along invocation chains, while maintaining consistent modeling of local bursts and overall trends. The training objective adopts a multi-objective regression strategy that jointly optimizes service level and cluster level predictions to improve cross-granularity stability. We further conduct single-factor sensitivity analyses on key structural and training hyperparameters. We systematically examine the effects of time window length, encoding depth, and regularization strength. The results support the necessity of multi-granularity fusion and structural injection and clarify their effective configuration ranges. Overall, the framework provides a reusable modeling paradigm and implementation path for capacity assessment, resource orchestration, and runtime situational understanding in cloud environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5fae\u670d\u52a1\u62d3\u6251\u7684\u8054\u5408\u8d1f\u8f7d\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u670d\u52a1\u8c03\u7528\u56fe\u548c\u5206\u5c42\u8868\u793a\u5efa\u6a21\u591a\u5c3a\u5ea6\u8d1f\u8f7d\u6a21\u5f0f\uff0c\u6539\u8fdb\u4e91\u539f\u751f\u73af\u5883\u4e2d\u7684\u8d44\u6e90\u8bc4\u4f30\u548c\u8c03\u5ea6", "motivation": "\u4e91\u539f\u751f\u73af\u5883\u4e2d\u5fae\u670d\u52a1\u8c03\u7528\u5173\u7cfb\u590d\u6742\u3001\u591a\u5c3a\u5ea6\u8d1f\u8f7d\u6ce2\u52a8\u53e0\u52a0\u3001\u8de8\u670d\u52a1\u5f71\u54cd\u663e\u8457\uff0c\u4f20\u7edf\u8d1f\u8f7d\u9884\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218", "method": "\u5c06\u7cfb\u7edf\u8868\u793a\u4e3a\u65f6\u53d8\u670d\u52a1\u8c03\u7528\u56fe\u4e0e\u591a\u5143\u8d1f\u8f7d\u5e8f\u5217\u7684\u8026\u5408\u5b9e\u4f53\uff0c\u6784\u5efa\u5b9e\u4f8b\u3001\u670d\u52a1\u548c\u96c6\u7fa4\u5c42\u7ea7\u7684\u5206\u5c42\u8d1f\u8f7d\u8868\u793a\uff0c\u5f15\u5165\u7ed3\u6784\u5148\u9a8c\u5230\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\uff0c\u91c7\u7528\u591a\u76ee\u6807\u56de\u5f52\u7b56\u7565\u8054\u5408\u4f18\u5316", "result": "\u5355\u56e0\u7d20\u654f\u611f\u6027\u5206\u6790\u8bc1\u5b9e\u4e86\u591a\u7c92\u5ea6\u878d\u5408\u548c\u7ed3\u6784\u6ce8\u5165\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u660e\u786e\u4e86\u6709\u6548\u914d\u7f6e\u8303\u56f4", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e91\u73af\u5883\u4e2d\u7684\u5bb9\u91cf\u8bc4\u4f30\u3001\u8d44\u6e90\u7f16\u6392\u548c\u8fd0\u884c\u65f6\u6001\u52bf\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u91cd\u7528\u7684\u5efa\u6a21\u8303\u5f0f\u548c\u5b9e\u65bd\u8def\u5f84"}}
{"id": "2602.23065", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23065", "abs": "https://arxiv.org/abs/2602.23065", "authors": ["Kunpeng Zhang", "Dongwei Xiao", "Daoyuan Wu", "Jiali Zhao", "Yuanyi Lin", "Tongtong Xu", "Shaohua Wang", "Shuai Wang"], "title": "LLM-Powered Silent Bug Fuzzing in Deep Learning Libraries via Versatile and Controlled Bug Transfer", "comment": null, "summary": "Deep learning (DL) libraries are widely used in critical applications, where even subtle silent bugs can lead to serious consequences. While existing DL fuzzing techniques have made progress in detecting crashes, they inherently struggle to detect silent bugs due to the lack of effective test programs and corresponding oracles.\n  Building on the observation that historical bug reports contain rich, underutilized information about silent bugs, we leverage large language models (LLMs) to perform versatile yet controlled bug transfer for silent bug fuzzing. Specifically, our approach uses LLMs to extract context-aware bug patterns from historical issues, match semantically related Application Programming Interfaces (APIs) using functionality-based embeddings, and synthesize test cases with customized oracles. This enables proactive detection of silent bugs by transferring high-risk contexts and oracle designs from known buggy APIs to functionally similar target APIs. To ensure the reliability of our context-aware bug transfer, we introduce an LLM-powered self-validation module that systematically evaluates the validity of each transferred bug instance. We implement this methodology in a tool named TransFuzz and evaluate it on three mainstream DL libraries: PyTorch, TensorFlow, and MindSpore. TransFuzz successfully discovers 79 previously unknown bugs (12 confirmed as Common Vulnerabilities and Exposures (CVEs)) in 10 bug types, demonstrating its effectiveness and generalizability in migrating DL library bug discovery capabilities.", "AI": {"tldr": "TransFuzz\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u4ece\u5386\u53f2\u62a5\u544a\u4e2d\u63d0\u53d6\u4e0a\u4e0b\u6587\u611f\u77e5\u7684bug\u6a21\u5f0f\uff0c\u901a\u8fc7\u529f\u80fd\u5339\u914dAPI\uff0c\u5408\u6210\u6d4b\u8bd5\u7528\u4f8b\u548c\u81ea\u5b9a\u4e49\u9a8c\u8bc1\u5668\uff0c\u5b9e\u73b0\u4e86\u6df1\u5ea6\u5b66\u4e60\u5e93\u4e2d\u9759\u9ed8bug\u7684\u4e3b\u52a8\u68c0\u6d4b\uff0c\u53d1\u73b0\u4e8679\u4e2a\u65b0bug(12\u4e2aCVE)\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5e93\u5728\u5173\u952e\u5e94\u7528\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u73b0\u6709\u7684\u6a21\u7cca\u6d4b\u8bd5\u6280\u672f\u96be\u4ee5\u68c0\u6d4b\u9759\u9ed8bug\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u6709\u6548\u7684\u6d4b\u8bd5\u7a0b\u5e8f\u548c\u76f8\u5e94\u7684\u9a8c\u8bc1\u5668\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884cbug\u8fc1\u79fb\uff1a\u4ece\u5386\u53f2\u95ee\u9898\u4e2d\u63d0\u53d6\u4e0a\u4e0b\u6587\u611f\u77e5bug\u6a21\u5f0f\uff0c\u4f7f\u7528\u57fa\u4e8e\u529f\u80fd\u7684\u5d4c\u5165\u5339\u914d\u8bed\u4e49\u76f8\u5173\u7684API\uff0c\u5408\u6210\u5e26\u6709\u81ea\u5b9a\u4e49\u9a8c\u8bc1\u5668\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u901a\u8fc7LLM\u9a71\u52a8\u7684\u81ea\u9a8c\u8bc1\u6a21\u5757\u786e\u4fdd\u8fc1\u79fb\u7684\u53ef\u9760\u6027\u3002", "result": "\u5728PyTorch\u3001TensorFlow\u548cMindSpore\u4e09\u4e2a\u4e3b\u6d41\u6df1\u5ea6\u5b66\u4e60\u5e93\u4e2d\u53d1\u73b0\u4e8679\u4e2a\u5148\u524d\u672a\u77e5\u7684bug\uff0c\u5176\u4e2d12\u4e2a\u5df2\u88ab\u786e\u8ba4\u4e3aCVE\uff0c\u8986\u76d6\u4e8610\u79cdbug\u7c7b\u578b\u3002", "conclusion": "TransFuzz\u5c55\u793a\u4e86\u5c06\u6df1\u5ea6\u5b66\u4e60\u5e93bug\u53d1\u73b0\u80fd\u529b\u8fc1\u79fb\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548cbug\u8f6c\u79fb\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u5e93\u4e2d\u9759\u9ed8bug\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2602.22852", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.22852", "abs": "https://arxiv.org/abs/2602.22852", "authors": ["Oliver Larsson", "Thijs Metsch", "Cristian Klein", "Erik Elmroth"], "title": "Workload Buoyancy: Keeping Apps Afloat by Identifying Shared Resource Bottlenecks", "comment": "14 pages, 10 figures, 4 tables", "summary": "Modern multi-tenant, hardware-heterogeneous computing environments pose significant challenges for effective workload orchestration. Simple heuristics for assessing workload performance, such as CPU utilization or application-level metrics, are often insufficient to capture the complex performance dynamics arising from resource contention and noisy-neighbor effects. In such environments, performance bottlenecks may emerge in any shared system resource, leading to unexpected and difficult-to-diagnose degradation.\n  This paper introduces buoyancy, a novel abstraction for characterizing workload performance in multi-tenant systems. Unlike traditional approaches, buoyancy integrates application-level metrics with system-level insights of shared resource contention to provide a holistic view of performance dynamics. By explicitly capturing bottlenecks and headroom across multiple resources, buoyancy facilitates resource-aware and application-aware orchestration in a manner that is intuitive, extensible, and generalizable across heterogeneous platforms. We evaluate buoyancy using representative multi-tenant workloads to illustrate its ability to expose performance-limiting resource interactions. Buoyancy provides a 19.3% better indication of bottlenecks compared to traditional heuristics on average. We additionally show how buoyancy can act as a drop-in replacement for conventional performance metrics, enabling improved observability and more informed scheduling and optimization decisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa'\u6d6e\u529b'(buoyancy)\u65b0\u62bd\u8c61\u6982\u5ff5\uff0c\u901a\u8fc7\u6574\u5408\u5e94\u7528\u7ea7\u6307\u6807\u4e0e\u7cfb\u7edf\u7ea7\u8d44\u6e90\u4e89\u7528\u6d1e\u5bdf\uff0c\u5728\u591a\u79df\u6237\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u5de5\u4f5c\u8d1f\u8f7d\u6027\u80fd\u8868\u5f81\uff0c\u8f83\u4f20\u7edf\u6307\u6807\u63d0\u534719.3%\u7684\u74f6\u9888\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u591a\u79df\u6237\u3001\u786c\u4ef6\u5f02\u6784\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u5de5\u4f5c\u8d1f\u8f7d\u7f16\u6392\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edfCPU\u5229\u7528\u7387\u7b49\u7b80\u5355\u6307\u6807\u65e0\u6cd5\u6355\u83b7\u8d44\u6e90\u4e89\u7528\u548c\u566a\u58f0\u90bb\u5c45\u6548\u5e94\u5bfc\u81f4\u7684\u590d\u6742\u6027\u80fd\u52a8\u6001\uff0c\u5f15\u53d1\u96be\u4ee5\u8bca\u65ad\u7684\u6027\u80fd\u9000\u5316\u3002", "method": "\u5f00\u53d1'\u6d6e\u529b'\u62bd\u8c61\u6982\u5ff5\uff0c\u6574\u5408\u5e94\u7528\u7ea7\u6307\u6807\u4e0e\u5171\u4eab\u8d44\u6e90\u4e89\u7528\u7684\u7cfb\u7edf\u7ea7\u6d1e\u5bdf\uff0c\u663e\u5f0f\u6355\u83b7\u591a\u8d44\u6e90\u74f6\u9888\u548c\u4f59\u91cf\uff0c\u63d0\u4f9b\u76f4\u89c2\u3001\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u6027\u80fd\u52a8\u6001\u8868\u5f81\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u4ee3\u8868\u6027\u591a\u79df\u6237\u5de5\u4f5c\u8d1f\u8f7d\u8bc4\u4f30\uff0c\u6d6e\u529b\u6bd4\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u5e73\u5747\u63d0\u9ad819.3%\u7684\u74f6\u9888\u6307\u793a\u80fd\u529b\uff0c\u53ef\u4f5c\u4e3a\u4f20\u7edf\u6027\u80fd\u6307\u6807\u7684\u66ff\u4ee3\u54c1\uff0c\u63d0\u5347\u53ef\u89c2\u6d4b\u6027\u5e76\u652f\u6301\u66f4\u660e\u667a\u7684\u8c03\u5ea6\u548c\u4f18\u5316\u51b3\u7b56\u3002", "conclusion": "\u6d6e\u529b\u4f7f\u591a\u79df\u6237\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u8d44\u6e90\u611f\u77e5\u548c\u5e94\u7528\u611f\u77e5\u7684\u5de5\u4f5c\u8d1f\u8f7d\u7f16\u6392\uff0c\u6539\u5584\u5f02\u6784\u5e73\u53f0\u4e0a\u7684\u6027\u80fd\u4f18\u5316\u6548\u679c\uff0c\u4e3a\u590d\u6742\u8ba1\u7b97\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6027\u80fd\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23331", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23331", "abs": "https://arxiv.org/abs/2602.23331", "authors": ["Salim Fares"], "title": "Utilizing LLMs for Industrial Process Automation", "comment": null, "summary": "A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5e94\u7528\u4e8e\u5de5\u4e1a\u6d41\u7a0b\u81ea\u52a8\u5316\u9886\u57df\u7684\u4e13\u7528\u7f16\u7a0b\u8bed\u8a00\uff0c\u76f8\u8f83\u4e8e\u901a\u7528\u7f16\u7a0b\u8bed\u8a00\u8fd9\u4e00\u65b9\u5411\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5927\u591a\u6570\u7814\u7a76\u96c6\u4e2d\u5728Python\u7b49\u901a\u7528\u7f16\u7a0b\u8bed\u8a00\u4e0a\uff0c\u800c\u4e13\u7528\u4e8e\u5de5\u4e1a\u81ea\u52a8\u5316\u9886\u57df\u7684\u7f16\u7a0b\u8bed\u8a00(\u901a\u5e38\u4ec5\u5728\u4e13\u6709\u73af\u5883\u4e2d\u4f7f\u7528)\u4e0eLLMs\u7684\u7ed3\u5408\u5e94\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u65e8\u5728\u5c06LLMs\u96c6\u6210\u5230\u5de5\u4e1a\u5f00\u53d1\u6d41\u7a0b\u4e2d\uff0c\u89e3\u51b3\u5b9e\u9645\u7f16\u7a0b\u4efb\u52a1(\u5982\u4e3a\u673a\u68b0\u81c2\u751f\u6210\u8fd0\u52a8\u4f8b\u7a0b)\u5e76\u52a0\u901f\u5236\u9020\u7cfb\u7edf\u7684\u5f00\u53d1\u5468\u671f\u3002", "result": "\u6458\u8981\u4e2d\u672a\u660e\u786e\u63d0\u53ca\u7814\u7a76\u7ed3\u679c(\u53ef\u80fd\u8bba\u6587\u5c55\u793a\u4e86\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u5e94\u7528LLMs\u7684\u53d1\u73b0)\u3002", "conclusion": "\u6458\u8981\u4e2d\u672a\u660e\u786e\u63d0\u53ca\u7814\u7a76\u7ed3\u8bba(\u53ef\u80fd\u8bba\u6587\u603b\u7ed3\u4e86LLMs\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b)\u3002"}}
{"id": "2602.22916", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.22916", "abs": "https://arxiv.org/abs/2602.22916", "authors": ["Yaseen Abd-Elhaleem", "Michal Dory", "Oren Weimann"], "title": "A Simple Distributed Deterministic Planar Separator", "comment": "19 pages, to appear in SIROCCO 2026", "summary": "A balanced separator of a graph $G$ is a set of vertices whose removal disconnects the graph into connected components that are a constant factor smaller than $G$. Lipton and Tarjan [FOCS'77] famously proved that every planar graph admits a balanced separator of size $O(\\sqrt{n})$, as well as a balanced separator of size $O(D)$ that is a simple path (where $D$ is $G$'s diameter). In the centralized setting, both separators can be found in linear time. In the distributed setting, $D$ is a universal lower bound for the round complexity of solving many optimization problems, so, separators of size $O(D)$ are preferable.\n  It was not until [DISC'17] that a distributed algorithm was devised by Ghaffari and Parter to compute such an $O(D)$-size separator in $\\tilde O(D)$ rounds, by adapting the Lipton-Tarjan algorithm to the distributed model. Since then, this algorithm was used in several distributed algorithms for planar graphs, e.g., [GP, DISC'17], [LP, STOC'19], [AEDPW, PODC'25]. However, the algorithm is randomized, deeming the algorithms that use it to be randomized as well. Obtaining a deterministic algorithm remained an interesting open question until [PODC'25], when a (complex) deterministic separator algorithm was given by Jauregui, Montealegre and Rapaport.\n  We present a much simpler deterministic separator algorithm with the same (near-optimal) $\\tilde O(D)$-round complexity. While previous works devised either complicated or randomized ways of transferring weights from vertices to faces of $G$, we show that a straightforward way also works: Each vertex simply transfers its weight to one arbitrary face it lies on. That's it!\n  We note that a deterministic separator algorithm directly derandomizes the state-of-the-art distributed algorithms for classical problems on planar graphs such as single-source shortest-paths, maximum-flow, directed global min-cut, and reachability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u786e\u5b9a\u6027\u7684\u5e73\u9762\u56fe\u5e73\u8861\u5206\u9694\u7b97\u6cd5\uff0c\u5728\u00d5(D)\u8f6e\u5185\u627e\u5230\u5927\u5c0f\u4e3aO(D)\u7684\u5206\u9694\u7b26\uff0c\u76f8\u6bd4\u4e4b\u524d\u590d\u6742\u7684\u968f\u673a\u5316\u7b97\u6cd5\u6216\u590d\u6742\u7684\u786e\u5b9a\u6027\u7b97\u6cd5\u66f4\u4e3a\u7b80\u6d01\u9ad8\u6548\u3002", "motivation": "\u5206\u5e03\u5f0f\u6a21\u578b\u4e2d\u9700\u8981\u786e\u5b9a\u6027\u7b97\u6cd5\u6765\u907f\u514d\u968f\u673a\u5316\uff0c\u800c\u73b0\u6709\u7684O(D)\u5927\u5c0f\u5206\u9694\u7b26\u7b97\u6cd5\u662f\u968f\u673a\u5316\u7684\uff0c\u786e\u5b9a\u6027\u7b97\u6cd5\u5219\u8fc7\u4e8e\u590d\u6742\u3002", "method": "\u6bcf\u4e2a\u7b80\u5355\u5730\u5c06\u81ea\u8eab\u6743\u91cd\u8f6c\u79fb\u5230\u5176\u6240\u5728\u7684\u4e00\u4e2a\u4efb\u610f\u9762\u4e0a\uff0c\u8fd9\u662f\u4e00\u79cd\u76f4\u63a5\u4e14\u7b80\u5355\u7684\u6743\u91cd\u8f6c\u79fb\u65b9\u6cd5\u3002", "result": "\u5b9e\u73b0\u4e86\u4e0e\u5148\u524d\u590d\u6742\u7b97\u6cd5\u76f8\u540c\u7684\u00d5(D)\u8f6e\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u4f46\u7b97\u6cd5\u66f4\u7b80\u5355\uff0c\u4e14\u76f4\u63a5\u5bf9\u5e73\u9762\u56fe\u4e0a\u591a\u79cd\u7ecf\u5178\u95ee\u9898\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\u8fdb\u884c\u4e86\u53bb\u968f\u673a\u5316\u3002", "conclusion": "\u8be5\u786e\u5b9a\u6027\u5206\u9694\u7b26\u7b97\u6cd5\u4e0d\u4ec5\u7b80\u5316\u4e86\u65b9\u6cd5\uff0c\u8fd8\u4e3a\u5e73\u9762\u56fe\u4e0a\u7684\u591a\u4e2a\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u786e\u5b9a\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23036", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23036", "abs": "https://arxiv.org/abs/2602.23036", "authors": ["Jaehong Cho", "Hyunmin Choi", "Guseul Heo", "Jongse Park"], "title": "LLMServingSim 2.0: A Unified Simulator for Heterogeneous and Disaggregated LLM Serving Infrastructure", "comment": "12 pages, 10 figures", "summary": "Large language model (LLM) serving infrastructures are undergoing a shift toward heterogeneity and disaggregation. Modern deployments increasingly integrate diverse accelerators and near-memory processing technologies, introducing significant hardware heterogeneity, while system software increasingly separates computation, memory, and model components across distributed resources to improve scalability and efficiency. As a result, LLM serving performance is no longer determined by hardware or software choices in isolation, but by their runtime interaction through scheduling, data movement, and interconnect behavior. However, understanding these interactions remains challenging, as existing simulators lack the ability to jointly model heterogeneous hardware and disaggregated serving techniques within a unified, runtime-driven framework.\n  This paper presents LLMServingSim 2.0, a unified system-level simulator designed to make runtime-driven hardware-software interactions in heterogeneous and disaggregated LLM serving infrastructures explicit and analyzable. LLMServingSim 2.0 embeds serving decisions and hardware behavior into a single runtime loop, enabling interaction-aware modeling of batching, routing, offloading, memory, and power. The simulator supports extensible integration of emerging accelerators and memory systems through profile-based modeling, while capturing dynamic serving behavior and system-level effects. We validate LLMServingSim 2.0 against real deployments, showing that it reproduces key performance, memory, and power metrics with an average error of 0.97%, while maintaining simulation times of around 10 minutes even for complex configurations. These results demonstrate that LLMServingSim 2.0 provides a practical bridge between hardware innovation and serving-system design, enabling systematic exploration and co-design for next-generation LLM serving infrastructures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86LLMServingSim 2.0\uff0c\u4e00\u4e2a\u7edf\u4e00\u7cfb\u7edf\u7ea7\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u6a21\u62df\u5f02\u6784\u548c\u5206\u5e03\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u8fd0\u884c\u65f6\u786c\u4ef6-\u8f6f\u4ef6\u4ea4\u4e92\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\uff080.97%\u8bef\u5dee\uff09\u548c\u5408\u7406\u6a21\u62df\u65f6\u95f4\uff08\u7ea610\u5206\u949f\uff09\u3002", "motivation": "\u73b0\u6709\u6a21\u62df\u5668\u65e0\u6cd5\u5728\u7edf\u4e00\u7684\u8fd0\u884c\u65f6\u9a71\u52a8\u6846\u67b6\u4e2d\u8054\u5408\u5efa\u6a21\u5f02\u6784\u786c\u4ef6\u548c\u5206\u5e03\u5f0f\u670d\u52a1\u6280\u672f\uff0c\u4f7f\u5f97\u96be\u4ee5\u7406\u89e3\u786c\u4ef6-\u8f6f\u4ef6\u4ea4\u4e92\u5982\u4f55\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u6027\u80fd\u3002", "method": "LLMServingSim 2.0\u5c06\u670d\u52a1\u51b3\u7b56\u548c\u786c\u4ef6\u884c\u4e3a\u5d4c\u5165\u5355\u4e00\u8fd0\u884c\u65f6\u5faa\u73af\uff0c\u5b9e\u73b0\u6279\u5904\u7406\u3001\u8def\u7531\u3001\u5378\u8f7d\u3001\u5185\u5b58\u548c\u7535\u529b\u7684\u4ea4\u4e92\u611f\u77e5\u5efa\u6a21\u3002\u5b83\u901a\u8fc7\u57fa\u4e8e\u914d\u7f6e\u6587\u4ef6\u7684\u5efa\u6a21\u652f\u6301\u65b0\u5174\u52a0\u901f\u5668\u7684\u53ef\u6269\u5c55\u96c6\u6210\u3002", "result": "\u8be5\u6a21\u62df\u5668\u4e0e\u771f\u5b9e\u90e8\u7f72\u76f8\u6bd4\uff0c\u91cd\u73b0\u4e86\u5173\u952e\u6027\u80fd\u3001\u5185\u5b58\u548c\u7535\u529b\u6307\u6807\uff0c\u5e73\u5747\u8bef\u5dee\u4e3a0.97%\uff0c\u5373\u4f7f\u5728\u590d\u6742\u914d\u7f6e\u4e0b\u4e5f\u80fd\u4fdd\u6301\u7ea610\u5206\u949f\u7684\u6a21\u62df\u65f6\u95f4\u3002", "conclusion": "LLMServingSim 2.0\u4e3a\u786c\u4ef6\u521b\u65b0\u548c\u670d\u52a1\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u9645\u6865\u6881\uff0c enabling \u7cfb\u7edf\u5316\u63a2\u7d22\u548c\u4e0b\u4e00\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u57fa\u7840\u8bbe\u65bd\u7684\u534f\u540c\u8bbe\u8ba1\u3002"}}
{"id": "2602.23220", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.23220", "abs": "https://arxiv.org/abs/2602.23220", "authors": ["Chris Egersdoerfer", "Philip Carns", "Shane Snyder", "Robert Ross", "Dong Dai"], "title": "STELLAR: Storage Tuning Engine Leveraging LLM Autonomous Reasoning for High Performance Parallel File Systems", "comment": "Published in the Proceedings of the 2025 International Conference for High Performance Computing, Networking, Storage, and Analysis (SC25)", "summary": "I/O performance is crucial to efficiency in data-intensive scientific computing; but tuning large-scale storage systems is complex, costly, and notoriously manpower-intensive, making it inaccessible for most domain scientists. To address this problem, we propose STELLAR, an autonomous tuner for high-performance parallel file systems. Our evaluations show that STELLAR almost always selects near-optimal parameter configurations for parallel file systems within the first five attempts, even for previously unseen applications.\n  STELLAR differs fundamentally from traditional autotuning methods, which often require hundreds of thousands of iterations to converge. Powered by large language models (LLMs), STELLAR enables autonomous end-to-end agentic tuning by (1) accurately extracting tunable parameters from software manuals, (2) analyzing I/O trace logs generated by applications, (3) selecting initial tuning strategies, (4) rerunning applications on real systems and collecting I/O performance feedback, (5) adjusting tuning strategies and repeating the tuning cycle, and (6) reflecting on and summarizing tuning experiences into reusable knowledge for future optimizations. STELLAR integrates retrieval-augmented generation (RAG), tool execution, LLM-based reasoning, and a multiagent design to stabilize reasoning and combat hallucinations.\n  We evaluate the impact of each component on optimization outcomes, providing design insights for similar systems in other optimization domains. STELLAR's architecture and empirical results highlight a promising approach to complex system optimization, especially for problems with large search spaces and high exploration costs, while making I/O tuning more accessible to domain scientists with minimal added resources.", "AI": {"tldr": "STELLAR\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a9\u5f0fI/O\u8c03\u4f18\u7cfb\u7edf\uff0c\u80fd\u5728\u524d\u4e94\u6b21\u5c1d\u8bd5\u4e2d\u4e3a\u5e76\u884c\u6587\u4ef6\u7cfb\u7edf\u9009\u62e9\u63a5\u8fd1\u6700\u4f18\u7684\u53c2\u6570\u914d\u7f6e\uff0c\u663e\u8457\u4f18\u4e8e\u9700\u8981\u6570\u5341\u4e07\u6b21\u8fed\u4ee3\u624d\u80fd\u6536\u655b\u7684\u4f20\u7edf\u81ea\u52a8\u8c03\u4f18\u65b9\u6cd5\u3002", "motivation": "I/O\u6027\u80fd\u8c03\u4f18\u5728\u6570\u636e\u5bc6\u96c6\u578b\u79d1\u5b66\u8ba1\u7b97\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u89c4\u6a21\u5b58\u50a8\u7cfb\u7edf\u7684\u8c03\u4f18\u8fc7\u7a0b\u590d\u6742\u3001\u6210\u672c\u9ad8\u4e14\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u4eba\u529b\uff0c\u4f7f\u5927\u591a\u6570\u9886\u57df\u79d1\u5b66\u5bb6\u96be\u4ee5\u8bbf\u95ee\u3002", "method": "STELLAR\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u81ea\u4e3b\u7aef\u5230\u7aef\u667a\u80fd\u8c03\u4f18\uff0c\u901a\u8fc7\u516d\u4e2a\u6b65\u9aa4\uff1a(1)\u4ece\u8f6f\u4ef6\u624b\u518c\u4e2d\u63d0\u53d6\u53ef\u8c03\u53c2\u6570\uff1b(2)\u5206\u6790\u5e94\u7528\u7a0b\u5e8f\u751f\u6210\u7684I/O\u8ddf\u8e2a\u65e5\u5fd7\uff1b(3)\u9009\u62e9\u521d\u59cb\u8c03\u4f18\u7b56\u7565\uff1b(4)\u5728\u771f\u5b9e\u7cfb\u7edf\u4e0a\u91cd\u65b0\u8fd0\u884c\u5e94\u7528\u7a0b\u5e8f\u5e76\u6536\u96c6I/O\u6027\u80fd\u53cd\u9988\uff1b(5)\u8c03\u6574\u8c03\u4f18\u7b56\u7565\u5e76\u91cd\u590d\u8c03\u4f18\u5468\u671f\uff1b(6)\u53cd\u601d\u5e76\u603b\u7ed3\u8c03\u4f18\u7ecf\u9a8c\u4e3a\u672a\u6765\u4f18\u5316\u53ef\u91cd\u7528\u77e5\u8bc6\u3002\u7cfb\u7edf\u6574\u5408\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u3001\u5de5\u5177\u6267\u884c\u3001\u57fa\u4e8eLLM\u7684\u63a8\u7406\u548c\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\u3002", "result": "STELLAR\u51e0\u4e4e\u603b\u662f\u80fd\u5728\u524d\u4e94\u6b21\u5c1d\u8bd5\u4e2d\u4e3a\u5e76\u884c\u6587\u4ef6\u7cfb\u7edf\u9009\u62e9\u63a5\u8fd1\u6700\u4f18\u7684\u53c2\u6570\u914d\u7f6e\uff0c\u5373\u4f7f\u662f\u5bf9\u4e8e\u4e4b\u524d\u672a\u89c1\u8fc7\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6bcf\u4e2a\u7ec4\u4ef6\u5bf9\u4f18\u5316\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u4e3a\u5176\u4ed6\u4f18\u5316\u9886\u57df\u7684\u7c7b\u4f3c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u89c1\u89e3\u3002", "conclusion": "STELLAR\u7684\u67b6\u6784\u548c\u5b9e\u8bc1\u7ed3\u679c\u4e3a\u590d\u6742\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u641c\u7d22\u7a7a\u95f4\u5927\u3001\u63a2\u7d22\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4f7f\u9886\u57df\u79d1\u5b66\u5bb6\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u989d\u5916\u8d44\u6e90\u8bbf\u95eeI/O\u8c03\u4f18\u80fd\u529b\u3002"}}
