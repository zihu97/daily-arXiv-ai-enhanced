<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 7]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Towards autonomous normative multi-agent systems for Human-AI software engineering teams](https://arxiv.org/abs/2512.02329)
*Hoa Khanh Dam,Geeta Mahala,Rashina Hoda,Xi Zheng,Cristina Conati*

Main category: cs.SE

TL;DR: 本文提出一种由具备类人推理能力的自主AI智能体驱动的新型软件工程范式，通过基于大语言模型的智能体协作，显著提升软件开发的速度、可靠性与适应性，并利用规范逻辑确保合规与可信。


<details>
  <summary>Details</summary>
Motivation: 当前软件开发流程在速度、可靠性和适应性方面存在局限，亟需引入更智能、自主和协作能力强的AI系统来革新软件工程实践。

Method: 构建一类新型软件工程智能体，融合大语言模型，并赋予其信念、愿望、意图和记忆等类人认知机制；智能体之间及与人类通过以道义模态（承诺、义务、禁止、许可）表达的规范进行协调与合作。

Result: 实现了能高效协同完成设计、实现、测试和部署等核心开发任务的AI智能体系统，展现出超越现有流程的性能，并建立了可扩展、透明且可信的人机协作框架。

Conclusion: 该研究为未来人机协同的软件工程团队奠定了基础，提供了一个以规范驱动、高度自主且可信赖的新型开发范式。

Abstract: This paper envisions a transformative paradigm in software engineering, where Artificial Intelligence, embodied in fully autonomous agents, becomes the primary driver of the core software development activities. We introduce a new class of software engineering agents, empowered by Large Language Models and equipped with beliefs, desires, intentions, and memory to enable human-like reasoning. These agents collaborate with humans and other agents to design, implement, test, and deploy software systems with a level of speed, reliability, and adaptability far beyond the current software development processes. Their coordination and collaboration are governed by norms expressed as deontic modalities - commitments, obligations, prohibitions and permissions - that regulate interactions and ensure regulatory compliance. These innovations establish a scalable, transparent and trustworthy framework for future Human-AI software engineering teams.

</details>


### [2] [Process-Centric Analysis of Agentic Software Systems](https://arxiv.org/abs/2512.02393)
*Shuyang Liu,Yang Chen,Rahul Krishna,Saurabh Sinha,Jatin Ganhotra,Reyhan Jabbarvand*

Main category: cs.SE

TL;DR: 本文提出Graphectory，一种用于系统化建模智能体系统执行轨迹的图结构方法，以支持过程导向的评估。通过对4000条SWE-agent和OpenHands在SWE-bench上的轨迹分析，发现更强的LLM或更丰富的提示能产生更复杂的推理路径，成功案例通常遵循定位-修复-验证流程，而失败案例则表现出混乱或重复行为，且即使成功也常伴随低效过程。


<details>
  <summary>Details</summary>
Motivation: 现有对智能体系统的评估过于关注最终成败，忽视了其推理、规划与策略调整等过程细节，缺乏对执行轨迹的深入理解。

Method: 提出Graphectory框架，将智能体系统的执行轨迹编码为包含时序与语义关系的图结构，并基于此设计过程导向的评估指标；在SWE-bench上对SWE-agent和OpenHands结合四种主流LLM生成的4000条轨迹进行自动化分析。

Result: 1）使用更强LLM或更丰富提示的智能体展现出更复杂的Graphectory，体现更深入的探索与验证；2）成功案例多呈现连贯的定位-修复-验证策略，失败案例则表现为混乱或重复行为；3）即使成功，智能体流程常存在效率低下问题。

Conclusion: Graphectory为智能体系统提供了细粒度、过程导向的分析工具，揭示了当前智能体编程工作流在策略连贯性与执行效率方面的不足，强调需超越结果导向的评估范式。

Abstract: Agentic systems are modern software systems: they consist of orchestrated modules, expose interfaces, and are deployed in software pipelines. Unlike conventional programs, their execution (i.e., trajectories) is inherently stochastic and adaptive to the problem they are solving. Evaluation of such systems is often outcome-centric, judging their performance based on success or failure at the final step. This narrow focus overlooks detailed insights about such systems, failing to explain how agents reason, plan, act, or change their strategies over time. Inspired by the structured representation of conventional software systems as graphs, we introduce Graphectory to systematically encode the temporal and semantic relations in such software systems. Graphectory facilitates the design of process-centric metrics and analyses to assess the quality of agentic workflows independent of final success.
  Using Graphectory, we analyze 4000 trajectories of two dominant agentic programming workflows, namely SWE-agent and OpenHands, with a combination of four backbone Large Language Models (LLMs), attempting to resolve SWE-bench Verified issues. Our fully automated analyses reveal that: (1) agents using richer prompts or stronger LLMs exhibit more complex Graphectory, reflecting deeper exploration, broader context gathering, and more thorough validation before patch submission; (2) agents' problem-solving strategies vary with both problem difficulty and the underlying LLM -- for resolved issues, the strategies often follow coherent localization-patching-validation steps, while unresolved ones exhibit chaotic, repetitive, or backtracking behaviors; (3) even when successful, agentic programming systems often display inefficient processes, leading to unnecessarily prolonged trajectories.

</details>


### [3] [Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System](https://arxiv.org/abs/2512.02567)
*Martin Weiss,Jesko Hecking-Harbusch,Jochen Quante,Matthias Woehrle*

Main category: cs.SE

TL;DR: 本文研究了反馈循环、大语言模型选择和行为保持的代码变换对C到Rust自动翻译系统效果的影响，发现反馈机制能显著缩小不同模型间的性能差距，并提升系统鲁棒性甚至整体表现。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在软件工程任务中的广泛应用，自动化方法需具备更高可靠性才能用于工业实践。作者聚焦影响结果质量的三个关键因素：自动化反馈循环、大语言模型选择和行为保持的代码变换，以C到Rust翻译为案例进行系统评估。

Method: 构建基于“生成-校验”模式的C-to-Rust自动翻译系统，利用大语言模型生成Rust代码，并通过自动检查其可编译性和与原始C代码的行为等价性；若校验失败，则通过反馈循环重新提示模型修复输出，并在此框架下对比不同变量配置下的成功率。

Result: 无反馈循环时，大语言模型的选择对翻译成功率影响显著；引入反馈循环后，不同模型间性能差异明显缩小，且系统在面对代码扰动时表现出更强鲁棒性；此外，代码扰动带来的多样性甚至能提升系统整体性能。

Conclusion: 反馈循环在自动化代码翻译中起关键作用，不仅能缓解模型选择带来的性能差异，还能增强系统鲁棒性；行为保持的代码扰动不仅无害，反而有助于提升翻译质量，这对工业级AI辅助软件工程工具的设计具有重要指导意义。

Abstract: The advent of strong generative AI has a considerable impact on various software engineering tasks such as code repair, test generation, or language translation. While tools like GitHub Copilot are already in widespread use in interactive settings, automated approaches require a higher level of reliability before being usable in industrial practice. In this paper, we focus on three aspects that directly influence the quality of the results: a) the effect of automated feedback loops, b) the choice of Large Language Model (LLM), and c) the influence of behavior-preserving code changes.
  We study the effect of these three variables on an automated C-to-Rust translation system. Code translation from C to Rust is an attractive use case in industry due to Rust's safety guarantees. The translation system is based on a generate-and-check pattern, in which Rust code generated by the LLM is automatically checked for compilability and behavioral equivalence with the original C code. For negative checking results, the LLM is re-prompted in a feedback loop to repair its output. These checks also allow us to evaluate and compare the respective success rates of the translation system when varying the three variables.
  Our results show that without feedback loops LLM selection has a large effect on translation success. However, when the translation system uses feedback loops the differences across models diminish. We observe this for the average performance of the system as well as its robustness under code perturbations. Finally, we also identify that diversity provided by code perturbations can even result in improved system performance.

</details>


### [4] [Integrative Analysis of Risk Management Methodologies in Data Science Projects](https://arxiv.org/abs/2512.02728)
*Sabrina Delmondes da Costa Feitosa*

Main category: cs.SE

TL;DR: 本文通过整合性文献综述，比较了数据科学项目中主流的风险管理方法，发现传统标准（如ISO 31000、PMBOK、NIST RMF）对新兴风险覆盖有限，而新兴框架（如DS EthiCo RMF）更强调伦理与社会技术维度；研究建议构建融合技术效率、组织协同与负责任数据实践的混合型风险管理框架。


<details>
  <summary>Details</summary>
Motivation: 数据科学项目失败率高，原因包括技术限制、组织能力不足及风险管理不善，尤其缺乏对伦理与社会技术风险的系统应对机制。

Method: 采用整合性文献综述方法，基于结构化筛选与内容分析协议，从索引数据库中系统分析主流风险管理标准（ISO 31000、PMBOK、NIST RMF）与数据科学专用框架（CRISP-DM、DS EthiCo RMF）。

Result: 传统风险管理方法在应对数据科学中的新兴风险方面存在明显不足；而新近提出的框架（如DS EthiCo RMF）通过多维结构整合伦理监督、治理机制与持续监控，更具适应性。

Conclusion: 应发展兼顾技术效率、组织对齐与负责任数据实践的混合型风险管理框架，并指出未来研究可聚焦于填补现有方法在伦理整合与动态风险监测方面的空白。

Abstract: Data science initiatives frequently exhibit high failure rates, driven by technical constraints, organizational limitations and insufficient risk management practices. Challenges such as low data maturity, lack of governance, misalignment between technical and business teams, and the absence of structured mechanisms to address ethical and sociotechnical risks have been widely identified in the literature. In this context, the purpose of this study is to conduct a comparative analysis of the main risk management methodologies applied to data science projects, aiming to identify, classify, and synthesize their similarities, differences and existing gaps. An integrative literature review was performed using indexed databases and a structured protocol for selection and content analysis. The study examines widely adopted risk management standards ISO 31000, PMBOK Risk Management and NIST RMF, as well as frameworks specific to data science workflows, such as CRISP DM and the recently proposed DS EthiCo RMF, which incorporates ethical and sociotechnical dimensions into the project life cycle. The findings reveal that traditional approaches provide limited coverage of emerging risks, whereas contemporary models propose multidimensional structures capable of integrating ethical oversight, governance and continuous monitoring. As a contribution, this work offers theoretical support for the development of hybrid frameworks that balance technical efficiency, organizational alignment and responsible data practices, while highlighting research gaps that can guide future investigations.

</details>


### [5] ["Can you feel the vibes?": An exploration of novice programmer engagement with vibe coding](https://arxiv.org/abs/2512.02750)
*Kiev Gama,Filipe Calegario,Victoria Jackson,Alexander Nolte,Luiz Augusto Morais,Vinicius Garcia*

Main category: cs.SE

TL;DR: 本文通过在巴西一所公立大学举办的一场为期一天的“氛围编程”（vibe coding）黑客松，研究了新手程序员和混合经验团队如何利用自然语言提示进行软件开发。研究发现，这种方法促进了快速原型设计和跨学科协作，但也暴露出创意过早收敛、代码质量不均以及对核心软件工程实践参与不足等问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和AI辅助编程的兴起，“氛围编程”作为一种通过自然语言提示而非直接编写代码来创建软件的新范式，有望降低编程门槛。然而，其在教育场景中的影响尚未得到充分探索，尤其是在新手学习者和多元背景团队中的实际应用效果。

Method: 研究者组织了一场包含31名来自计算机与非计算机专业的本科生参与的9小时黑客松活动，将其分为9个团队。通过现场观察、退出问卷和半结构化访谈，收集了关于创意过程、工具使用模式、协作动态和学习成果的数据。

Result: 研究发现，“氛围编程”支持快速原型开发和跨学科合作，参与者在短时间内掌握了提示工程技能并完成了可运行的演示。但同时也出现了创意阶段过早收敛、生成代码质量参差不齐需大量返工、以及对软件工程核心实践（如测试、架构设计）关注不足等问题。团队普遍采用多AI工具串联的工作流，且人类判断在关键优化环节不可或缺。短时活动有效提升了新手信心，并适合时间有限的参与者。

Conclusion: “氛围编程”黑客松可作为低风险、高包容性的学习环境，但需辅以明确的教学支架，包括促进发散思维、培养对AI输出的批判性评估能力，以及建立对产出质量的合理预期。

Abstract: Emerging alongside generative AI and the broader trend of AI-assisted coding, the term "vibe coding" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.

</details>


### [6] [Towards Observation Lakehouses: Living, Interactive Archives of Software Behavior](https://arxiv.org/abs/2512.02795)
*Marcus Kessel*

Main category: cs.SE

TL;DR: 本文提出“观测湖仓”（Observation Lakehouse）架构，通过持续记录和存储代码执行过程中的刺激-响应-上下文三元组（SRC），实现对程序行为的高效、可扩展分析，支持无需重新执行即可进行多版本评估、行为聚类和共识预言等任务，并在单机上验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成大模型主要基于静态代码训练，容易学习到错误或标注不当的代码；而程序的真实语义行为只能通过动态执行观察获得。已有工作虽提出了SRM/SRC等结构来表示行为，但缺乏可扩展的持久化、演化与交互分析能力。

Method: 构建基于Apache Parquet + Iceberg + DuckDB的观测湖仓系统，以追加式表格持续存储所有执行观测数据（刺激、响应、上下文），并通过SQL按需物化SRC切片；数据来源于LASSO控制管道和CI流水线（如单元测试）。

Result: 在包含509个问题的基准上，系统摄入约860万条观测记录（<51MiB），在普通笔记本电脑上可在100毫秒内重建SRM/SRC视图和聚类结果，证明无需分布式集群即可高效实现持续行为挖掘。

Conclusion: 观测湖仓使程序行为的动态观测数据成为一类一等公民，为行为感知的模型评估与训练提供了可行的基础设施，并已作为开源项目公开发布。

Abstract: Code-generating LLMs are trained largely on static artifacts (source, comments, specifications) and rarely on materializations of run-time behavior. As a result, they readily internalize buggy or mislabeled code. Since non-trivial semantic properties are undecidable in general, the only practical way to obtain ground-truth functionality is by dynamic observation of executions. In prior work, we addressed representation with Sequence Sheets, Stimulus-Response Matrices (SRMs), and Stimulus-Response Cubes (SRCs) to capture and compare behavior across tests, implementations, and contexts. These structures make observation data analyzable offline and reusable, but they do not by themselves provide persistence, evolution, or interactive analytics at scale. In this paper, therefore, we introduce observation lakehouses that operationalize continual SRCs: a tall, append-only observations table storing every actuation (stimulus, response, context) and SQL queries that materialize SRC slices on demand. Built on Apache Parquet + Iceberg + DuckDB, the lakehouse ingests data from controlled pipelines (LASSO) and CI pipelines (e.g., unit test executions), enabling n-version assessment, behavioral clustering, and consensus oracles without re-execution. On a 509-problem benchmark, we ingest $\approx$8.6M observation rows ($<$51MiB) and reconstruct SRM/SRC views and clusters in $<$100ms on a laptop, demonstrating that continual behavior mining is practical without a distributed cluster of machines. This makes behavioral ground truth first-class alongside other run-time data and provides an infrastructure path toward behavior-aware evaluation and training. The Observation Lakehouse, together with the accompanying dataset, is publicly available as an open-source project on GitHub: https://github.com/SoftwareObservatorium/observation-lakehouse

</details>


### [7] [Model-Based Diagnosis with Multiple Observations: A Unified Approach for C Software and Boolean Circuits](https://arxiv.org/abs/2512.02898)
*Pedro Orvalho,Marta Kwiatkowska,Mikoláš Janota,Vasco Manquinho*

Main category: cs.SE

TL;DR: 本文提出CFaults，一种基于模型诊断（MBD）与MaxSAT的新型多故障定位工具，适用于C程序和布尔电路。它通过整合所有失败测试用例为统一MaxSAT公式，确保诊断一致性并避免冗余结果，在多个基准测试中优于现有FBFL方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于公式的故障定位（FBFL）方法在处理多故障场景时存在不足：无法保证覆盖所有失败测试用例，或产生非子集最小的冗余诊断结果。

Method: CFaults结合模型诊断（MBD）与多观测信息，将所有失败测试用例聚合为一个统一的最大可满足性（MaxSAT）公式，从而实现一致且精简的故障定位。

Result: 在TCAS、C-Pack-IPAs（C程序）和ISCAS85（布尔电路）三个基准测试中，CFaults在C程序上比BugAssist、SNIPER和HSD更快；在ISCAS85上虽略慢于HSD，但仅少定位6%的电路，并且只生成子集最小的诊断结果，避免了其他方法的冗余问题。

Conclusion: CFaults是一种高效、精确的多故障定位工具，能有效克服现有FBFL方法在一致性与冗余性方面的缺陷，在软件与电路故障定位任务中具有竞争力。

Abstract: Debugging is one of the most time-consuming and expensive tasks in software development and circuit design. Several formula-based fault localisation (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs/circuits with multiple faults.
  This paper introduces CFaults, a novel fault localisation tool for C software and Boolean circuits with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified Maximum Satisfiability (MaxSAT) formula. Consequently, our method guarantees consistency across observations and simplifies the fault localisation procedure. Experimental results on three benchmark sets, two of C programs, TCAS and C-Pack-IPAs, and one of Boolean circuits, ISCAS85, show that CFaults is faster at localising faults in C software than other FBFL approaches such as BugAssist, SNIPER, and HSD. On the ISCAS85 benchmark, CFaults is generally slower than HSD; however, it localises faults in only 6% fewer circuits, demonstrating that it remains competitive in this domain. Furthermore, CFaults produces only subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses (e.g., BugAssist and SNIPER).

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [DOLMA: A Data Object Level Memory Disaggregation Framework for HPC Applications](https://arxiv.org/abs/2512.02300)
*Haoyu Zheng,Shouwei Gao,Jie Ren,Wenqian Dong*

Main category: cs.DC

TL;DR: DOLMA 是一种面向高性能计算（HPC）的数据对象级内存解耦框架，通过智能识别数据对象、定量分析本地内存需求、利用可预测的访问模式进行远程预取，在显著降低本地内存使用的同时将性能损失控制在16%以内。


<details>
  <summary>Details</summary>
Motivation: 内存解耦虽能提升HPC系统内存容量和利用率，但远程内存访问带来的性能开销对计算密集型HPC应用影响显著，因其执行时间高度依赖数据局部性。

Method: DOLMA 框架通过智能识别并卸载数据对象至远程内存，提供本地内存大小的定量分析；利用HPC应用中可预测的内存访问模式，采用双缓冲设计实现远程内存预取，并平衡本地/远程内存使用与多线程并发。

Result: 在8个HPC工作负载和计算核上的评估表明，DOLMA 平均最多减少63%的本地内存使用，同时将性能下降控制在16%以内。

Conclusion: DOLMA 为HPC领域提供了一种灵活高效的内存解耦方案，在大幅降低本地内存需求的同时，对应用性能影响极小。

Abstract: Memory disaggregation is promising to scale memory capacity and improves utilization in HPC systems. However, the performance overhead of accessing remote memory poses a significant challenge, particularly for compute-intensive HPC applications where execution times are highly sensitive to data locality. In this work, we present DOLMA, a Data Object Level M emory dis Aggregation framework designed for HPC applications. DOLMA intelligently identifies and offloads data objects to remote memory, while providing quantitative analysis to decide a suitable local memory size. Furthermore, DOLMA leverages the predictable memory access patterns typical in HPC applications and enables remote memory prefetch via a dual-buffer design. By carefully balancing local and remote memory usage and maintaining multi-thread concurrency, DOLMA provides a flexible and efficient solution for leveraging disaggregated memory in HPC domains while minimally compromising application performance. Evaluating with eight HPC workloads and computational kernels, DOLMA limits performance degradation to less than 16% while reducing local memory usage by up to 63%, on average.

</details>


### [9] [Offloading Artificial Intelligence Workloads across the Computing Continuum by means of Active Storage Systems](https://arxiv.org/abs/2512.02646)
*Alex Barceló,Sebastián A. Cajas Ordoñez,Jaydeep Samanta,Andrés L. Suárez-Cetrulo,Romila Ghosh,Ricardo Simón Carbajo,Anna Queralt*

Main category: cs.DC

TL;DR: 本文提出一种基于主动存储的软件架构，用于在计算连续体中高效分发AI工作负载，显著提升内存效率和训练速度，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 传统云架构难以应对AI工作负载带来的海量数据处理需求，存在存储、计算和数据传输效率低下的问题；现有分布式框架缺乏对设备异构性和算法快速演进的适应能力。

Method: 通过将计算嵌入存储架构（即主动存储），利用dataClay平台与主流Python库实现一个可在计算连续体中无缝分发AI工作负载的软件架构。

Result: 实验表明，通过主动存储卸载工作负载可显著提高内存效率和训练速度，同时在不同设备上保持模型准确性，并降低资源消耗。

Conclusion: 主动存储有望革新AI工作负载管理方式，使分布式AI部署更具可扩展性和资源效率，且对领域专家和开发者具有较低使用门槛。

Abstract: The increasing demand for artificial intelligence (AI) workloads across diverse computing environments has driven the need for more efficient data management strategies. Traditional cloud-based architectures struggle to handle the sheer volume and velocity of AI-driven data, leading to inefficiencies in storage, computation, and data movement. This paper explores the integration of active storage systems within the computing continuum to optimize AI workload distribution.
  By embedding computation directly into storage architectures, active storage is able to reduce data transfer overhead, enhancing performance and improving resource utilization. Other existing frameworks and architectures offer mechanisms to distribute certain AI processes across distributed environments; however, they lack the flexibility and adaptability that the continuum requires, both regarding the heterogeneity of devices and the rapid-changing algorithms and models being used by domain experts and researchers.
  This article proposes a software architecture aimed at seamlessly distributing AI workloads across the computing continuum, and presents its implementation using mainstream Python libraries and dataClay, an active storage platform. The evaluation shows the benefits and trade-offs regarding memory consumption, storage requirements, training times, and execution efficiency across different devices. Experimental results demonstrate that the process of offloading workloads through active storage significantly improves memory efficiency and training speeds while maintaining accuracy. Our findings highlight the potential of active storage to revolutionize AI workload management, making distributed AI deployments more scalable and resource-efficient with a very low entry barrier for domain experts and application developers.

</details>


### [10] [Distributed and Autonomic Minimum Spanning Trees](https://arxiv.org/abs/2512.02683)
*Luiz A. Rodrigues,Elias P. Duarte,Luciana Arantes*

Main category: cs.DC

TL;DR: 本文提出了一种自适应算法，用于在分布式系统中构建和维护一种深度与节点入度均不超过 log₂n 的生成树，并基于该结构实现了可扩展的尽力而为和可靠广播机制。


<details>
  <summary>Details</summary>
Motivation: 传统的一对多广播方法在分布式系统中不可扩展，会给发送方带来沉重负载；因此需要一种能自动构建、容错且可扩展的通信结构来支持高效广播。

Method: 利用VCube虚拟拓扑作为故障检测器，设计一种自适应算法，动态构建并维护一个所有节点入度和树深度至多为 log₂n 的生成树；在此基础上实现两种广播算法：尽力而为广播和可靠广播。

Result: 仿真结果表明，该方法在容错性（最多容忍 n−1 个进程失效）和可扩展性方面优于其他方案，且在所有进程正常时每个节点的度数恰好为 log₂n。

Conclusion: 所提出的自适应生成树算法能够有效支持分布式系统中的可扩展、容错广播通信，具有良好的性能和鲁棒性。

Abstract: The most common strategy for enabling a process in a distributed system to broadcast a message is one-to-all communication. However, this approach is not scalable, as it places a heavy load on the sender. This work presents an autonomic algorithm that enables the $n$ processes in a distributed system to build and maintain a spanning tree connecting themselves. In this context, processes are the vertices of the spanning tree. By definition, a spanning tree connects all processes without forming cycles. The proposed algorithm ensures that every vertex in the spanning tree has both an in-degree and the tree depth of at most $log_2 n$. When all processes are correct, the degree of each process is exactly $log_2 n$. A spanning tree is dynamically created from any source process and is transparently reconstructed as processes fail or recover. Up to $n-1$ processes can fail, and the correct processes remain connected through a scalable, functioning spanning tree. To build and maintain the tree, processes use the VCube virtual topology, which also serves as a failure detector. Two broadcast algorithms based on the autonomic spanning tree algorithm are presented: one for best-effort broadcast and one for reliable broadcast. Simulation results are provided, including comparisons with other alternatives.

</details>


### [11] [Designing FAIR Workflows at OLCF: Building Scalable and Reusable Ecosystems for HPC Science](https://arxiv.org/abs/2512.02818)
*Sean R. Wilkinson,Patrick Widener,Sarp Oral,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 本文提出HPC中心应积极推动跨学科的FAIR生态系统建设，通过使工作流中的单个组件（而非整个工作流）符合FAIR原则，提升科研数字成果的可发现性、共享性和复用性。


<details>
  <summary>Details</summary>
Motivation: 当前HPC用户常因系统环境差异而重复开发与特定中心绑定的数字工件，造成资源浪费；尽管各学科社区已开展FAIR实践，但存在领域孤岛问题，限制了跨学科协作。

Method: 借鉴欧洲开放科学云（EOSC）中EOSC-Life FAIR工作流协作平台的架构，设计一种面向HPC环境的组件级FAIR模型，强调对工作流中独立组件实施FAIR原则。

Result: 该模型能更好地适应HPC用户多样化和动态变化的需求，提升数字工件的长期价值，并促进跨学科研究中的资源共享与重用。

Conclusion: HPC中心应在推动跨学科FAIR生态中发挥更积极作用，通过基础设施设计支持组件级FAIR实践，从而增强科研效率与协作广度。

Abstract: High Performance Computing (HPC) centers provide advanced infrastructure that enables scientific research at extreme scale. These centers operate with hardware configurations, software environments, and security requirements that differ substantially from most users' local systems. As a result, users often develop customized digital artifacts that are tightly coupled to a given HPC center. This practice can lead to significant duplication of effort as multiple users independently create similar solutions to common problems. The FAIR Principles offer a framework to address these challenges. Initially designed to improve data stewardship, the FAIR approach has since been extended to encompass software, workflows, models, and infrastructure. By encouraging the use of rich metadata and community standards, FAIR practices aim to make digital artifacts easier to share and reuse, both within and across scientific domains. Many FAIR initiatives have emerged within individual research communities, often aligned by discipline (e.g. bioinformatics, earth sciences). These communities have made progress in adopting FAIR practices, but their domain-specific nature can lead to silos that limit broader collaboration. Thus, we propose that HPC centers play a more active role in fostering FAIR ecosystems that support research across multiple disciplines. This requires designing infrastructure that enables researchers to discover, share, and reuse computational components more effectively. Here, we build on the architecture of the European Open Science Cloud (EOSC) EOSC-Life FAIR Workflows Collaboratory to propose a model tailored to the needs of HPC. Rather than focusing on entire workflows, we emphasize the importance of making individual workflow components FAIR. This component-based approach better supports the diverse and evolving needs of HPC users while maximizing the long-term value of their work.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [12] [ProtO-RU: An O-RAN Split-7.2 Radio Unit using SDRs](https://arxiv.org/abs/2512.02398)
*Zhiyu Zhou,Xin Zhe Khooi,Satis Kumar Permal,Mun Choon Chan*

Main category: cs.NI

TL;DR: ProtO-RU 是首个基于开源软件和通用硬件实现的 O-RAN Split-7.2 射频单元，具备可编程性、兼容性和商用级性能，推动了 O-RAN 端到端研究的普及。


<details>
  <summary>Details</summary>
Motivation: 现有商用 O-RU 多为封闭式硬件实现，限制了射频单元层面的创新与研究；因此需要一个开源、可编程且能与现有 5G 软件栈兼容的 O-RU 实现。

Method: 基于开源 srsRAN 软件栈，在 SDR 和通用 CPU 上构建完全可编程的 O-RAN Split-7.2 射频单元（ProtO-RU），并集成 srsRAN 与 OpenAirInterface5G 的 CU/DU 软件栈。

Result: ProtO-RU 支持 TDD/FDD 双工模式，可与商用 5G 终端互操作，在多用户持续负载下保持稳定，并实现与 Split-8 及商用 O-RU 相当的吞吐性能。

Conclusion: ProtO-RU 为 O-RAN 射频单元层级的研究与创新提供了开放平台，显著降低了端到端 O-RAN 研究的门槛。

Abstract: We present ProtO-RU, the first open source, software-defined O-RAN Split-7.2 Radio Unit built using SDRs and commodity CPUs. Unlike proprietary hardware-based commercial O-RUs, ProtO-RU is built on the open-source srsRAN software stack, and it is fully programmable. We demonstrate that ProtO-RU integrates with the srsRAN and OpenAirInterface5G CU/DU stacks, supports both TDD and FDD duplexing modes, and interoperates with commercial 5G UEs. Our evaluation shows that ProtO-RU remains stable under sustained load with multiple UEs and delivers throughput comparable to Split-8 and commercial O-RUs. ProtO-RU opens up new opportunities for RU-level innovations and lowers the barrier of entry for end-to-end O-RAN research.

</details>


### [13] [Wi-Fi Rate Adaptation for Moving Equipment in Industrial Environments](https://arxiv.org/abs/2512.02455)
*Pietro Chiavassa,Stefano Scanzio,Gianluca Cena*

Main category: cs.NI

TL;DR: 本文评估了Minstrel速率自适应算法在静态和移动场景下的性能，重点关注工业环境中关键的延迟和丢包率指标，为未来基于集中式数字孪生的增强型速率自适应算法开发提供初步依据。


<details>
  <summary>Details</summary>
Motivation: 工业环境中对无线通信（如Wi-Fi）提出了严格的可靠性要求，尤其是有界传输延迟。而Wi-Fi的传输速率选择显著影响数据包成功送达的概率，现有速率自适应算法（如Minstrel）在工业场景下的性能尚需评估，以支持未来更优算法的设计。

Method: 对Linux内核默认的开源速率自适应算法Minstrel在静态和移动两种场景下进行性能评估，重点考察其在工业应用中关注的延迟和丢包率等指标。

Result: 论文提供了Minstrel算法在不同场景下关于延迟和丢包率的具体性能数据，作为后续研究的基线。

Conclusion: 对Minstrel的评估结果可作为初步参考，用于指导未来面向工业应用、基于集中式数字孪生技术的增强型速率自适应算法的开发。

Abstract: Wi-Fi is currently considered one of the most promising solutions for interconnecting mobile equipment (e.g., autonomous mobile robots and active exoskeletons) in industrial environments. However, relability requirements imposed by the industrial context, such as ensuring bounded transmission latency, are a major challenge for over-the-air communication. One of the aspects of Wi-Fi technology that greatly affects the probability of a packet reaching its destination is the selection of the appropriate transmission rate. Rate adaptation algorithms are in charge of this operation, but their design and implementation are not regulated by the IEEE 802.11 standard. One of the most popular solutions, available as open source, is Minstrel, which is the default choice for the Linux Kernel. In this paper, Minstrel performance is evaluated for both static and mobility scenarios. Our analysis focuses on metrics of interest for industrial contexts, i.e., latency and packet loss ratio, and serves as a preliminary evaluation for the future development of enhanced rate adaptation algorithms based on centralized digital twins.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [14] [Decentralized Multi-Agent System with Trust-Aware Communication](https://arxiv.org/abs/2512.02410)
*Yepeng Ding,Ahmed Twabi,Junwei Yu,Lingfeng Zhang,Tohru Kondo,Hiroyuki Sato*

Main category: cs.MA

TL;DR: 本文提出了一种基于区块链的去中心化多智能体系统（DMAS）架构，以解决传统集中式多智能体系统中存在的单点故障、审查风险、可扩展性限制和信任问题。


<details>
  <summary>Details</summary>
Motivation: 传统集中式多智能体系统存在单点故障、易受审查、可扩展性差和信任机制薄弱等关键问题，亟需一种更安全、可扩展且抗审查的新架构。

Method: 设计了一种去中心化的多智能体系统架构（DMAS），其核心包括基于区块链的去中心化智能体运行时环境，并提出了一种结合密码学原语与链上操作的信任感知通信协议。

Result: 该架构实现了可验证的交互周期、通信完整性、身份真实性、不可否认性以及条件保密性；性能分析表明DMAS具备良好的可扩展性和效率。

Conclusion: 所提出的DMAS架构有效解决了传统多智能体系统的根本缺陷，为构建可信、安全、可扩展的自主多智能体系统提供了可行路径。

Abstract: The emergence of Large Language Models (LLMs) is rapidly accelerating the development of autonomous multi-agent systems (MAS), paving the way for the Internet of Agents. However, traditional centralized MAS architectures present significant challenges, including single points of failure, vulnerability to censorship, inherent scalability limitations, and critical trust issues. We propose a novel Decentralized Multi-Agent System (DMAS) architecture designed to overcome these fundamental problems by enabling trust-aware, scalable, and censorship-resistant interactions among autonomous agents. Our DMAS features a decentralized agent runtime underpinned by a blockchain-based architecture. We formalize a trust-aware communication protocol that leverages cryptographic primitives and on-chain operations to provide security properties: verifiable interaction cycles, communication integrity, authenticity, non-repudiation, and conditional confidentiality, which we further substantiate through a comprehensive security analysis. Our performance analysis validates the DMAS as a scalable and efficient solution for building trustworthy multi-agent systems.

</details>


### [15] [EZYer: A simulacrum of high school with generative agent](https://arxiv.org/abs/2512.02561)
*Jinming Yang,Zimu Ji,Weiqi Luo,Gaoxi Wang,Bin Ma,Yueling Deng*

Main category: cs.MA

TL;DR: 本文提出了一种名为 EZYer 的生成式教育智能体，包含教师模块、学生模块和控制器三部分，分别用于自动生成结构化教学材料与课件、协作生成学习笔记以及保障内容的学术严谨性，并通过五维指标评估证明其生成内容质量优异。


<details>
  <summary>Details</summary>
Motivation: 现有在线教育工具在课件生成、交互式笔记和内容质量保障方面存在服务不完整、性能不足和互动性弱的问题，亟需结合大语言模型技术提升教育智能化水平。

Method: EZYer 包含三个核心模块：1）教师模块融合文本语料检索与深度生成技术，自动生成符合高中数学教学大纲的结构化教材和 LaTeX Beamer 课件，支持用户插入图片；2）学生模块通过教师、助教、优等生和后进生四角色协作互动，由笔记记录者生成学术笔记；3）控制器设置关键词过滤、内容评分、角色互验和动态修正机制，确保输入输出的学术规范性。评估采用内容准确性、知识覆盖度、可用性、格式正确性及视觉设计五个维度，由五个大语言模型对100份生成内容打分。

Result: 评估结果显示 EZYer 生成的 Beamer 课件和学习笔记在五项指标上表现优秀，具备良好的应用前景。

Conclusion: EZYer 有效解决了当前教育工具在内容生成与互动性方面的不足，其多模块协同架构和严格的内容控制机制显著提升了生成内容的质量与教学适用性。

Abstract: With the rapid development of the online education and large language model, the existing educational tools still suffer from incomplete service, insufficient performance and weak interactivity in terms of courseware generation, interactive notes and quality assurance of content. In particular, the proposed generative agent EZYer : 1) Teacher Module: Integrating the Text Corpus retrieval and in-depth generation technologies, it automatically generates structured teaching materials and LaTeX Beamer courseware in line with the high school mathematics syllabus and supports user-defined image insertion. 2) Student Module: Throughout the collaborative interaction of the four roles of Teacher, Assistant, Top Student and Struggling Student, Note Taker summarizes and generates academic notes to enhance the depth and interest of learning. 3) Controller: set up keyword filtering system, content scoring system, role co-validation system, and dynamic content correction system. This ensure academic strictness and pedagogical propriety of EZYer inputs and outputs. In order to evaluate EZYer, this paper designs five-dimensional evaluation indexes of content accuracy, knowledge coverage, usability, formatting correctness and visual design and appeal, and scores 100 Beamer and Notes generated by EZYer by five large language models, separately, and the results show that the quality of EZYer-generated content is excellent and has a good application prospect.

</details>


### [16] [Beyond Single-Agent Safety: A Taxonomy of Risks in LLM-to-LLM Interactions](https://arxiv.org/abs/2512.02682)
*Piercosma Bisconti,Marcello Galisai,Federico Pierucci,Marcantonio Bracale,Matteo Prandi*

Main category: cs.MA

TL;DR: 当前针对人与大语言模型（LLM）交互设计的安全机制无法有效应对多个LLM相互作用的复杂环境，本文提出从模型级安全转向系统级安全，并引入“涌现系统性风险视界”（ESRH）框架和InstitutionalAI架构以应对多智能体系统中的集体风险。


<details>
  <summary>Details</summary>
Motivation: 现有安全机制主要针对单个LLM在人类监督下的行为进行约束，但随着LLM之间相互交互的生态系统迅速发展，这些机制无法管控多模型交互中产生的系统性风险。

Method: 提出“涌现系统性风险视界”（ESRH）理论框架，分析交互结构如何引发系统不稳定；构建微-中-宏观层面的失败模式分类法；设计InstitutionalAI架构，将自适应监管嵌入多智能体系统。

Result: 揭示了即使每个LLM个体对齐良好，其交互仍可能导致集体失效；提供了理解与治理多LLM系统风险的新理论视角与技术路径。

Conclusion: 需将安全治理范式从单模型扩展至整个系统层面，通过制度化、自适应的监管机制应对LLM交互带来的涌现性风险。

Abstract: This paper examines why safety mechanisms designed for human-model interaction do not scale to environments where large language models (LLMs) interact with each other. Most current governance practices still rely on single-agent safety containment, prompts, fine-tuning, and moderation layers that constrain individual model behavior but leave the dynamics of multi-model interaction ungoverned. These mechanisms assume a dyadic setting: one model responding to one user under stable oversight. Yet research and industrial development are rapidly shifting toward LLM-to-LLM ecosystems, where outputs are recursively reused as inputs across chains of agents. In such systems, local compliance can aggregate into collective failure even when every model is individually aligned. We propose a conceptual transition from model-level safety to system-level safety, introducing the framework of the Emergent Systemic Risk Horizon (ESRH) to formalize how instability arises from interaction structure rather than from isolated misbehavior. The paper contributes (i) a theoretical account of collective risk in interacting LLMs, (ii) a taxonomy connecting micro, meso, and macro-level failure modes, and (iii) a design proposal for InstitutionalAI, an architecture for embedding adaptive oversight within multi-agent systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis](https://arxiv.org/abs/2512.02189)
*Aaron Jarmusch,Sunita Chandrasekaran*

Main category: cs.AR

TL;DR: 本文提出了一套开源微基准测试套件，系统评估了NVIDIA Blackwell（B200）GPU相较于H200在张量核心、内存子系统和多种浮点精度下的性能提升，发现B200在混合精度吞吐量上提升1.56倍，能效提高42%，并显著降低缓存未命中时的内存访问延迟。


<details>
  <summary>Details</summary>
Motivation: 随着GPU架构快速演进以满足E级计算和机器学习需求，业界缺乏系统性方法来量化新架构特性对多样化工作负载的性能影响，阻碍了开发者优化应用和指导未来硬件设计。

Method: 构建开源微基准测试套件，对Blackwell（B200）与H200 GPU在内存子系统、张量核心流水线及FP32/FP16/FP8/FP6/FP4等精度下进行系统性对比评估，涵盖稠密/稀疏GEMM、Transformer推理与训练等工作负载。

Result: B200相比H200实现1.56倍更高的混合精度吞吐量、42%更优的能效，并在缓存未命中场景下将内存访问延迟降低58%，显著影响算法设计策略。

Conclusion: 所提出的微基准套件为开发者提供了实用工具以充分利用现代GPU架构特性，有助于做出更优的架构决策，并为未来GPU设计提供实证依据。

Abstract: As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.
  Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies.

</details>


### [18] [Near-Memory Architecture for Threshold-Ordinal Surface-Based Corner Detection of Event Cameras](https://arxiv.org/abs/2512.02346)
*Hongyang Shang,An Guo,Shuai Dong,Junyi Yang,Ye Ke,Arindam Basu*

Main category: cs.AR

TL;DR: 本文提出了一种面向事件相机角点检测的近存架构NM-TOS，通过读写解耦的8T SRAM单元、流水线优化、软硬件协同设计及DVFS技术，在65nm工艺下显著降低延迟与能耗，同时保持高检测精度。


<details>
  <summary>Details</summary>
Motivation: 事件相机虽具备高速与低功耗优势，但其基于TOS等表示的角点检测算法在资源受限边缘设备上存在高延迟问题，限制了实际应用。

Method: 提出NM-TOS近存架构，采用读写解耦8T SRAM单元、流水线加速局部块更新，并结合软硬件协同优化的外围电路与动态电压频率调节（DVFS）策略。

Result: 在65nm CMOS工艺下，相比传统数字实现，该架构在1.2V时延迟/能耗降低24.7倍/1.2倍，在0.6V时分别降低1.93倍/6.6倍；蒙特卡洛仿真显示0.62V以上电压零误码率，角点检测AUC仅轻微下降0.015–0.027。

Conclusion: 所提NM-TOS架构有效解决了事件相机角点检测在边缘设备上的延迟与能效瓶颈，在保证鲁棒性与精度的同时显著提升系统效率。

Abstract: Event-based Cameras (EBCs) are widely utilized in surveillance and autonomous driving applications due to their high speed and low power consumption. Corners are essential low-level features in event-driven computer vision, and novel algorithms utilizing event-based representations, such as Threshold-Ordinal Surface (TOS), have been developed for corner detection. However, the implementation of these algorithms on resource-constrained edge devices is hindered by significant latency, undermining the advantages of EBCs. To address this challenge, a near-memory architecture for efficient TOS updates (NM-TOS) is proposed. This architecture employs a read-write decoupled 8T SRAM cell and optimizes patch update speed through pipelining. Hardware-software co-optimized peripheral circuits and dynamic voltage and frequency scaling (DVFS) enable power and latency reductions. Compared to traditional digital implementations, our architecture reduces latency/energy by 24.7x/1.2x at Vdd = 1.2 V or 1.93x/6.6x at Vdd = 0.6 V based on 65nm CMOS process. Monte Carlo simulations confirm robust circuit operation, demonstrating zero bit error rate at operating voltages above 0.62 V, with only 0.2% at 0.61 V and 2.5% at 0.6 V. Corner detection evaluation using precision-recall area under curve (AUC) metrics reveals minor AUC reductions of 0.027 and 0.015 at 0.6 V for two popular EBC datasets.

</details>


### [19] [SAT-MapIt: A SAT-based Modulo Scheduling Mapper for Coarse Grain Reconfigurable Architectures](https://arxiv.org/abs/2512.02875)
*Cristian Tirelli,Lorenzo Ferretti,Laura Pozzi*

Main category: cs.AR

TL;DR: 本文提出一种基于SAT（布尔可满足性）的新型映射方法SAT-MapIt，用于将计算密集型循环高效映射到粗粒度可重构阵列（CGRAs）上。该方法引入“核迁移调度”（KMS）来构建约束，并利用SAT求解器探索解空间，在47.72%的基准测试中优于现有技术，有时能获得更低的迭代间隔（II）或找到此前无法获得的有效映射。


<details>
  <summary>Details</summary>
Motivation: 现有CGRAs映射方法依赖模调度和图算法（如最大团枚举），难以充分探索解空间，限制了映射质量；作者旨在通过SAT形式化方法更有效地搜索高质量映射。

Method: 提出一种基于SAT的映射方法：引入“核迁移调度”（KMS），结合数据流图和CGRA架构信息，将映射问题转化为一组布尔约束；利用SAT求解器在给定迭代间隔（II）下寻找可行映射，若失败则递增II并重复过程。

Result: 实验表明，SAT-MapIt在47.72%的基准测试中优于现有技术，部分情况下获得更低的II，甚至在原有方法无法找到有效映射时成功找到。

Conclusion: 基于SAT的映射方法能更有效地探索CGRAs映射解空间，显著提升映射质量，证明了其在加速计算密集型循环方面的潜力。

Abstract: Coarse-Grain Reconfigurable Arrays (CGRAs) are emerging low-power architectures aimed at accelerating compute-intensive application loops. The acceleration that a CGRA can ultimately provide, however, heavily depends on the quality of the mapping, i.e. on how effectively the loop is compiled onto the given platform. State of the Art compilation techniques achieve mapping through modulo scheduling, a strategy which attempts to minimize the II (Iteration Interval) needed to execute a loop, and they do so usually through well known graph algorithms, such as Max-Clique Enumeration.
  We address the mapping problem through a SAT formulation, instead, and thus explore the solution space more effectively than current SoA tools. To formulate the SAT problem, we introduce an ad-hoc schedule called the \textit{kernel mobility schedule} (KMS), which we use in conjunction with the data-flow graph and the architectural information of the CGRA in order to create a set of boolean statements that describe all constraints to be obeyed by the mapping for a given II. We then let the SAT solver efficiently navigate this complex space. As in other SoA techniques, the process is iterative: if a valid mapping does not exist for the given II, the II is increased and a new KMS and set of constraints is generated and solved.
  Our experimental results show that SAT-MapIt obtains better results compared to SoA alternatives in $47.72\%$ of the benchmarks explored: sometimes finding a lower II, and others even finding a valid mapping when none could previously be found.

</details>


### [20] [Mapping code on Coarse Grained Reconfigurable Arrays using a SAT solver](https://arxiv.org/abs/2512.02884)
*Cristian Tirelli,Laura Pozzi*

Main category: cs.AR

TL;DR: 本文提出一种基于SAT求解的新型编译方法——核迁移调度（Kernel Mobility Schedule），用于在粗粒度可重构阵列（CGRA）上为给定数据流图寻找最小迭代间隔（II）的映射，从而提升映射质量并缩短编译时间。


<details>
  <summary>Details</summary>
Motivation: 现有CGRA编译技术虽使用模调度优化迭代间隔（II），但在映射质量和编译效率方面仍有提升空间；作者旨在通过更优的调度建模方法，为任意CGRA拓扑结构找到最低II的有效映射。

Method: 将CGRA映射问题形式化为可满足性（SAT）问题，提出“核迁移调度”来编码给定数据流图（DFG）和目标II下所有可能的映射，并结合CGRA架构信息生成约束条件，利用SAT求解器寻找有效映射。

Result: 实验表明，该方法相比现有先进技术，不仅平均编译时间更短，而且能获得更高质量的映射结果。

Conclusion: 所提出的基于SAT与核迁移调度的编译方法在CGRA映射中兼顾了效率与质量，优于当前主流技术。

Abstract: Emerging low-powered architectures like Coarse-Grain Reconfigurable Arrays (CGRAs) are becoming more common. Often included as co-processors, they are used to accelerate compute-intensive workloads like loops. The speedup obtained is defined by the hardware design of the accelerator and by the quality of the compilation. State of the art (SoA) compilation techniques leverage modulo scheduling to minimize the Iteration Interval (II), exploit the architecture parallelism and, consequentially, reduce the execution time of the accelerated workload. In our work, we focus on improving the compilation process by finding the lowest II for any given topology, through a satisfiability (SAT) formulation of the mapping problem. We introduce a novel schedule, called Kernel Mobility Schedule, to encode all the possible mappings for a given Data Flow Graph (DFG) and for a given II. The schedule is used together with the CGRA architectural information to generate all the constraints necessary to find a valid mapping. Experimental results demonstrate that our method not only reduces compilation time on average but also achieves higher quality mappings compared to existing SoA techniques.

</details>
