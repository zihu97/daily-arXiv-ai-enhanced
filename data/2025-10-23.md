<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 14]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.OS](#cs.OS) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CosmoCore Affective Dream-Replay Reinforcement Learning for Code Generation](https://arxiv.org/abs/2510.18895)
*Santhosh Kumar Ravindran*

Main category: cs.SE

TL;DR: CosmoCore 是一种受神经科学启发的强化学习架构，通过引入情感信号（如“尴尬”）来提升大语言模型在代码生成中的表现，显著减少幻觉代码并加速自我修正。


<details>
  <summary>Details</summary>
Motivation: 受人类和动物学习机制启发，特别是错误引发的“尴尬”情绪能促使快速修正行为（如训狗时一次责备即可避免重复错误），作者希望将此类情感信号融入大语言模型的代码生成过程中，以提升其纠错能力和生成质量。

Method: 使用轻量多层感知机（MLP）为代码生成轨迹打上情感价（valence）和惊讶度（surprise）标签；高负向情感（如产生错误代码）的样本被放入“Dream Queue”进行五倍回放用于离策略更新，而低惊讶度的成功样本则被剪枝以避免过度自信和缓冲区膨胀。

Result: 在 HumanEval 和 BigCodeBench 等代码生成基准以及自定义数据管道环境中，CosmoCore 将幻觉代码（如语法错误或逻辑漏洞）减少 48%，自我修正速度提升 45%；消融实验验证了情感标签提升探索好奇心，剪枝策略提升效率。

Conclusion: CosmoCore 成功将情感信号融入强化学习框架，扩展了基于人类反馈的强化学习（RLHF），为构建更具情感意识的代码助手提供了新路径，适用于集成开发环境（IDE）和数据管道等场景。

Abstract: We introduce CosmoCore, a neuroscience-inspired reinforcement learning (RL)
architecture that integrates affective signals to enhance code generation in
large language models (LLMs). Motivated by human and animal learning where
embarrassment from mistakes drives rapid correction, as observed in training a
puppy to avoid repeating errors after a single scolding CosmoCore tags code
generation trajectories with valence and surprise using a lightweight
multi-layer perceptron (MLP). High-negative valence (cringe) episodes, such as
buggy code outputs, are prioritized in a Dream Queue for five-fold replay
during off-policy updates, while low-surprise successes are pruned to prevent
overconfidence and buffer bloat. Evaluated on code generation benchmarks like
HumanEval and BigCodeBench, alongside simulations with a custom data pipeline
environment, CosmoCore reduces hallucinated code (e.g., syntax errors or
logical bugs) by 48\% and accelerates self-correction by 45\%. Local
experiments using Hugging Face models in a PySpark environment validate these
gains, with code snippets provided for replication. Ablations confirm valence
tagging boosts curiosity in exploration, and pruning mitigates inefficiency.
This framework extends RL from human feedback (RLHF) for more emotionally aware
code assistants, with applications in IDEs and data pipelines. Code and the
custom mini-world simulation are released.

</details>


### [2] [A Survey on Feedback Types in Automated Programming Assessment Systems](https://arxiv.org/abs/2510.18923)
*Eduard Frankford,Tobias Antensteiner,Michael Vierhauser,Clemens Sauerwein,Vivien Wallner,Iris Groher,Reinhold Plösch,Ruth Breu*

Main category: cs.SE

TL;DR: 本文研究了在自动编程评估系统（APAS）中，不同反馈机制（编译器反馈、单元测试反馈和基于大语言模型的反馈）对学生学习体验和编程表现的影响。通过对200多名学生的实验发现，尽管学生认为单元测试反馈最有帮助，但LLM生成的反馈显著提升了编程表现，建议结合两者以优化学习效果。


<details>
  <summary>Details</summary>
Motivation: 随着编程课程在各类专业中的普及，教学资源压力增大，自动编程评估系统（APAS）被广泛采用。然而，传统APAS依赖预定义单元测试，反馈内容有限。大语言模型（LLM）的发展为提升反馈质量与个性化提供了新可能，因此有必要评估不同反馈机制的实际效果与学生感知。

Method: 开展大规模对照实验，招募来自两所大学的200多名学生，比较三种反馈机制：编译器反馈、单元测试反馈和基于LLM的反馈，从学生感知质量和对问题解决表现的影响两个维度进行评估。

Result: 学生普遍认为单元测试反馈最有帮助，但接受LLM反馈的学生在编程任务中的表现显著更好。

Conclusion: 结合单元测试反馈的明确性与LLM反馈的个性化指导，可优化自动反馈机制，有效提升编程教育的学习成效。

Abstract: With the recent rapid increase in digitization across all major industries,
acquiring programming skills has increased the demand for introductory
programming courses. This has further resulted in universities integrating
programming courses into a wide range of curricula, including not only
technical studies but also business and management fields of study.
  Consequently, additional resources are needed for teaching, grading, and
tutoring students with diverse educational backgrounds and skills. As part of
this, Automated Programming Assessment Systems (APASs) have emerged, providing
scalable and high-quality assessment systems with efficient evaluation and
instant feedback. Commonly, APASs heavily rely on predefined unit tests for
generating feedback, often limiting the scope and level of detail of feedback
that can be provided to students. With the rise of Large Language Models (LLMs)
in recent years, new opportunities have emerged as these technologies can
enhance feedback quality and personalization.
  To investigate how different feedback mechanisms in APASs are perceived by
students, and how effective they are in supporting problem-solving, we have
conducted a large-scale study with over 200 students from two different
universities. Specifically, we compare baseline Compiler Feedback, standard
Unit Test Feedback, and advanced LLM-based Feedback regarding perceived quality
and impact on student performance.
  Results indicate that while students rate unit test feedback as the most
helpful, AI-generated feedback leads to significantly better performances.
These findings suggest combining unit tests and AI-driven guidance to optimize
automated feedback mechanisms and improve learning outcomes in programming
education.

</details>


### [3] [Docker-based CI/CD for Rocq/OCaml projects](https://arxiv.org/abs/2510.19089)
*Érik Martin-Dorel*

Main category: cs.SE

TL;DR: 本文介绍了三个紧密相关的软件项目：docker-coq、docker-coq-action 和 docker-keeper，旨在推广基于 Docker 的 CI/CD 在 Rocq（原 Coq）或 OCaml 项目中的使用，并为未来维护者记录其设计决策与技术需求。


<details>
  <summary>Details</summary>
Motivation: 推动 Rocq/OCaml 项目采用基于 Docker 的持续集成与部署（CI/CD）流程，并为这些 DevOps 工具的后续维护提供文档支持。

Method: 对 docker-coq、docker-coq-action 和 docker-keeper 三个项目进行高阶功能描述，并分析其底层需求与核心设计选择。

Result: 提供了清晰的功能概述和架构说明，有助于用户采用这些工具并支持其长期维护。

Conclusion: 这三个工具共同构成了支持 Rocq/OCaml 项目 DevOps 的 Docker 化解决方案，其设计文档有助于社区更有效地使用和维护。

Abstract: This paper presents three closely-related software projects, namely:
docker-coq, docker-coq-action, and docker-keeper. It aims at two objectives:
provide a high-level description of the available features -- to foster the use
of a Docker-based CI/CD for Rocq (formerly known as Coq) or OCaml projects --
and document the underlying requirements and the main design choices of these
three DevOps tools -- to help their future maintainers.

</details>


### [4] [Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary](https://arxiv.org/abs/2510.19692)
*Rashina Hoda*

Main category: cs.SE

TL;DR: 本文提出将智能体软件工程（Agentic SE）的研究范围从仅关注编码扩展到覆盖整个软件工程流程，倡导建立指导原则与统一术语，以推动该领域有意识、可持续地发展。


<details>
  <summary>Details</summary>
Motivation: 当前智能体AI在软件工程中的应用主要聚焦于编码活动，但实践表明需考虑更广泛的社交技术因素；为引导社区建立坚实基础，作者呼吁扩展研究视野。

Method: 通过结合软件工程的基础理论、发展历程及新兴的智能体SE框架，提出范围扩展建议、初步价值观与原则，并提供术语设计与使用的指导。

Result: 提出了“全流程”视角的智能体SE愿景，制定了一套初步的价值观与原则，并建议采用清晰定义的术语体系。

Conclusion: 通过扩大研究范围、确立指导原则和统一术语，可促进社区协作，使智能体SE的发展不仅是必然的，更是有意识且可取的。

Abstract: Agentic AI is poised to usher in a seismic paradigm shift in Software
Engineering (SE). As technologists rush head-along to make agentic AI a
reality, SE researchers are driven to establish agentic SE as a research area.
While early visions of agentic SE are primarily focused on code-related
activities, early empirical evidence calls for a consideration of a range of
socio-technical concerns to make it work in practice. This paper contributes to
the emerging community vision by: (a) recommending an expansion of its scope
beyond code, toward a 'whole of process' vision, grounding it in SE foundations
and evolution and emerging agentic SE frameworks, (b) proposing a preliminary
set of values and principles to guide efforts, and (c) sharing guidance on
designing/using well-defined vocabulary for agentic SE. It is hoped that these
ideas will encourage community collaborations and steer the SE community
towards laying strong foundations of agentic SE so its not only inevitable but
also deliberate and desirable in the long run.

</details>


### [5] [A General Solution for the Implementation of CI/CD in Embedded Linux Development](https://arxiv.org/abs/2510.19240)
*Behnam Agahi,Hamed Farbeh*

Main category: cs.SE

TL;DR: 本文提出了一种基于Yocto Project的三层架构，用于自动化构建和测试定制Linux系统，结合GitLab CI、Docker和本地缓存服务器，实现了可复现、可扩展的嵌入式操作系统开发流程，并通过QEMU验证了其稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着嵌入式系统在各行业的广泛应用，对自动化平台来开发和部署定制化Linux操作系统的需求日益增长。现有方法在可复现性、版本同步和构建效率方面存在不足，因此需要一种集成且高效的解决方案。

Method: 采用三层架构（Yocto主仓库、自定义meta层、协调manifest层），结合GitLab CI实现CI/CD流水线，利用Docker隔离环境，并通过本地hashserv缓存服务器加速构建；开发了三个示例项目并集成到构建流程中，在QEMU中执行六种启动测试验证系统功能。

Result: 构建时间显著减少，系统在六种QEMU启动场景下表现稳定，验证了架构的可复现性和可扩展性，并展示了其在实时Linux持续部署等高级应用中的潜力。

Conclusion: 所提出的架构为嵌入式Linux系统提供了一个稳定、可扩展且高效的开发与部署模型，未来可通过增强自动化测试、引入监控工具、分布式构建和多阶段Docker优化进一步提升其工业与科研应用价值。

Abstract: With the growing use of embedded systems in various industries, the need for
automated platforms for the development and deployment of customized
Linux-based operating systems has become more important. This research was
conducted with the aim of designing and implementing an integrated and
reproducible infrastructure for the development, building, and testing of a
Linux-based operating system using the Yocto Project. The proposed structure
was implemented based on a three-layer architecture consisting of the main
Yocto repositories, a custom layer (meta-custom), and a coordinating manifest
layer to ensure version synchronization, scalability, and reproducibility.
Three sample projects, including libhelloworld, helloworld, and the kernel
module hello mod, were developed and integrated into the build process.
Continuous Integration and Continuous Deployment pipelines were implemented
with GitLab CI and combined with an isolated Docker environment to automate and
streamline the build and testing workflows. Using a local cache server
containing hashserv, downloads and sstate cache significantly reduced the build
time. The functionality and stability of the system were verified through six
boot test scenarios in the QEMU simulator. The results show that the proposed
design not only ensures reproducibility but also can be extended to advanced
applications such as continuous deployment of real-time Linux versions. Future
recommendations include expanding automated tests, implementing system
monitoring with Prometheus and Grafana, using distributed builds, optimizing
with Docker multi-stage builds, and enabling continuous deployment of real-time
Linux changes to provide a stable and scalable model for industrial and
research projects in embedded systems with a rapid and reliable development
cycle.

</details>


### [6] [Trace: Securing Smart Contract Repository Against Access Control Vulnerability](https://arxiv.org/abs/2510.19254)
*Chong Chen,Jiachi Chen,Lingfeng Bao,David Lo,Yanlin Wang,Zhenyu Shan,Ting Chen,Guangqiang Yin,Jianxing Yu,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出TRACE工具，利用大语言模型（LLM）修复不可编译的智能合约代码片段，并通过构建函数调用图和控制流图检测访问控制漏洞，在多个数据集上显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有智能合约漏洞检测工具难以处理不可编译的复杂代码仓库，而开发者常复用含漏洞的第三方代码，导致严重安全风险，尤其在访问控制方面。

Method: TRACE使用大语言模型定位涉及关键操作（如转账）的敏感函数，将代码片段补全为可编译合约，基于AST构建函数调用图，并结合各函数的控制流图作为节点信息，分析敏感函数节点以检测访问控制漏洞。

Result: 在开源CVE数据集上检测出15个中的14个漏洞；在5000个链上合约中达到89.2%的精确率（优于现有最佳工具的76.9%）；在83个真实仓库中精确率达87.0%（远超DeepSeek-R1的14.3%）。

Conclusion: TRACE能有效检测不可编译智能合约仓库中的访问控制漏洞，显著提升检测精度，为智能合约安全开发提供实用工具。

Abstract: Smart contract vulnerabilities, particularly improper Access Control that
allows unauthorized execution of restricted functions, have caused billions of
dollars in losses. GitHub hosts numerous smart contract repositories containing
source code, documentation, and configuration files-these serve as intermediate
development artifacts that must be compiled and packaged before deployment.
Third-party developers often reference, reuse, or fork code from these
repositories during custom development. However, if the referenced code
contains vulnerabilities, it can introduce significant security risks. Existing
tools for detecting smart contract vulnerabilities are limited in their ability
to handle complex repositories, as they typically require the target contract
to be compilable to generate an abstract representation for further analysis.
This paper presents TRACE, a tool designed to secure non-compilable smart
contract repositories against access control vulnerabilities. TRACE employs
LLMs to locate sensitive functions involving critical operations (e.g.,
transfer) within the contract and subsequently completes function snippets into
a fully compilable contract. TRACE constructs a function call graph from the
abstract syntax tree (AST) of the completed contract. It uses the control flow
graph (CFG) of each function as node information. The nodes of the sensitive
functions are then analyzed to detect Access Control vulnerabilities.
Experimental results demonstrate that TRACE outperforms state-of-the-art tools
on an open-sourced CVE dataset, detecting 14 out of 15 CVEs. In addition, it
achieves 89.2% precision on 5,000 recent on-chain contracts, far exceeding the
best existing tool at 76.9%. On 83 real-world repositories, TRACE achieves
87.0% precision, significantly surpassing DeepSeek-R1's 14.3%.

</details>


### [7] [An Empirical Study of Bitwise Operators Intuitiveness through Performance Metrics](https://arxiv.org/abs/2510.19281)
*Shubham Joshi*

Main category: cs.SE

TL;DR: 该研究通过实验发现，尽管位运算符整体未显著延长任务完成时间，但部分运算符（如 OR、NOT 和左移）对响应时间和错误率有显著影响，表明其可理解性存在差异，值得进一步优化。


<details>
  <summary>Details</summary>
Motivation: 探究编程中位运算符的可读性与可理解性，检验不同位运算符是否会影响用户在任务中的表现（响应时间和错误率）。

Method: 采用被试内实验设计，招募23名具有不同编程背景的参与者（从无编程经验到博士生），要求其完成JavaScript中涉及位运算符的任务，并记录任务完成时间和准确率进行分析。

Result: 结果显示运算符类型对响应时间有微小但显著的影响（R² = 0.032, F(1, 494) = 16.5, p < .001），其中OR、NOT和左移运算符在任务完成时间上表现出统计显著性。

Conclusion: 尽管位运算符的复杂性未普遍导致任务时间延长，但某些运算符不够直观，建议进一步研究并可能重新设计以提升可理解性。

Abstract: Objectives: This study aims to investigate the readability and
understandability of bitwise operators in programming, with the main hypothesis
that there will be a difference in the performance metrics (response time and
error rate) between participants exposed to various bitwise operators related
questions and those who are not.
  Participants: Participants in this human research study include people
without programming background, novice programmers, and university students
with varying programming experience (from freshmen to PhD level). There were 23
participants for this study.
  Study Methods: This study uses an Within-Subjects Experimental Design to
assess how people with diverse programming backgrounds understand and use
bitwise operators. Participants complete tasks in JavaScript program, and their
task completion time and accuracy of the tasks are recorded for analysis.
  Findings: The results indicate that operators can be one of the factors
predicting response time, with a small but significant effect, with R-squared
0.032, (1, 494) = 16.5, p < .001. Additionally, some operators like OR, NOT,
and Left Shift showed statistical significance in task completion times
compared to other operators.
  Conclusions: While the complexity of bitwise operators did not generally
result in longer task completion times, certain operators were found to be less
intuitive, suggesting the need for further investigation and potential redesign
for improved understandability.

</details>


### [8] [Bytecode-centric Detection of Known-to-be-vulnerable Dependencies in Java Projects](https://arxiv.org/abs/2510.19393)
*Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Jonas Klauke,Eric Bodden*

Main category: cs.SE

TL;DR: Jaralyzer 是一种基于字节码的 Java 依赖漏洞扫描工具，能有效识别经过重编译、重打包等修改后的开源依赖中的已知漏洞，在准确性和覆盖范围上优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现代 Java 项目高度依赖开源软件（OSS），但这种依赖引入了已知安全漏洞的风险。现有依赖扫描工具难以有效检测经过修改（如重编译、重打包）的 OSS 依赖中的漏洞。

Method: 提出 Jaralyzer，一种不依赖元数据或源代码、直接分析依赖字节码的 Java 依赖扫描器。

Result: 在对 56 个流行 OSS 组件的评估中，Jaralyzer 能识别所有类型修改后的依赖漏洞，是唯一做到这一点的工具；即使在未修改依赖上，也比 Eclipse Steady 多检出 28 个真实漏洞并减少 29 个误报。

Conclusion: Jaralyzer 通过字节码分析显著提升了对修改后 OSS 依赖中漏洞的检测能力，优于当前主流依赖扫描工具。

Abstract: On average, 71% of the code in typical Java projects comes from open-source
software (OSS) dependencies, making OSS dependencies the dominant component of
modern software code bases. This high degree of OSS reliance comes with a
considerable security risk of adding known security vulnerabilities to a code
base. To remedy this risk, researchers and companies have developed various
dependency scanners, which try to identify inclusions of known-to-be-vulnerable
OSS dependencies. However, there are still challenges that modern dependency
scanners do not overcome, especially when it comes to dependency modifications,
such as re-compilations, re-bundlings or re-packagings, which are common in the
Java ecosystem. To overcome these challenges, we present Jaralyzer, a
bytecode-centric dependency scanner for Java. Jaralyzer does not rely on the
metadata or the source code of the included OSS dependencies being available
but directly analyzes a dependency's bytecode. Our evaluation across 56 popular
OSS components demonstrates that Jaralyzer outperforms other popular dependency
scanners in detecting vulnerabilities within modified dependencies. It is the
only scanner capable of identifying vulnerabilities across all the above
mentioned types of modifications. But even when applied to unmodified
dependencies, Jaralyzer outperforms the current state-of-the-art code-centric
scanner Eclipse Steady by detecting 28 more true vulnerabilities and yielding
29 fewer false warnings.

</details>


### [9] [AutoMT: A Multi-Agent LLM Framework for Automated Metamorphic Testing of Autonomous Driving Systems](https://arxiv.org/abs/2510.19438)
*Linfeng Liang,Chenkai Tan,Yao Deng,Yingfeng Cai,T. Y Chen,Xi Zheng*

Main category: cs.SE

TL;DR: AutoMT 是一个基于大语言模型的多智能体变形测试框架，能自动从本地交通规则中提取变形关系并生成有效的后续测试用例，显著提升测试多样性和行为违规检测率。


<details>
  <summary>Details</summary>
Motivation: 现有面向自动驾驶系统的变形测试方法严重依赖人工，缺乏自动化，难以高效发现安全关键场景中的故障。

Method: 利用大语言模型从Gherkin语法描述的交通规则中自动提取变形关系（MRs），结合视觉-语言智能体分析场景，并通过基于RAG的检索智能体从数据库中获取合适的MRs，再借助计算机视觉生成后续测试用例。

Result: 相比人工定义的最佳基线方法，AutoMT在后续测试用例生成中实现了高达5倍的测试多样性（以验证率衡量），并多检测出20.55%的行为违规。

Conclusion: AutoMT通过自动化提取多样化的变形关系，有效补充真实世界数据集，发现传统测试中易遗漏的边缘案例，其模块化架构便于集成到工业流程中，支持系统性覆盖安全关键场景。

Abstract: Autonomous Driving Systems (ADS) are safety-critical, where failures can be
severe. While Metamorphic Testing (MT) is effective for fault detection in ADS,
existing methods rely heavily on manual effort and lack automation. We present
AutoMT, a multi-agent MT framework powered by Large Language Models (LLMs) that
automates the extraction of Metamorphic Relations (MRs) from local traffic
rules and the generation of valid follow-up test cases. AutoMT leverages LLMs
to extract MRs from traffic rules in Gherkin syntax using a predefined
ontology. A vision-language agent analyzes scenarios, and a search agent
retrieves suitable MRs from a RAG-based database to generate follow-up cases
via computer vision. Experiments show that AutoMT achieves up to 5 x higher
test diversity in follow-up case generation compared to the best baseline
(manual expert-defined MRs) in terms of validation rate, and detects up to
20.55% more behavioral violations. While manual MT relies on a fixed set of
predefined rules, AutoMT automatically extracts diverse metamorphic relations
that augment real-world datasets and help uncover corner cases often missed
during in-field testing and data collection. Its modular architecture
separating MR extraction, filtering, and test generation supports integration
into industrial pipelines and potentially enables simulation-based testing to
systematically cover underrepresented or safety-critical scenarios.

</details>


### [10] [Mapping and Evolving Interoperability Testing in European Energy Systems: The int:net Perspective](https://arxiv.org/abs/2510.19460)
*Thomas I. Strasser,Edmund Widl,Carlos Ayon Mac Gregor,Mirko Ginocchi,Rene Kuchenbuch*

Main category: cs.SE

TL;DR: 本文通过调研30个欧洲测试设施，系统分析了欧洲互操作性测试能力现状，提出了分类清单和未来测试环境的蓝图，旨在推动建立协调统一的欧洲互操作性测试生态系统。


<details>
  <summary>Details</summary>
Motivation: 欧洲能源系统正经历由可再生能源、数字技术和去中心化系统驱动的转型，亟需实现各组件与系统间的高度互操作性，但目前缺乏对互操作性测试能力的系统性梳理与协调。

Method: 对欧洲30个互操作性测试设施开展结构化调研，整理测试基础设施、方法论和参考测试用例，并据此提出未来测试环境的构建蓝图。

Result: 形成了欧洲互操作性测试设施的分类清单，明确了现有测试能力、方法和用例，并提出了未来测试环境的设计框架。

Conclusion: 该研究为构建协调统一的欧洲互操作性测试生态系统奠定了基础，有助于推动能源转型中的协作、创新与标准对齐。

Abstract: The ongoing transformation of the European energy landscape, driven by the
integration of renewable energy sources, digital technologies, and
decentralized systems, requires a high degree of interoperability across
diverse components and systems. Ensuring that these elements can exchange
information and operate together reliably is essential for achieving a secure,
flexible, and efficient energy supply infrastructure. While several initiatives
have contributed to the development of smart grid testing infrastructures, they
do not provide a dedicated or comprehensive focus on interoperability testing.
A structured and harmonized overview of interoperability testing capabilities
across Europe is therefore still missing. This work therefore presents a novel
contribution by analyzing the European interoperability testing facility
landscape through a structured survey of 30 facilities. It provides a
categorized inventory of testing infrastructures, applied methodologies, and
reference test cases, and introduces a blueprint for the development of future
testing environments. The findings contribute to the establishment of a
coordinated European ecosystem for interoperability testing, supporting
collaboration, innovation, and alignment with the goals of the energy
transition.

</details>


### [11] [A Goal-Driven Survey on Root Cause Analysis](https://arxiv.org/abs/2510.19593)
*Aoyang Fang,Haowen Yang,Haoze Dong,Qisheng Lu,Junjielong Xu,Pinjia He*

Main category: cs.SE

TL;DR: 本文提出了一种以目标为导向的框架，对2014至2025年间135篇云服务根因分析（RCA）相关论文按其不同目标进行分类整合，弥补了以往综述仅按输入数据类型分类而忽视任务目标差异的不足，并探讨了RCA的终极目标、开放挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有RCA综述多按输入数据类型（如指标、追踪数据）分类方法，忽略了不同研究在目标上的本质差异（如快速定位故障服务 vs. 精确定位功能缺陷），导致研究进展和空白被掩盖；而RCA综述的读者（初学者或研究人员）更关注任务目标和整体图景，因此亟需一种基于目标的分类框架。

Method: 提出一个目标驱动的分类框架，依据RCA研究的多样化目标对135篇云服务RCA论文进行系统性归类与整合，并在此基础上提炼RCA的统一终极目标。

Result: 成功将135篇RCA论文按目标分类，揭示了不同目标导向下的研究脉络，并明确了RCA领域的开放挑战与未来方向。

Conclusion: 以目标为导向的RCA分类框架能更清晰地展现研究进展与空白，有助于推动云服务故障管理中RCA方法的发展，并为后续研究提供指导。

Abstract: Root Cause Analysis (RCA) is a crucial aspect of incident management in
large-scale cloud services. While the term root cause analysis or RCA has been
widely used, different studies formulate the task differently. This is because
the term "RCA" implicitly covers tasks with distinct underlying goals. For
instance, the goal of localizing a faulty service for rapid triage is
fundamentally different from identifying a specific functional bug for a
definitive fix. However, previous surveys have largely overlooked these
goal-based distinctions, conventionally categorizing papers by input data types
(e.g., metric-based vs. trace-based methods). This leads to the grouping of
works with disparate objectives, thereby obscuring the true progress and gaps
in the field. Meanwhile, the typical audience of an RCA survey is either laymen
who want to know the goals and big picture of the task or RCA researchers who
want to figure out past research under the same task formulation. Thus, an RCA
survey that organizes the related papers according to their goals is in high
demand. To this end, this paper presents a goal-driven framework that
effectively categorizes and integrates 135 papers on RCA in the context of
cloud incident management based on their diverse goals, spanning the period
from 2014 to 2025. In addition to the goal-driven categorization, it discusses
the ultimate goal of all RCA papers as an umbrella covering different RCA
formulations. Moreover, the paper discusses open challenges and future
directions in RCA.

</details>


### [12] [Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1](https://arxiv.org/abs/2510.19600)
*Qianli Ma,Siyu Wang,Yilin Chen,Yinhao Tang,Yixiang Yang,Chang Guo,Bingjie Gao,Zhening Xing,Yanan Sun,Zhipeng Zhang*

Main category: cs.SE

TL;DR: AutoPage 是一个基于多智能体协作的系统，能高效、低成本地将科研论文自动转化为高质量、交互式的项目网页，并通过新构建的 PageBench 基准验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 科研人员常需耗费大量时间手动创建项目网页以传播研究成果，而现有自动化工具难以应对网页的动态与交互特性，因此亟需一种能兼顾效率、准确性与作者意图的新方法。

Method: 提出 AutoPage 多智能体系统，采用从粗到细的流水线架构，涵盖叙事规划、多模态内容生成与交互式渲染；引入“Checker”智能体防止幻觉，并支持人类介入校验，确保输出与论文一致且符合作者预期。

Result: 实验表明 AutoPage 能在 15 分钟内以低于 0.1 美元的成本生成高质量、视觉美观且交互性强的项目网页，并发布了首个相关基准 PageBench。

Conclusion: AutoPage 通过多智能体协作与人机协同机制，有效解决了科研论文到项目网页自动化生成的难题，为科研传播提供了高效、可靠的新工具。

Abstract: In the quest for scientific progress, communicating research is as vital as
the discovery itself. Yet, researchers are often sidetracked by the manual,
repetitive chore of building project webpages to make their dense papers
accessible. While automation has tackled static slides and posters, the
dynamic, interactive nature of webpages has remained an unaddressed challenge.
To bridge this gap, we reframe the problem, arguing that the solution lies not
in a single command, but in a collaborative, hierarchical process. We introduce
$\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy.
AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline
from narrative planning to multimodal content generation and interactive
rendering. To combat AI hallucination, dedicated "Checker" agents verify each
step against the source paper, while optional human checkpoints ensure the
final product aligns perfectly with the author's vision, transforming the
system from a mere tool into a powerful collaborative assistant. To rigorously
validate our approach, we also construct $\textbf{PageBench}$, the first
benchmark for this new task. Experiments show AutoPage not only generates
high-quality, visually appealing pages but does so with remarkable efficiency
in under 15 minutes for less than \$0.1. Code and dataset will be released at
$\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.

</details>


### [13] [FidelityGPT: Correcting Decompilation Distortions with Retrieval Augmented Generation](https://arxiv.org/abs/2510.19615)
*Zhiping Zhou,Xiaohong Li,Ruitao Feng,Yao Zhang,Yuekang Li,Wenbu Feng,Yunqian Wang,Yuqing Li*

Main category: cs.SE

TL;DR: FidelityGPT 是一个提升反编译代码准确性和可读性的新框架，通过检测并修正语义失真，在闭源二进制文件上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有反编译方法在处理复杂闭源二进制文件时，难以有效检测和纠正语义失真，导致输出代码可读性和语义准确性不足。

Method: FidelityGPT 引入面向失真的提示模板，结合检索增强生成（RAG）与动态语义强度算法定位失真代码行，并通过变量依赖算法缓解长上下文限制，将冗余变量依赖整合进提示上下文。

Result: 在包含620个函数对的二进制相似性基准上，FidelityGPT 实现了89%的平均检测准确率和83%的精确率；相比 DeGPT（修复率83%，正确修复率37%），其修复率达94%，正确修复率达64%。

Conclusion: FidelityGPT 显著提升了基于大语言模型的反编译质量，在准确性和可读性方面取得重要进展，有望推动逆向工程的发展。

Abstract: Decompilation converts machine code into human-readable form, enabling
analysis and debugging without source code. However, fidelity issues often
degrade the readability and semantic accuracy of decompiled output. Existing
methods, such as variable renaming or structural simplification, provide
partial improvements but lack robust detection and correction, particularly for
complex closed-source binaries. We present FidelityGPT, a framework that
enhances decompiled code accuracy and readability by systematically detecting
and correcting semantic distortions. FidelityGPT introduces distortion-aware
prompt templates tailored to closed-source settings and integrates
Retrieval-Augmented Generation (RAG) with a dynamic semantic intensity
algorithm to locate distorted lines and retrieve semantically similar code from
a database. A variable dependency algorithm further mitigates long-context
limitations by analyzing redundant variables and integrating their dependencies
into the prompt context. Evaluated on 620 function pairs from a binary
similarity benchmark, FidelityGPT achieved an average detection accuracy of 89%
and a precision of 83%. Compared to the state-of-the-art DeGPT (Fix Rate 83%,
Corrected Fix Rate 37%), FidelityGPT attained 94% FR and 64% CFR, demonstrating
significant gains in accuracy and readability. These results highlight its
potential to advance LLM-based decompilation and reverse engineering.

</details>


### [14] [BOSQTGEN: Breaking the Sound Barrier in Test Generation](https://arxiv.org/abs/2510.19777)
*S M Sadrul Islam Asif,James Chen,Earl T. Barr,Mark Marron*

Main category: cs.SE

TL;DR: BOSQTGEN 是一种新型黑盒 API 测试生成方法，通过将 API 规约分解为基本单元、利用大语言模型建议结构化分层，并结合组合测试高效采样，在 RESTful 基准上平均达到 82% 的代码覆盖率，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现代软件依赖 API 组合构建，但不完善的 API 合约易导致预期不一致和系统故障，亟需有效的黑盒一致性测试方法。现有测试生成技术面临多语言系统、源码不可用、成本与可靠性权衡，尤其是结构化输入生成困难等挑战。

Method: BOSQTGEN 将 API 规范分解为基本原语，利用大语言模型（LLM）建议合理的结构分层，并采用组合测试策略在这些结构上高效采样，以覆盖关键交互并避免随机采样的冗余。

Result: 在 RESTful 基准测试中，BOSQTGEN 平均实现 82% 的代码覆盖率，相比当前最先进的系统平均提升 20% 以上，接近手工编写测试套件的水平。

Conclusion: BOSQTGEN 提供了一种完全基于 API 的自动化测试生成方案，能有效支持开发者进行高质量测试用例生成，适用于验证和测试驱动开发场景。

Abstract: Modern software is increasingly built by composing APIs, elevating the API
contract to a critical role. Inadequate contracts, however, lead to mismatched
expectations and failures, creating a pressing need for robust conformance
testing. Current test generation techniques are hindered by key challenges:
polyglot systems, source code inaccessibility, a cost-reliability trade-off,
and, most critically, the difficulty of generating structured inputs.
  We introduce BOSQTGEN, a novel black-box methodology and tool for API test
generation. BOSQTGEN utilizes a novel approach for decomposing API
specifications into primitives, using LLMs to suggest coherent strata for them,
and employing combinatorial testing to efficiently sample over these values.
This approach ensures coverage of critical interactions while avoiding the
redundancy of random sampling.
  The resulting BOSQTGEN system achieves an average of 82% code coverage on
RESTful benchmarks, often a 20% or more increase over prior state-of-the-art
systems and nearing parity with hand-written test suites. Providing a fully
API-driven approach to test generation, enables developers to automatically
create high-quality test cases for validation or test-driven development.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [15] [CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation](https://arxiv.org/abs/2510.18893)
*Sergey Pugachev*

Main category: cs.DC

TL;DR: CodeCRDT 是一种基于观察驱动的协调模式，利用 CRDT 实现多智能体 LLM 系统在代码生成任务中的无锁、无冲突并发协作，在部分任务中实现最高 21.1% 的加速，但也可能在其他任务中导致最多 39.4% 的减速，同时保证 100% 收敛且无合并失败。


<details>
  <summary>Details</summary>
Motivation: 多智能体大语言模型（LLM）系统因高昂的协调开销而无法实现并行加速，亟需一种更高效的协调机制。

Method: 提出 CodeCRDT，一种基于可观察共享状态和确定性收敛的观察驱动协调模式，利用冲突-free 复制数据类型（CRDT）实现无锁、无冲突的并发代码生成。

Result: 在 600 次实验中，CodeCRDT 在某些任务上实现最高 21.1% 的速度提升，但在其他任务上最多减慢 39.4%；所有实验均实现 100% 收敛且无合并失败，并揭示了语义冲突率（5–10%）及质量与性能之间的权衡。

Conclusion: 观察驱动的协调机制在特定任务结构下能有效提升多智能体 LLM 系统的并行效率，但其效果高度依赖任务特性，需根据任务结构权衡使用。

Abstract: Multi-agent LLM systems fail to realize parallel speedups due to costly
coordination. We present CodeCRDT, an observation-driven coordination pattern
where agents coordinate by monitoring a shared state with observable updates
and deterministic convergence, rather than explicit message passing. Using
Conflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,
conflict-free concurrent code generation with strong eventual consistency.
Evaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits
and trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on
others, and 100% convergence with zero merge failures. The study formalizes
observation-driven coordination for stochastic LLM agents, revealing semantic
conflict rates (5-10%) and quality-performance tradeoffs, and provides
empirical characterization of when parallel coordination succeeds versus fails
based on task structure.

</details>


### [16] [AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators](https://arxiv.org/abs/2510.18897)
*Jacopo Tagliabue*

Main category: cs.DC

TL;DR: 本文提出一种结合大语言模型（LLM）与领域专用模拟器的迭代方法，用于设计可解释且高效的分布式系统调度策略。


<details>
  <summary>Details</summary>
Motivation: 传统分布式系统策略设计难以兼顾可解释性与在大规模设计空间中的高效搜索，作者希望借助AI提升策略生成效率，同时保留人工可理解性。

Method: 采用“生成-验证”循环：LLM生成Python调度策略，利用开源模拟器Eudoxia在标准轨迹上评估策略性能，并将结构化反馈用于引导后续生成。

Result: 在多个模型上实现了吞吐量的初步提升，验证了该方法在Function-as-a-Service运行时Bauplan中的有效性。

Conclusion: 该方法在保持策略可解释性的同时有效探索了设计空间，未来AI有望通过辅助构建新模拟器进一步扩展此范式。

Abstract: We explore AI-driven distributed-systems policy design by combining
stochastic code generation from large language models (LLMs) with deterministic
verification in a domain-specific simulator. Using a Function-as-a-Service
runtime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we
frame scheduler design as an iterative generate-and-verify loop: an LLM
proposes a Python policy, the simulator evaluates it on standardized traces,
and structured feedback steers subsequent generations. This setup preserves
interpretability while enabling targeted search over a large design space. We
detail the system architecture and report preliminary results on throughput
improvements across multiple models. Beyond early gains, we discuss the limits
of the current setup and outline next steps; in particular, we conjecture that
AI will be crucial for scaling this methodology by helping to bootstrap new
simulators.

</details>


### [17] [RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training](https://arxiv.org/abs/2510.19262)
*Heng Xu,Zhiwei Yu,Chengze Du,Ying Zhou,Letian Li,Haojie Wang,Weiqiang Cheng,Jialong Li*

Main category: cs.DC

TL;DR: RailS 是一种针对 Rail 架构优化的 MoE 训练负载均衡框架，通过利用拓扑对称性和本地调度策略，显著提升带宽利用率并降低通信完成时间。


<details>
  <summary>Details</summary>
Motivation: MoE 模型训练中的稀疏且高度不平衡的 all-to-all 通信成为迭代时间瓶颈，而传统负载均衡方法未能有效利用 Rail 架构的确定性拓扑结构，导致多 NIC 带宽未被充分利用。

Method: RailS 利用 Rail 拓扑的对称性，证明均匀发送可确保均匀接收，从而将全局协调转化为本地调度；每个节点独立运行 LPT（Longest Processing Time First）喷洒调度器，基于本地信息主动平衡流量，并激活 N 条并行通道实现细粒度、拓扑感知的多路径传输。

Result: 在合成和真实 MoE 工作负载上，RailS 将总线带宽提升 20%–78%，通信完成时间减少 17%–78%；在 Mixtral 工作负载上，迭代时间缩短 18%–40%，实现接近最优的负载均衡。

Conclusion: RailS 有效挖掘了 Rail 架构的并行潜力，在分布式 MoE 训练中显著提升通信效率和负载均衡性能。

Abstract: Training Mixture-of-Experts (MoE) models introduces sparse and highly
imbalanced all-to-all communication that dominates iteration time. Conventional
load-balancing methods fail to exploit the deterministic topology of Rail
architectures, leaving multi-NIC bandwidth underutilized. We present RailS, a
distributed load-balancing framework that minimizes all-to-all completion time
in MoE training. RailS leverages the Rail topology's symmetry to prove that
uniform sending ensures uniform receiving, transforming global coordination
into local scheduling. Each node independently executes a Longest Processing
Time First (LPT) spraying scheduler to proactively balance traffic using local
information. RailS activates N parallel rails for fine-grained, topology-aware
multipath transmission. Across synthetic and real-world MoE workloads, RailS
improves bus bandwidth by 20%--78% and reduces completion time by 17%--78%. For
Mixtral workloads, it shortens iteration time by 18%--40% and achieves
near-optimal load balance, fully exploiting architectural parallelism in
distributed training.

</details>


### [18] [Comparative analysis of large data processing in Apache Spark using Java, Python and Scala](https://arxiv.org/abs/2510.19012)
*Ivan Borodii,Illia Fedorovych,Halyna Osukhivska,Diana Velychko,Roman Butsii*

Main category: cs.DC

TL;DR: 本文对比了在 Apache Spark 平台上使用 Java、Python 和 Scala 处理大规模数据集的性能差异，发现编程语言对 ETL 工作流效率有显著影响：Python 在小数据量下更快，而 Scala 和 Java 在大数据量和复杂操作中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注 Apache Spark 中 ETL 流程的个别阶段，缺乏对使用不同编程语言（Java、Python、Scala）在 Apache Iceberg 上执行完整 ETL 工作流的系统性性能比较。

Method: 通过在 Apache Spark 上使用三种语言分别执行从 CSV 文件读取、转换并加载至 Apache Iceberg 表的操作，对不同数据规模（5MB、1.6GB）及复杂度（单文件 vs. 合并双文件）下的运行时间进行对比分析。

Result: 处理 5MB 文件时，Python 最快（6.71 秒），优于 Scala（9.13 秒）和 Java（9.62 秒）；处理 1.6GB 文件时，三者性能接近，Python 仍略快（46.34 秒 vs. Scala 47.72 秒、Java 50.56 秒）；但在合并两个 CSV 文件的复杂任务中，Scala 表现最佳（374.42 秒），Java 次之（379.8 秒），Python 最慢（398.32 秒）。

Conclusion: 编程语言显著影响 Apache Spark 数据处理效率：Python 适合小数据量任务，而 Scala 和 Java 更适合大数据量和复杂操作，实际应用中应根据数据规模和性能需求选择合适语言。

Abstract: During the study, the results of a comparative analysis of the process of
handling large datasets using the Apache Spark platform in Java, Python, and
Scala programming languages were obtained. Although prior works have focused on
individual stages, comprehensive comparisons of full ETL workflows across
programming languages using Apache Iceberg remain limited. The analysis was
performed by executing several operations, including downloading data from CSV
files, transforming and loading it into an Apache Iceberg analytical table. It
was found that the performance of the Spark algorithm varies significantly
depending on the amount of data and the programming language used. When
processing a 5-megabyte CSV file, the best result was achieved in Python: 6.71
seconds, which is superior to Scala's score of 9.13 seconds and Java's time of
9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming
languages demonstrated similar results: the fastest performance was showed in
Python: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56
seconds, respectively. When performing a more complex operation that involved
combining two CSV files into a single dataset for further loading into an
Apache Iceberg table, Scala demonstrated the highest performance, at 374.42
seconds. Java processing was completed in 379.8 seconds, while Python was the
least efficient, with a runtime of 398.32 seconds. It follows that the
programming language significantly affects the efficiency of data processing by
the Apache Spark algorithm, with Scala and Java being more productive for
processing large amounts of data and complex operations, while Python
demonstrates an advantage in working with small amounts of data. The results
obtained can be useful for optimizing data handling processes depending on
specific performance requirements and the amount of information being
processed.

</details>


### [19] [On the Randomized Locality of Matching Problems in Regular Graphs](https://arxiv.org/abs/2510.19151)
*Seri Khoury,Manish Purohit,Aaron Schild,Joshua Wang*

Main category: cs.DC

TL;DR: 本文研究正则图中匹配问题的局部性，证明了$(1+\epsilon)$-近似匹配具有真正局部的性质（仅依赖于$\epsilon$），并给出了匹配的上下界；同时揭示了最大匹配在节点平均复杂度与最坏情况复杂度之间的显著差异。


<details>
  <summary>Details</summary>
Motivation: 理解分布式对称性破缺问题中的局部性，特别是在正则图这一关键基准图类中匹配问题的局部性行为。

Method: 提出随机算法分析$(1+\epsilon)$-近似匹配的局部性，并利用基于鞅的新颖分析方法重新审视Luby算法在正则图线图上的效果。

Result: 证明了$(1+\epsilon)$-近似匹配的局部性仅依赖于$\epsilon$（当度$\Delta \geq \text{poly}(1/\epsilon)$时为对数依赖），并展示了最大匹配的节点平均复杂度为$O(1)$，同时给出了匹配的上下界。

Conclusion: 近似匹配在正则图中具有强局部性，而最大匹配则不然；通过改进Luby算法的分析，揭示了匹配问题在不同复杂度模型下的本质差异。

Abstract: The main goal in distributed symmetry-breaking is to understand the locality
of problems; i.e., the radius of the neighborhood that a node needs to explore
in order to arrive at its part of a global solution. In this work, we study the
locality of matching problems in the family of regular graphs, which is one of
the main benchmarks for establishing lower bounds on the locality of
symmetry-breaking problems, as well as for obtaining classification results.
For approximate matching, we develop randomized algorithms to show that $(1 +
\epsilon)$-approximate matching in regular graphs is truly local; i.e., the
locality depends only on $\epsilon$ and is independent of all other graph
parameters. Furthermore, as long as the degree $\Delta$ is not very small
(namely, as long as $\Delta \geq \text{poly}(1/\epsilon)$), this dependence is
only logarithmic in $1/\epsilon$. This stands in sharp contrast to maximal
matching in regular graphs which requires some dependence on the number of
nodes $n$ or the degree $\Delta$. We show matching lower bounds for both
results. For maximal matching, our techniques further allow us to establish a
strong separation between the node-averaged complexity and worst-case
complexity of maximal matching in regular graphs, by showing that the former is
only $O(1)$. Central to our main technical contribution is a novel
martingale-based analysis for the $\approx 40$-year-old algorithm by Luby. In
particular, our analysis shows that applying one round of Luby's algorithm on
the line graph of a $\Delta$-regular graph results in an almost
$\Delta/2$-regular graph.

</details>


### [20] [RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs](https://arxiv.org/abs/2510.19225)
*Yongji Wu,Xueshen Liu,Haizhong Zheng,Juncheng Gu,Beidi Chen,Z. Morley Mao,Arvind Krishnamurthy,Ion Stoica*

Main category: cs.DC

TL;DR: RLBoost 是一种利用可抢占 GPU 资源（如云上的 Spot 实例）提升强化学习训练效率的系统方案，通过混合架构和三项关键技术，在降低成本的同时显著提升训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习框架在资源利用上存在不足：共置架构无法缓解 rollout 与训练阶段对资源需求的冲突，而解耦架构在不修改算法的前提下难以高效利用资源；同时，可抢占 GPU 资源提供了显著的成本节约潜力，但缺乏有效利用机制。

Method: RLBoost 采用混合架构，结合三项关键技术：(1) 自适应 rollout 卸载，动态调整预留集群上的负载；(2) 基于拉取的权重传输，快速配置新可用实例；(3) 令牌级响应收集与迁移，高效处理抢占并实现持续负载均衡。

Result: 实验表明，RLBoost 相比仅使用按需 GPU 资源，训练吞吐量提升 1.51–1.97 倍，成本效率提升 28%–49%。

Conclusion: RLBoost 有效利用可抢占 GPU 资源，显著提升强化学习训练的吞吐量与成本效率，为大规模语言模型的高效训练提供了实用解决方案。

Abstract: Reinforcement learning (RL) has become essential for unlocking advanced
reasoning capabilities in large language models (LLMs). RL workflows involve
interleaving rollout and training stages with fundamentally different resource
requirements. Rollout typically dominates overall execution time, yet scales
efficiently through multiple independent instances. In contrast, training
requires tightly-coupled GPUs with full-mesh communication. Existing RL
frameworks fall into two categories: co-located and disaggregated
architectures. Co-located ones fail to address this resource tension by forcing
both stages to share the same GPUs. Disaggregated architectures, without
modifications of well-established RL algorithms, suffer from resource
under-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances
on public clouds and spare capacity in production clusters, present significant
cost-saving opportunities for accelerating RL workflows, if efficiently
harvested for rollout.
  In this paper, we present RLBoost, a systematic solution for cost-efficient
RL training that harvests preemptible GPU resources. Our key insight is that
rollout's stateless and embarrassingly parallel nature aligns perfectly with
preemptible and often fragmented resources. To efficiently utilize these
resources despite frequent and unpredictable availability changes, RLBoost
adopts a hybrid architecture with three key techniques: (1) adaptive rollout
offload to dynamically adjust workloads on the reserved (on-demand) cluster,
(2) pull-based weight transfer that quickly provisions newly available
instances, and (3) token-level response collection and migration for efficient
preemption handling and continuous load balancing. Extensive experiments show
RLBoost increases training throughput by 1.51x-1.97x while improving cost
efficiency by 28%-49% compared to using only on-demand GPU resources.

</details>


### [21] [FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems](https://arxiv.org/abs/2510.19301)
*Ziheng Deng,Xue Liu,Jiantong Jiang,Yankai Li,Qingxu Deng,Xiaochun Yang*

Main category: cs.DC

TL;DR: 本文提出了FLASH Viterbi和FLASH-BS Viterbi两种新型维特比解码算法，通过非递归分治、剪枝与并行化技术，在保证解码精度的同时显著提升时间和内存效率，并针对边缘设备设计了FPGA硬件加速器，展现出良好的适应性与硬件友好性。


<details>
  <summary>Details</summary>
Motivation: 标准维特比算法在资源受限的边缘平台上存在内存占用高、计算不灵活的问题，现有方法在时间与空间之间权衡时往往带来显著运行开销且缺乏对不同系统约束的适应能力。

Method: 结合非递归分治策略、剪枝与并行化技术设计FLASH Viterbi算法；进一步提出基于高效内存结构的动态束搜索变体FLASH-BS Viterbi；并为两者开发FPGA硬件加速器。

Result: 实验表明，所提算法在解码时间和内存效率上均优于现有基线方法，同时保持良好的适应性和硬件友好特性。

Conclusion: FLASH Viterbi系列算法有效解决了传统维特比解码在边缘计算场景下的资源效率与适应性问题，为现代数据系统中的结构化序列推理提供了高效可行的解决方案。

Abstract: The Viterbi algorithm is a key operator for structured sequence inference in
modern data systems, with applications in trajectory analysis, online
recommendation, and speech recognition. As these workloads increasingly migrate
to resource-constrained edge platforms, standard Viterbi decoding remains
memory-intensive and computationally inflexible. Existing methods typically
trade decoding time for space efficiency, but often incur significant runtime
overhead and lack adaptability to various system constraints. This paper
presents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly
Viterbi decoding operator that enhances adaptability and resource efficiency.
FLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning
and parallelization techniques to enhance both time and memory efficiency,
making it well-suited for resource-constrained data systems.To further decouple
space complexity from the hidden state space size, we present FLASH-BS Viterbi,
a dynamic beam search variant built on a memory-efficient data structure. Both
proposed algorithms exhibit strong adaptivity to diverse deployment scenarios
by dynamically tuning internal parameters.To ensure practical deployment on
edge devices, we also develop FPGA-based hardware accelerators for both
algorithms, demonstrating high throughput and low resource usage. Extensive
experiments show that our algorithms consistently outperform existing baselines
in both decoding time and memory efficiency, while preserving adaptability and
hardware-friendly characteristics essential for modern data systems. All codes
are publicly available at https://github.com/Dzh-16/FLASH-Viterbi.

</details>


### [22] [HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission](https://arxiv.org/abs/2510.19470)
*Weihao Yang,Hao Huang,Donglei Wu,Ningke Li,Yanqi Pan,Qiyang Zheng,Wen Xia,Shiyi Li,Qiang Wang*

Main category: cs.DC

TL;DR: 本文提出HybridEP，一种面向带宽受限场景的混合专家并行（EP）优化框架，通过动态调整专家的空间布局，结合基于域的划分和参数高效迁移技术，显著降低跨数据中心（DC）训练中的通信开销，在低带宽条件下相比现有方法最高提速5.6倍。


<details>
  <summary>Details</summary>
Motivation: 随着混合专家（MoE）模型规模迅速增长，单数据中心训练已无法满足需求，转向跨数据中心训练成为趋势。然而，现有专家并行（EP）方法在跨DC低带宽环境下通信开销大、可扩展性差，成为MoE模型持续扩展的关键瓶颈。

Method: 提出HybridEP框架，通过建模指导动态调整专家的空间布局以减少通信流量与频率；具体包括：（1）基于域的划分，构建混合通信模式与GPU级拓扑的映射；（2）参数高效迁移，进一步优化拓扑结构、减少专家传输开销并扩大域规模。

Result: 实验表明，在带宽受限条件下，HybridEP相比当前最先进的MoE训练系统最高提速5.6倍；在大规模模拟中，使用1000个数据中心时，HybridEP在不同带宽下最高获得1.45倍加速。

Conclusion: HybridEP是一种更具通用性和可扩展性的专家并行方案，有效缓解了跨数据中心训练中因带宽限制导致的通信瓶颈，为大规模MoE模型的高效训练提供了新思路。

Abstract: Mixture-of-Experts (MoE) has become a popular architecture for scaling large
models. However, the rapidly growing scale outpaces model training on a single
DC, driving a shift toward a more flexible, cross-DC training paradigm. Under
this, Expert Parallelism (EP) of MoE faces significant scalability issues due
to the limited cross-DC bandwidth. Specifically, existing EP optimizations
attempt to overlap data communication and computation, which has little benefit
in low-bandwidth scenarios due to a much longer data communication time.
Therefore, the trends of cross-DC EP scaling is fast becoming a critical
roadblock to the continued growth of MoE models.
  To address this, we propose HybridEP, a modeling-guided framework to optimize
EP under constrained bandwidth. Our key idea is to dynamically transform the
spatial placement of experts to reduce data communication traffic and
frequency, thereby minimizing EP's communication overheads. However, it is
non-trivial to find the optimal solution because it complicates the original
communication pattern by mixing data and expert communication. We therefore
build a stream-based model to determine the optimal transmission ratio. Guided
by this, we incorporate two techniques: (1) domain-based partition to construct
the mapping between hybrid patterns and specific communication topology at GPU
level, and (2) parameter-efficient migration to further refine this topology by
reducing expert transmission overhead and enlarging the domain size. Combining
all these designs, HybridEP can be considered as a more general EP with better
scalability. Experimental results show that HybridEP outperforms existing
state-of-the-art MoE training systems by up to 5.6x under constrained
bandwidth. We further compare HybridEP and EP on large-scale simulations.
HybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths.

</details>


### [23] [Propius: A Platform for Collaborative Machine Learning across the Edge and the Cloud](https://arxiv.org/abs/2510.19617)
*Eric Ding*

Main category: cs.DC

TL;DR: 本文提出了Propius系统，一种面向协作机器学习的可扩展、高效且支持多租户的资源管理系统，通过控制平面和数据平面分别优化资源调度与模型共享，显著提升了资源利用率、吞吐量和任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 现有协作机器学习研究多集中于算法和硬件优化，缺乏通用、可扩展且可复用的基础设施，难以应对日益增长的多租户资源管理需求。

Method: 设计并实现Propius系统，包含控制平面（支持多任务间高效资源共享与策略配置）和数据平面（提升模型分发与结果聚合的可扩展性），以适应客户端异构性并优化计算流调度。

Result: 实验表明，Propius在资源利用率（最高提升1.88倍）、吞吐量（最高2.76倍）和任务完成时间（最高加速1.26倍）方面优于现有资源管理方法和框架。

Conclusion: Propius为协作机器学习提供了一种高效、可扩展且支持多租户的基础设施解决方案，有效弥补了当前系统层面研究的不足。

Abstract: Collaborative Machine Learning is a paradigm in the field of distributed
machine learning, designed to address the challenges of data privacy,
communication overhead, and model heterogeneity. There have been significant
advancements in optimization and communication algorithm design and ML hardware
that enables fair, efficient and secure collaborative ML training. However,
less emphasis is put on collaborative ML infrastructure development. Developers
and researchers often build server-client systems for a specific collaborative
ML use case, which is not scalable and reusable. As the scale of collaborative
ML grows, the need for a scalable, efficient, and ideally multi-tenant resource
management system becomes more pressing. We propose a novel system, Propius,
that can adapt to the heterogeneity of client machines, and efficiently manage
and control the computation flow between ML jobs and edge resources in a
scalable fashion. Propius is comprised of a control plane and a data plane. The
control plane enables efficient resource sharing among multiple collaborative
ML jobs and supports various resource sharing policies, while the data plane
improves the scalability of collaborative ML model sharing and result
collection. Evaluations show that Propius outperforms existing resource
management techniques and frameworks in terms of resource utilization (up to
$1.88\times$), throughput (up to $2.76$), and job completion time (up to
$1.26\times$).

</details>


### [24] [Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and Beyond](https://arxiv.org/abs/2510.19805)
*Carl-Johan Fauvelle Munck af Rosensch"old,Feras M. Awaysheh,Ahmad Awad*

Main category: cs.DC

TL;DR: 本文对Redis的替代方案Valkey、KeyDB和Garnet在Kubernetes环境下的性能与可行性进行了系统性基准测试，评估了吞吐量、尾延迟、资源效率及迁移复杂度等指标，揭示了性能、兼容性与长期可持续性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对现代内存键值存储系统（如Redis替代品）在真实工作负载下的系统性实验评估，而这些系统在云原生基础设施中日益关键，面临可扩展性、兼容性和可持续性等挑战。

Method: 在Kubernetes部署环境中，对Valkey、KeyDB和Garnet三种Redis替代方案进行基准测试，采用真实工作负载，评估指标包括吞吐量、尾延迟、CPU与内存效率以及迁移复杂度，并综合考察项目成熟度、社区支持和持续开发情况。

Result: 实验结果揭示了各内存键值存储系统在性能、兼容性和长期可行性之间的明显权衡；不同系统在吞吐量、延迟、资源使用效率和迁移难度等方面表现各异。

Conclusion: 该研究为云原生环境下选择和部署内存键值存储系统提供了实证依据，强调需根据具体应用场景在性能、兼容性和项目可持续性之间做出权衡。

Abstract: In-memory key-value datastores have become indispensable building blocks of
modern cloud-native infrastructures, yet their evolution faces scalability,
compatibility, and sustainability constraints. The current literature lacks an
experimental evaluation of state-of-the-art tools in the domain. This study
addressed this timely gap by benchmarking Redis alternatives and systematically
evaluating Valkey, KeyDB, and Garnet under realistic workloads within
Kubernetes deployments. The results demonstrate clear trade-offs among the
benchmarked data systems. Our study presents a comprehensive performance and
viability assessment of the emerging in-memory key-value stores. Metrics
include throughput, tail latency, CPU and memory efficiency, and migration
complexity. We highlight trade-offs between performance, compatibility, and
long-term viability, including project maturity, community support, and
sustained development.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [25] [gem5 Co-Pilot: AI Assistant Agent for Architectural Design Space Exploration](https://arxiv.org/abs/2510.19577)
*Zuoming Fu,Alex Manley,Mohammad Alian*

Main category: cs.AR

TL;DR: 本文提出 gem5 Co-Pilot，一个基于大语言模型（LLM）的智能助手，用于自动化 gem5 的设计空间探索，通过网页界面、专用探索语言和设计空间数据库（DSDB），实现高效、低成本的架构优化。


<details>
  <summary>Details</summary>
Motivation: 计算机体系结构的设计空间探索过程复杂且耗时，需分析大量参数与仿真数据；大语言模型在长文本分析与智能决策方面的优势为提升该过程效率提供了新机遇。

Method: 构建 gem5 Co-Pilot 智能体，集成网页 GUI、设计空间探索语言及设计空间数据库（DSDB），形成基于检索增强生成（RAG）的自动化探索系统。

Result: 在四种成本约束下进行实验，gem5 Co-Pilot 能在少量用户交互下快速识别满足性能与成本要求的最优参数，优于两个基线模型。

Conclusion: gem5 Co-Pilot 有效利用 LLM 能力，显著提升 gem5 设计空间探索的自动化水平与效率，为体系结构研究提供实用工具。

Abstract: Generative AI is increasing the productivity of software and hardware
development across many application domains. In this work, we utilize the power
of Large Language Models (LLMs) to develop a co-pilot agent for assisting gem5
users with automating design space exploration. Computer architecture design
space exploration is complex and time-consuming, given that numerous parameter
settings and simulation statistics must be analyzed before improving the
current design. The emergence of LLMs has significantly accelerated the
analysis of long-text data as well as smart decision making, two key functions
in a successful design space exploration task. In this project, we first build
gem5 Co-Pilot, an AI agent assistant for gem5, which comes with a webpage-GUI
for smooth user interaction, agent automation, and result summarization. We
also implemented a language for design space exploration, as well as a Design
Space Database (DSDB). With DSDB, gem5 Co-Pilot effectively implements a
Retrieval Augmented Generation system for gem5 design space exploration. We
experiment on cost-constraint optimization with four cost ranges and compare
our results with two baseline models. Results show that gem5 Co-Pilot can
quickly identify optimal parameters for specific design constraints based on
performance and cost, with limited user interaction.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [26] [Local Guidance for Configuration-Based Multi-Agent Pathfinding](https://arxiv.org/abs/2510.19072)
*Tomoki Arita,Keisuke Okumura*

Main category: cs.MA

TL;DR: 本文提出一种局部引导方法，在每个智能体附近提供时空线索，显著提升多智能体路径规划（MAPF）求解质量，且在合理计算时间内优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有引导方法依赖全局信息以缓解拥堵，但本文探索是否可通过局部引导实现类似甚至更优的性能，同时避免高昂计算开销。

Method: 在每个智能体周围引入局部引导机制，向规划器提供信息丰富的时空线索，并将其集成到基于配置的先进求解器LaCAM中。

Result: 实验表明，该局部引导方法在不超出中等时间预算的前提下显著提升了解的质量，并在MAPF任务中建立了新的性能前沿。

Conclusion: 局部引导是一种高效且有效的策略，能够在保持计算效率的同时显著提升多智能体路径规划的整体协调性和解的质量。

Abstract: Guidance is an emerging concept that improves the empirical performance of
real-time, sub-optimal multi-agent pathfinding (MAPF) methods. It offers
additional information to MAPF algorithms to mitigate congestion on a global
scale by considering the collective behavior of all agents across the entire
workspace. This global perspective helps reduce agents' waiting times, thereby
improving overall coordination efficiency. In contrast, this study explores an
alternative approach: providing local guidance in the vicinity of each agent.
While such localized methods involve recomputation as agents move and may
appear computationally demanding, we empirically demonstrate that supplying
informative spatiotemporal cues to the planner can significantly improve
solution quality without exceeding a moderate time budget. When applied to
LaCAM, a leading configuration-based solver, this form of guidance establishes
a new performance frontier for MAPF.

</details>


### [27] [SORA-ATMAS: Adaptive Trust Management and Multi-LLM Aligned Governance for Future Smart Cities](https://arxiv.org/abs/2510.19327)
*Usama Antuley,Shahbaz Siddiqui,Sufian Hameed,Waqas Arif,Subhan Shah,Syed Attique Shah*

Main category: cs.MA

TL;DR: SORA-ATMAS 是一个面向智能城市中多智能体系统的治理框架，通过集成领域特定规则与后备机制，在保障合规性与安全性的前提下，显著提升多LLM协同决策的准确性与实时性。


<details>
  <summary>Details</summary>
Motivation: 智能城市依赖智能互联服务，而基于大语言模型的自主智能体虽能提升系统响应能力，但在异构环境中面临治理、风险与合规（GRC）挑战，如问责制、数据隐私和法规一致性问题。

Method: 提出 SORA-ATMAS 框架，整合 Weather、Traffic 和 Safety 三个领域智能体，通过治理策略（包括高风险场景的后备机制）引导多个 LLM（GPT、Grok、DeepSeek）生成符合政策且领域优化的输出，并进行跨域规则约束与运行时性能评估。

Result: 实验表明，该框架平均降低各智能体 MAE 达 35%，在天气监测、高风险交通（得分 0.85）和安全/火灾场景（得分 0.65）中表现稳健；3 智能体部署下吞吐量达 13.8–17.2 请求/秒，执行时间低于 72 毫秒，治理延迟低于 100 毫秒，且具备良好可扩展性。

Conclusion: SORA-ATMAS 提供了一个符合法规、上下文感知且可验证的治理框架，能够将分布式智能体输出整合为可问责的实时决策，为智能城市管理奠定可靠基础。

Abstract: The rapid evolution of smart cities has increased the reliance on intelligent
interconnected services to optimize infrastructure, resources, and citizen
well-being. Agentic AI has emerged as a key enabler by supporting autonomous
decision-making and adaptive coordination, allowing urban systems to respond in
real time to dynamic conditions. Its benefits are evident in areas such as
transportation, where the integration of traffic data, weather forecasts, and
safety sensors enables dynamic rerouting and a faster response to hazards.
However, its deployment across heterogeneous smart city ecosystems raises
critical governance, risk, and compliance (GRC) challenges, including
accountability, data privacy, and regulatory alignment within decentralized
infrastructures. Evaluation of SORA-ATMAS with three domain agents (Weather,
Traffic, and Safety) demonstrated that its governance policies, including a
fallback mechanism for high-risk scenarios, effectively steer multiple LLMs
(GPT, Grok, DeepSeek) towards domain-optimized, policy-aligned outputs,
producing an average MAE reduction of 35% across agents. Results showed stable
weather monitoring, effective handling of high-risk traffic plateaus 0.85, and
adaptive trust regulation in Safety/Fire scenarios 0.65. Runtime profiling of a
3-agent deployment confirmed scalability, with throughput between 13.8-17.2
requests per second, execution times below 72~ms, and governance delays under
100 ms, analytical projections suggest maintained performance at larger scales.
Cross-domain rules ensured safe interoperability, with traffic rerouting
permitted only under validated weather conditions. These findings validate
SORA-ATMAS as a regulation-aligned, context-aware, and verifiable governance
framework that consolidates distributed agent outputs into accountable,
real-time decisions, offering a resilient foundation for smart-city management.

</details>


### [28] [ColorAgent: Building A Robust, Personalized, and Interactive OS Agent](https://arxiv.org/abs/2510.19386)
*Ning Li,Qiqiang Lin,Zheng Wu,Xiaoyun Mo,Weiming Zhang,Yin Zhao,Xiangmou Qu,Jiamu Zhou,Jun Wang,Congmin Zheng,Yuanyi Song,Hongjiang Chen,Heyuan Huang,Jihong Wang,Jiaxin Yin,Jingwei Yu,Junwei Liao,Qiuying Peng,Xingyu Lou,Jun Wang,Weiwen Liu,Zhuosheng Zhang,Weinan Zhang*

Main category: cs.MA

TL;DR: 本文提出了ColorAgent，一种能够在操作系统中进行长周期、鲁棒环境交互并支持个性化主动用户交互的OS智能体。通过逐步强化学习、自进化训练和多智能体框架提升性能，在AndroidWorld和AndroidLab基准上分别达到77.2%和50.7%的成功率，刷新了当前最优水平。


<details>
  <summary>Details</summary>
Motivation: 随着软硬件和大语言模型技术的发展，人与操作系统的交互正从命令行向AI智能体演进。构建能准确执行用户指令、忠实反映用户意图的OS智能体成为现实需求。然而，现有基准和方法在长周期交互、个性化和安全性等方面仍存在不足。

Method: 作者采用逐步强化学习与自进化训练提升模型能力，并设计了专用的多智能体框架以确保通用性、一致性和鲁棒性。在用户交互方面，引入个性化意图识别与主动交互机制，使智能体成为协作伙伴而非单纯自动化工具。

Result: ColorAgent在AndroidWorld和AndroidLab基准测试中分别取得了77.2%和50.7%的成功率，达到当前最优水平。

Conclusion: 尽管ColorAgent表现优异，但当前评估基准仍不足以全面衡量OS智能体的能力。未来工作应聚焦于评估范式、智能体协作与安全性等方向。

Abstract: With the advancements in hardware, software, and large language model
technologies, the interaction between humans and operating systems has evolved
from the command-line interface to the rapidly emerging AI agent interactions.
Building an operating system (OS) agent capable of executing user instructions
and faithfully following user desires is becoming a reality. In this technical
report, we present ColorAgent, an OS agent designed to engage in long-horizon,
robust interactions with the environment while also enabling personalized and
proactive user interaction. To enable long-horizon interactions with the
environment, we enhance the model's capabilities through step-wise
reinforcement learning and self-evolving training, while also developing a
tailored multi-agent framework that ensures generality, consistency, and
robustness. In terms of user interaction, we explore personalized user intent
recognition and proactive engagement, positioning the OS agent not merely as an
automation tool but as a warm, collaborative partner. We evaluate ColorAgent on
the AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2%
and 50.7%, respectively, establishing a new state of the art. Nonetheless, we
note that current benchmarks are insufficient for a comprehensive evaluation of
OS agents and propose further exploring directions in future work, particularly
in the areas of evaluation paradigms, agent collaboration, and security. Our
code is available at https://github.com/MadeAgents/mobile-use.

</details>


### [29] [Modeling realistic human behavior using generative agents in a multimodal transport system: Software architecture and Application to Toulouse](https://arxiv.org/abs/2510.19497)
*Trung-Dung Vu,Benoit Gaudou,Kamaldeep Singh Oberoi*

Main category: cs.MA

TL;DR: 本文提出一种结合大语言模型（LLM）与基于智能体的仿真框架，用于模拟真实城市环境中人类在多模式交通系统中的出行决策行为，并在法国图卢兹案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 建模真实的人类行为以理解其出行方式选择，从而提供个性化出行解决方案仍具挑战性。

Method: 将大语言模型嵌入基于智能体的仿真系统中，结合GAMA仿真平台、GTFS公共交通数据和OpenTripPlanner多模式路径规划工具，在真实城市环境中模拟人类出行决策。

Result: 在一个月的模拟中，智能体不仅能做出情境感知的交通决策，还能随时间形成出行习惯。

Conclusion: 结合大语言模型与基于智能体的仿真为发展智能交通系统和个性化多模式出行方案提供了有前景的方向，未来工作包括扩展至更大区域、整合实时数据及优化记忆模型。

Abstract: Modeling realistic human behaviour to understand people's mode choices in
order to propose personalised mobility solutions remains challenging. This
paper presents an architecture for modeling realistic human mobility behavior
in complex multimodal transport systems, demonstrated through a case study in
Toulouse, France. We apply Large Language Models (LLMs) within an agent-based
simulation to capture decision-making in a real urban setting. The framework
integrates the GAMA simulation platform with an LLM-based generative agent,
along with General Transit Feed Specification (GTFS) data for public transport,
and OpenTripPlanner for multimodal routing. GAMA platform models the
interactive transport environment, providing visualization and dynamic agent
interactions while eliminating the need to construct the simulation environment
from scratch. This design enables a stronger focus on developing generative
agents and evaluating their performance in transport decision-making processes.
Over a simulated month, results show that agents not only make context-aware
transport decisions but also form habits over time. We conclude that combining
LLMs with agent-based simulation offers a promising direction for advancing
intelligent transportation systems and personalised multimodal mobility
solutions. We also discuss some limitations of this approach and outline future
work on scaling to larger regions, integrating real-time data, and refining
memory models.

</details>


### [30] [Polynomial-time Configuration Generator for Connected Unlabeled Multi-Agent Pathfinding](https://arxiv.org/abs/2510.19567)
*Takahiro Suzuki,Keisuke Okumura*

Main category: cs.MA

TL;DR: 本文研究了连通无标签多智能体路径规划（CUMAPF）问题，提出了一种名为PULL的轻量级、多项式时间算法，能在保持智能体连通性的同时高效求解，并在实验中展现出良好的性能。


<details>
  <summary>Details</summary>
Motivation: 标准的多智能体路径规划（MAPF）无法保证智能体之间的连通性，而这一特性在群体机器人应用（如自重构和行进）中至关重要。因此，作者研究了要求智能体始终维持连通性的CUMAPF问题。

Method: 作者提出了PULL算法，这是一种基于规则的单步函数，在每一步计算出一个既能保持连通性又向目标构型推进的新构型。该算法在二维网格中每步运行时间为$O(n^2)$，并进一步将其集成到现有基于搜索的MAPF算法中，构建了一个渐近最优求解器。

Result: 实验表明，PULL算法在数百个智能体的随机实例中能生成与平凡解相比具有竞争力的解质量，且运行高效。

Conclusion: PULL是一种完整且高效的CUMAPF求解算法，适用于大规模场景；结合搜索方法后还可为小规模实例提供渐近最优解，为连通性约束下的多智能体路径规划提供了实用工具。

Abstract: We consider Connected Unlabeled Multi-Agent Pathfinding (CUMAPF), a variant
of MAPF where the agents must maintain connectivity at all times. This problem
is fundamental to swarm robotics applications like self-reconfiguration and
marching, where standard MAPF is insufficient as it does not guarantee the
required connectivity between agents. While unlabeled MAPF is tractable in
optimization, CUMAPF is NP-hard even on highly restricted graph classes. To
tackle this challenge, we propose PULL, a complete and polynomial-time
algorithm with a simple design. It is based on a rule-based one-step function
that computes a subsequent configuration that preserves connectivity and
advances towards the target configuration. PULL is lightweight, and runs in
$O(n^2)$ time per step in 2D grid, where $n$ is the number of agents. Our
experiments further demonstrate its practical performance: PULL finds
competitive solution qualities against trivial solutions for hundreds of
agents, in randomly generated instances. Furthermore, we develop an eventually
optimal solver that integrates PULL into an existing search-based MAPF
algorithm, providing a valuable tool for small-scale instances.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [31] [Tidying Up the Address Space](https://arxiv.org/abs/2510.19765)
*Vinay Banakar,Suli Yang,Kan Wu,Andrea C. Arpaci-Dusseau,Remzi H. Arpaci-Dusseau,Kimberly Keeton*

Main category: cs.OS

TL;DR: 该论文提出了一种名为“地址空间工程”的方法，通过动态重组应用程序的虚拟地址空间，将热对象和冷对象分别聚集到统一的区域中，从而解决内存分层中因“热度碎片”导致的效率低下问题。该方法在几乎不影响性能的前提下，显著提升了内存回收效率。


<details>
  <summary>Details</summary>
Motivation: 数据中心的内存分层技术因“热度碎片”问题（即热对象与冷对象混杂在同一内存页中）而无法充分发挥潜力，导致基于页的回收系统难以准确识别真正热的页面，限制了内存利用效率。

Method: 提出“地址空间工程”方法，通过编译器-运行时系统（HADES）动态追踪对象访问模式，并迁移对象以形成统一热度的内存区域，使现有页级分层后端能更高效地管理内存。

Result: 在十个数据结构上的评估表明，该方法最多可减少70%的内存使用，仅带来3%的性能开销。

Conclusion: 地址空间工程能有效提升现有内存回收系统的效率，在不显著影响性能的前提下实现更激进的内存回收。

Abstract: Memory tiering in datacenters does not achieve its full potential due to
hotness fragmentation -- the intermingling of hot and cold objects within
memory pages. This fragmentation prevents page-based reclamation systems from
distinguishing truly hot pages from pages containing mostly cold objects,
fundamentally limiting memory efficiency despite highly skewed accesses. We
introduce address-space engineering: dynamically reorganizing application
virtual address spaces to create uniformly hot and cold regions that any
page-level tiering backend can manage effectively. HADES demonstrates this
frontend/backend approach through a compiler-runtime system that tracks and
migrates objects based on access patterns, requiring minimal developer
intervention. Evaluations across ten data structures achieve up to 70% memory
reduction with 3% performance overhead, showing that address space engineering
enables existing reclamation systems to reclaim memory aggressively without
performance degradation.

</details>
