<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A Hybrid Reactive-Proactive Auto-scaling Algorithm for SLA-Constrained Edge Computing](https://arxiv.org/abs/2512.14290)
*Suhrid Gupta,Muhammed Tawfiqul Islam,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文提出了一种结合机器学习预测与实时资源调整的混合自动扩缩容算法，用于保障边缘计算应用的服务等级协议（SLA），并在Kubernetes中实现，实验表明其SLA违规率仅为6%，显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有边缘计算自动扩缩容算法存在性能不足和配置复杂的问题，难以满足严格SLA要求，亟需更高效解决方案。

Method: 提出一种融合ML预测式扩缩容与基于当前资源利用率及SLA约束的反应式扩缩容的混合算法，并集成到Kubernetes平台。

Result: 在真实边缘环境中实验显示，该方案将SLA违规率从最高23%降至6%，显著提升稳定性与合规性。

Conclusion: 所提混合扩缩容算法有效降低SLA违规率，为边缘计算环境提供可靠资源管理方案，具备实际部署价值。

Abstract: Edge computing decentralizes computing resources, allowing for novel applications in domains such as the Internet of Things (IoT) in healthcare and agriculture by reducing latency and improving performance. This decentralization is achieved through the implementation of microservice architectures, which require low latencies to meet stringent service level agreements (SLA) such as performance, reliability, and availability metrics. While cloud computing offers the large data storage and computation resources necessary to handle peak demands, a hybrid cloud and edge environment is required to ensure SLA compliance. This is achieved by sophisticated orchestration strategies such as Kubernetes, which help facilitate resource management. The orchestration strategies alone do not guarantee SLA adherence due to the inherent delay of scaling resources. Existing auto-scaling algorithms have been proposed to address these challenges, but they suffer from performance issues and configuration complexity. In this paper, a novel auto-scaling algorithm is proposed for SLA-constrained edge computing applications. This approach combines a Machine Learning (ML) based proactive auto-scaling algorithm, capable of predicting incoming resource requests to forecast demand, with a reactive autoscaler which considers current resource utilization and SLA constraints for immediate adjustments. The algorithm is integrated into Kubernetes as an extension, and its performance is evaluated through extensive experiments in an edge environment with real applications. The results demonstrate that existing solutions have an SLA violation rate of up to 23%, whereas the proposed hybrid solution outperforms the baselines with an SLA violation rate of only 6%, ensuring stable SLA compliance across various applications.

</details>


### [2] [Performance and Stability of Barrier Mode Parallel Systems with Heterogeneous and Redundant Jobs](https://arxiv.org/abs/2512.14445)
*Brenton Walker,Markus Fidler*

Main category: cs.DC

TL;DR: 本文分析了并行计算中屏障机制对系统稳定性和性能的影响，并提出了混合屏障系统的性能界限模型。


<details>
  <summary>Details</summary>
Motivation: 屏障机制虽支持任务同步，但会导致部分工作节点空闲，降低系统稳定性和性能，需深入研究其影响。

Method: 通过分析(s,k,l)屏障系统稳定性、推导混合屏障系统性能界限，并结合Spark实测数据与仿真验证模型准确性。

Result: 发现1-屏障情况下的性能界限与仿真结果吻合良好，系统开销主要源于事件与轮询双驱动调度机制。

Conclusion: 提出的开销模型能有效解释真实系统行为，为优化屏障模式调度提供理论依据。

Abstract: In some models of parallel computation, jobs are split into smaller tasks and can be executed completely asynchronously. In other situations the parallel tasks have constraints that require them to synchronize their start and possibly departure times. This is true of many parallelized machine learning workloads, and the popular Apache Spark processing engine has recently added support for Barrier Execution Mode, which allows users to add such barriers to their jobs. These barriers necessarily result in idle periods on some of the workers, which reduces their stability and performance, compared to equivalent workloads with no barriers.
  In this paper we will consider and analyze the stability and performance penalties resulting from barriers. We include an analysis of the stability of $(s,k,l)$ barrier systems that allow jobs to depart after $l$ out of $k$ of their tasks complete. We also derive and evaluate performance bounds for hybrid barrier systems servicing a mix of jobs, both with and without barriers, and with varying degrees of parallelism. For the purely 1-barrier case we compare the bounds and simulation results to benchmark data from a standalone Spark system. We study the overhead in the real system, and based on its distribution we attribute it to the dual event and polling-driven mechanism used to schedule barrier-mode jobs. We develop a model for this type of overhead and validate it against the real system through simulation.

</details>


### [3] [PruneX: A Hierarchical Communication-Efficient System for Distributed CNN Training with Structured Pruning](https://arxiv.org/abs/2512.14628)
*Alireza Olama,Andreas Lundell,Izzat El Hajj,Johan Lilius,Jerker Björkqvist*

Main category: cs.DC

TL;DR: PruneX通过协同设计剪枝算法与集群层次结构，显著减少多节点GPU集群训练中的跨节点通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有分布式训练系统难以利用非结构化稀疏性降低通信开销，限制了大规模训练效率。

Method: 提出H-SADMM算法强制节点级结构化稀疏，并采用领导者-跟随者执行模型分离节点内外通信。

Result: 在64个GPU上测试，跨节点通信量减少约60%，强扩展加速比达6.75倍，优于基线和Top-K方法。

Conclusion: PruneX有效优化了分布式训练通信效率，为大规模紧凑模型训练提供了实用解决方案。

Abstract: Inter-node communication bandwidth increasingly constrains distributed training at scale on multi-node GPU clusters. While compact models are the ultimate deployment target, conventional pruning-aware distributed training systems typically fail to reduce communication overhead because unstructured sparsity cannot be efficiently exploited by highly optimized dense collective primitives. We present PruneX, a distributed data-parallel training system that co-designs pruning algorithms with cluster hierarchy to reduce inter-node bandwidth usage. PruneX introduces the Hierarchical Structured ADMM (H-SADMM) algorithm, which enforces node-level structured sparsity before inter-node synchronization, enabling dynamic buffer compaction that eliminates both zero-valued transmissions and indexing overhead. The system adopts a leader-follower execution model with separated intra-node and inter-node process groups, performing dense collectives on compacted tensors over bandwidth-limited links while confining full synchronization to high-bandwidth intra-node interconnects. Evaluation on ResNet architectures across 64 GPUs demonstrates that PruneX reduces inter-node communication volume by approximately 60% and achieves 6.75x strong scaling speedup, outperforming the dense baseline (5.81x) and Top-K gradient compression (3.71x) on the Puhti supercomputer at CSC - IT Center for Science (Finland).

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [4] [Multi-Agent Collaborative Framework for Intelligent IT Operations: An AOI System with Context-Aware Compression and Dynamic Task Scheduling](https://arxiv.org/abs/2512.13956)
*Zishan Bai,Enze Ge,Junfeng Hao*

Main category: cs.MA

TL;DR: AOI框架通过多智能体协作与上下文压缩技术，显著提升云原生运维效率并降低修复时间。


<details>
  <summary>Details</summary>
Motivation: 云原生架构复杂性导致运维数据过载、任务协调低效及上下文丢失，亟需智能化解决方案。

Method: 提出AOI框架，结合动态任务调度策略与三层记忆架构（工作、情景、语义），利用LLM压缩上下文。

Result: 实验显示AOI实现72.4%上下文压缩率、92.8%关键信息保留率、94.2%任务成功率，MTTR降低34.4%。

Conclusion: AOI为下一代IT基础设施提供可扩展、自适应、上下文感知的自主运维新范式。

Abstract: The proliferation of cloud-native architectures, characterized by microservices and dynamic orchestration, has rendered modern IT infrastructures exceedingly complex and volatile. This complexity generates overwhelming volumes of operational data, leading to critical bottlenecks in conventional systems: inefficient information processing, poor task coordination, and loss of contextual continuity during fault diagnosis and remediation. To address these challenges, we propose AOI (AI-Oriented Operations), a novel multi-agent collaborative framework that integrates three specialized agents with an LLM-based Context Compressor. Its core innovations include: (1) a dynamic task scheduling strategy that adaptively prioritizes operations based on real-time system states, and (2) a three-layer memory architecture comprising Working, Episodic, and Semantic layers that optimizes context retention and retrieval. Extensive experiments on both synthetic and real-world benchmarks demonstrate that AOI effectively mitigates information overload, achieving a 72.4% context compression ratio while preserving 92.8% of critical information and significantly enhances operational efficiency, attaining a 94.2% task success rate and reducing the Mean Time to Repair (MTTR) by 34.4% compared to the best baseline. This work presents a paradigm shift towards scalable, adaptive, and context-aware autonomous operations, enabling robust management of next-generation IT infrastructures with minimal human intervention.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [5] [ReadyPower: A Reliable, Interpretable, and Handy Architectural Power Model Based on Analytical Framework](https://arxiv.org/abs/2512.14172)
*Qijun Zhang,Shang Liu,Yao Lu,Mengming Li,Zhiyao Xie*

Main category: cs.AR

TL;DR: ReadyPower是一种新的分析性功耗建模框架，通过引入多层次参数显著提升准确性，优于基于机器学习的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有分析模型精度低，而机器学习模型虽准但存在可靠性、可解释性和易用性问题，工业界采纳有限。

Method: 在McPAT基础上引入架构级、实现级和技术级参数，构建可靠、可解释且易用的ReadyPower模型。

Result: 在BOOM和XiangShan架构上，ReadyPower平均MAPE降低超20%，相关系数R提高超0.2，优于ML基线。

Conclusion: ReadyPower有效弥合了分析模型与实际处理器实现间的差距，兼具高精度与实用性，适合工业部署。

Abstract: Power is a primary objective in modern processor design, requiring accurate yet efficient power modeling techniques. Architecture-level power models are necessary for early power optimization and design space exploration. However, classical analytical architecture-level power models (e.g., McPAT) suffer from significant inaccuracies. Emerging machine learning (ML)-based power models, despite their superior accuracy in research papers, are not widely adopted in the industry. In this work, we point out three inherent limitations of ML-based power models: unreliability, limited interpretability, and difficulty in usage. This work proposes a new analytical power modeling framework named ReadyPower, which is ready-for-use by being reliable, interpretable, and handy. We observe that the root cause of the low accuracy of classical analytical power models is the discrepancies between the real processor implementation and the processor's analytical model. To bridge the discrepancies, we introduce architecture-level, implementation-level, and technology-level parameters into the widely adopted McPAT analytical model to build ReadyPower. The parameters at three different levels are decided in different ways. In our experiment, averaged across different training scenarios, ReadyPower achieves >20% lower mean absolute percentage error (MAPE) and >0.2 higher correlation coefficient R compared with the ML-based baselines, on both BOOM and XiangShan CPU architectures.baselines, on both BOOM and XiangShan CPU architectures.

</details>


### [6] [TEMP: A Memory Efficient Physical-aware Tensor Partition-Mapping Framework on Wafer-scale Chips](https://arxiv.org/abs/2512.14256)
*Huizheng Wang,Taiquan Wei,Zichuan Wang,Dingcheng Jiang,Qize Yang,Jiaxin Liu,Jingxiang Hou,Chao Li,Jinyi Deng,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: TEMP框架通过拓扑感知的张量流分区和流量感知映射，在晶圆级芯片上实现高效大语言模型训练，平均吞吐量提升1.7倍。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法在晶圆级芯片上平衡内存与计算资源，且未充分利用其高带宽优势。

Method: 提出TSPP范式并结合拓扑感知分区、流量感知映射及双层晶圆求解器优化内存效率与吞吐量。

Result: 在多种模型上相比最先进系统平均吞吐量提升1.7倍。

Conclusion: TEMP有效释放了TSPP在晶圆级芯片上的潜力，显著提升LLM训练性能。

Abstract: Large language models (LLMs) demand significant memory and computation resources. Wafer-scale chips (WSCs) provide high computation power and die-to-die (D2D) bandwidth but face a unique trade-off between on-chip memory and compute resources due to limited wafer area. Therefore, tensor parallelism strategies for wafer should leverage communication advantages while maintaining memory efficiency to maximize WSC performance. However, existing approaches fail to address these challenges.
  To address these challenges, we propose the tensor stream partition paradigm (TSPP), which reveals an opportunity to leverage WSCs' abundant communication bandwidth to alleviate stringent on-chip memory constraints. However, the 2D mesh topology of WSCs lacks long-distance and flexible interconnects, leading to three challenges: 1) severe tail latency, 2) prohibitive D2D traffic contention, and 3) intractable search time for optimal design.
  We present TEMP, a framework for LLM training on WSCs that leverages topology-aware tensor-stream partition, traffic-conscious mapping, and dual-level wafer solving to overcome hardware constraints and parallelism challenges. These integrated approaches optimize memory efficiency and throughput, unlocking TSPP's full potential on WSCs. Evaluations show TEMP achieves 1.7x average throughput improvement over state-of-the-art LLM training systems across various models.

</details>


### [7] [PADE: A Predictor-Free Sparse Attention Accelerator via Unified Execution and Stage Fusion](https://arxiv.org/abs/2512.14322)
*Huizheng Wang,Hongbin Wang,Zichuan Wang,Zhiheng Yue,Yang Wang,Chao Li,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: PADE是一种无需额外稀疏性预测器的动态稀疏注意力算法-硬件协同设计，通过三项创新技术显著提升能效与速度。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法因预测器开销大而硬件效率低，需更实用方案。

Method: 提出BUI-GF、BS-OOE和ISTA三项核心技术，结合定制加速器实现高效稀疏注意力计算。

Result: 在22个基准测试中，相较H100 GPU实现7.43倍加速与31.1倍能效提升，优于SOTA加速器。

Conclusion: PADE有效解决了稀疏注意力中的准确性和硬件利用率问题，是实用高效的注意力加速方案。

Abstract: Attention-based models have revolutionized AI, but the quadratic cost of self-attention incurs severe computational and memory overhead. Sparse attention methods alleviate this by skipping low-relevance token pairs. However, current approaches lack practicality due to the heavy expense of added sparsity predictor, which severely drops their hardware efficiency.
  This paper advances the state-of-the-art (SOTA) by proposing a bit-serial enable stage-fusion (BSF) mechanism, which eliminates the need for a separate predictor. However, it faces key challenges: 1) Inaccurate bit-sliced sparsity speculation leads to incorrect pruning; 2) Hardware under-utilization due to fine-grained and imbalanced bit-level workloads. 3) Tiling difficulty caused by the row-wise dependency in sparsity pruning criteria.
  We propose PADE, a predictor-free algorithm-hardware co-design for dynamic sparse attention acceleration. PADE features three key innovations: 1) Bit-wise uncertainty interval-enabled guard filtering (BUI-GF) strategy to accurately identify trivial tokens during each bit round; 2) Bidirectional sparsity-based out-of-order execution (BS-OOE) to improve hardware utilization; 3) Interleaving-based sparsity-tiled attention (ISTA) to reduce both I/O and computational complexity. These techniques, combined with custom accelerator designs, enable practical sparsity acceleration without relying on an added sparsity predictor. Extensive experiments on 22 benchmarks show that PADE achieves 7.43x speed up and 31.1x higher energy efficiency than Nvidia H100 GPU. Compared to SOTA accelerators, PADE achieves 5.1x, 4.3x and 3.4x energy saving than Sanger, DOTA and SOFA.

</details>


### [8] [Focus: A Streaming Concentration Architecture for Efficient Vision-Language Models](https://arxiv.org/abs/2512.14661)
*Chiyue Wei,Cong Guo,Junyao Zhang,Haoxuan Shan,Yifan Xu,Ziyue Zhang,Yudong Liu,Qinsi Wang,Changchun Zhou,Hai "Helen" Li,Yiran Chen*

Main category: cs.AR

TL;DR: Focus是一种流式浓缩架构，通过多层次细粒度冗余消除加速视觉语言模型推理，显著提升性能与能效。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型因规模庞大和视频级输入导致计算与内存开销大，难以实时部署。

Method: 提出三级浓缩机制：语义引导的token剪枝、时空块级浓缩、运动感知向量级去冗余，并与硬件架构协同设计。

Result: 在脉动阵列加速器中实现2.4倍加速与3.3倍能耗降低，优于现有方案。

Conclusion: Focus为视觉语言模型提供高效、低功耗的推理加速方案，支持流式芯片执行并已开源。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance on tasks such as video captioning and visual question answering. However, their growing scale and video-level inputs lead to significant computational and memory overhead, posing challenges for real-time deployment on hardware accelerators. While prior work attempts to reduce redundancy via token pruning or merging, these methods typically operate at coarse granularity and incur high runtime overhead due to global token-level operations. In this study, we propose Focus, a Streaming Concentration Architecture that efficiently accelerates VLM inference through progressive, fine-grained redundancy elimination. Focus introduces a multilevel concentration paradigm that hierarchically compresses vision-language inputs at three levels: (1) semantic-guided token pruning based on textual prompts, (2) spatial-temporal block-level concentration using localized comparisons, and (3) vector-level redundancy removal via motion-aware matching. All concentration steps are tightly co-designed with the architecture to support streaming-friendly, on-chip execution. Focus leverages GEMM tiling, convolution-style layout, and cross-modal attention to minimize off-chip access while enabling high throughput. Implemented as a modular unit within a systolic-array accelerator, Focus achieves a 2.4x speedup and 3.3x reduction in energy, significantly outperforming state-of-the-art accelerators in both performance and energy efficiency. Full-stack implementation of Focus is open-sourced at https://github.com/dubcyfor3/Focus.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [Practitioner Insights on Fairness Requirements in the AI Development Life Cycle: An Interview Study](https://arxiv.org/abs/2512.13830)
*Chaima Boufaied,Thanh Nguyen,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本文通过26位来自不同国家和领域的从业者访谈，探讨AI公平性在软件开发生命周期中的实践现状与挑战。


<details>
  <summary>Details</summary>
Motivation: AI模型常作为黑箱运行，易对不同人群产生不公平影响，因此需从软件工程角度研究如何将公平性融入开发流程。

Method: 采用半结构化访谈方法，对23国26位从业者进行定性主题分析，覆盖公平性意识、需求转化、评估维度及与其他目标的权衡。

Result: 从业者虽认知公平性维度，但实践不一致、常被降级处理，且存在知识缺口；亟需明确定义、评估指标与正式流程以整合公平性。

Conclusion: 为有效实现AI公平性，必须与利益相关者达成共识，建立情境适配的定义、度量标准与制度化流程，将其系统性嵌入AI/ML项目。

Abstract: Nowadays, Artificial Intelligence (AI), particularly Machine Learning (ML) and Large Language Models (LLMs), is widely applied across various contexts. However, the corresponding models often operate as black boxes, leading them to unintentionally act unfairly towards different demographic groups. This has led to a growing focus on fairness in AI software recently, alongside the traditional focus on the effectiveness of AI models. Through 26 semi-structured interviews with practitioners from different application domains and with varied backgrounds across 23 countries, we conducted research on fairness requirements in AI from software engineering perspective. Our study assesses the participants' awareness of fairness in AI / ML software and its application within the Software Development Life Cycle (SDLC), from translating fairness concerns into requirements to assessing their arising early in the SDLC. It also examines fairness through the key assessment dimensions of implementation, validation, evaluation, and how it is balanced with trade-offs involving other priorities, such as addressing all the software functionalities and meeting critical delivery deadlines. Findings of our thematic qualitative analysis show that while our participants recognize the aforementioned AI fairness dimensions, practices are inconsistent, and fairness is often deprioritized with noticeable knowledge gaps. This highlights the need for agreement with relevant stakeholders on well-defined, contextually appropriate fairness definitions, the corresponding evaluation metrics, and formalized processes to better integrate fairness into AI/ML projects.

</details>


### [10] [Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors](https://arxiv.org/abs/2512.13860)
*Henger Li,Shuangjie You,Flavio Di Palo,Yiyue Qian,Ayush Jain*

Main category: cs.SE

TL;DR: VGCO框架通过LLM编辑器自动优化工具文档，提升单轮大规模工具调用的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有工具文档多为人设计，与LLM理解方式不匹配，尤其在工业场景下面临扩展性、多样性和歧义性挑战。

Method: 提出两阶段框架：评估阶段收集失败案例识别错配，优化阶段通过分层编辑进行离线结构感知优化；编辑器具备层次结构、状态感知、动作特定和验证引导特性。

Result: 在多个LLM上显著提升准确性、鲁棒性和泛化能力，优于强调多轮推理的先前方法。

Conclusion: VGCO为大规模单轮工具调用提供高效、低成本且可扩展的上下文优化方案。

Abstract: Tool calling enables large language models (LLMs) to interact with external environments through tool invocation, providing a practical way to overcome the limitations of pretraining. However, the effectiveness of tool use depends heavily on the quality of the associated documentation and knowledge base context. These materials are usually written for human users and are often misaligned with how LLMs interpret information. This problem is even more pronounced in industrial settings, where hundreds of tools with overlapping functionality create challenges in scalability, variability, and ambiguity. We propose Verification-Guided Context Optimization (VGCO), a framework that uses LLMs as editors to automatically refine tool-related documentation and knowledge base context. VGCO works in two stages. First, Evaluation collects real-world failure cases and identifies mismatches between tools and their context. Second, Optimization performs hierarchical editing through offline learning with structure-aware, in-context optimization. The novelty of our LLM editors has three main aspects. First, they use a hierarchical structure that naturally integrates into the tool-calling workflow. Second, they are state-aware, action-specific, and verification-guided, which constrains the search space and enables efficient, targeted improvements. Third, they enable cost-efficient sub-task specialization, either by prompt engineering large editor models or by post-training smaller editor models. Unlike prior work that emphasizes multi-turn reasoning, VGCO focuses on the single-turn, large-scale tool-calling problem and achieves significant improvements in accuracy, robustness, and generalization across LLMs.

</details>


### [11] [Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming](https://arxiv.org/abs/2512.13914)
*Bhargav Chickmagalur Nanjundappa,Spandan Maaheshwari*

Main category: cs.SE

TL;DR: ContextBranch引入版本控制语义管理LLM对话，通过分支机制隔离探索路径，显著提升多轮对话质量并减少上下文污染。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在多轮对话中因上下文污染导致性能下降的问题，尤其在探索性编程任务中支持灵活尝试不同方案。

Method: 设计checkpoint、branch、switch、inject四个原语，实现对话状态保存、分支隔离与选择性合并。

Result: 实验显示分支对话在复杂场景下显著提升响应质量，聚焦性和上下文感知能力增强，同时减少58.1%的上下文消息量。

Conclusion: 对话分支是AI辅助探索性工作的基础原语，能有效防止探索过程中的上下文污染。

Abstract: Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context.
  We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.

</details>


### [12] [PerfCoder: Large Language Models for Interpretable Code Performance Optimization](https://arxiv.org/abs/2512.14018)
*Jiuding Yang,Shengyao Lu,Hongxuan Liu,Shayan Shirahmad Gale Bagi,Zahra Fazel,Tomasz Czajkowski,Di Niu*

Main category: cs.SE

TL;DR: PerfCoder 是一种专为生成高性能代码设计的大语言模型，通过可解释的优化策略和强化微调显著提升代码运行效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成高性能代码方面存在不足，主要因缺乏引导性能优化的监督机制。

Method: 基于真实世界优化轨迹数据集进行微调，并结合运行时测量的强化微调对齐偏好，实现输入特定的直接优化。

Result: 在 PIE 基准测试中，PerfCoder 在运行加速和有效优化率上超越现有模型；配合协作流程还能进一步提升 32B 模型和 GPT-5 的性能。

Conclusion: 仅靠模型规模无法实现性能优化，必须引入对优化策略的认知与引导机制。

Abstract: Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.

</details>


### [13] [Aligning Security Compliance and DevOps: A Longitudinal Study](https://arxiv.org/abs/2512.14453)
*Fabiola Moyón,Florian Angermeir,Daniel Mendez,Tony Gorschek,Markus Voggenreiter,Pierre-Louis Bonvin*

Main category: cs.SE

TL;DR: 本文提出基于IEC 62443-4-1标准的RefA框架，帮助企业在DevOps实践中实现安全合规，支持跨职能团队高效交付合规产品。


<details>
  <summary>Details</summary>
Motivation: 企业采用敏捷与DevOps带来安全合规挑战，尤其在关键基础设施领域，亟需适配方案。

Method: 通过西门子公司的纵向研究，分阶段验证并初步应用RefA框架，该框架为非安全专家设计，便于实施。

Result: RefA有效促进安全合规知识向开发团队转移，提升团队自主交付合规产品的能力。

Conclusion: RefA框架有助于企业在保持敏捷性的同时满足安全标准，推动DevOps在受监管行业的落地。

Abstract: Companies adopt agile methodologies and DevOps to facilitate efficient development and deployment of software-intensive products. This, in turn, introduces challenges in relation to security standard compliance traditionally following a more linear workflow. This is especially a challenge for the engineering of products and services associated with critical infrastructures. To support companies in their transition towards DevOps, this paper presents an adaptation of DevOps according to security regulations and standards. We report on our longitudinal study at Siemens AG, consisting of several individual sub-studies in the inception, validation, and initial adoption of our framework based on RefA as well as the implications for practice. RefA is a prescriptive model of a security compliant DevOps lifecycle based on the IEC 62443-4-1 standard. The overall framework is aimed at professionals, not only security experts, being able to use it on implementing DevOps processes while remaining compliant with security norms. We demonstrate how RefA facilitates the transfer of security compliance knowledge to product development teams. This knowledge transfer supports the agility aim of ensuring that cross-functional teams have all the skills needed to deliver the compliant products.

</details>


### [14] [Teralizer: Semantics-Based Test Generalization from Conventional Unit Tests to Property-Based Tests](https://arxiv.org/abs/2512.14475)
*Johann Glock,Clemens Bauer,Martin Pinzger*

Main category: cs.SE

TL;DR: Teralizer自动将单元测试转换为基于属性的测试，通过符号分析提取语义规范，提升测试覆盖率，但在实际项目中面临类型支持和静态分析限制。


<details>
  <summary>Details</summary>
Motivation: 传统单元测试仅验证单个输入输出对，覆盖不足；基于属性的测试虽能生成多组输入，但需人工定义约束，效率低。

Method: 提出语义驱动方法，利用单路径符号分析从实现中自动提取规范，将JUnit测试转为jqwik测试。

Result: 在EvoSuite和Apache Commons数据集上，变异分数提升1-4个百分点；成熟测试仅提升0.05-0.07个百分点；632个项目中仅1.7%成功完成泛化。

Conclusion: 该方法有效提升测试覆盖，但需解决符号分析类型支持与静态分析工程挑战，方能广泛适用。

Abstract: Conventional unit tests validate single input-output pairs, leaving most inputs of an execution path untested. Property-based testing addresses this shortcoming by generating multiple inputs satisfying properties but requires significant manual effort to define properties and their constraints. We propose a semantics-based approach that automatically transforms unit tests into property-based tests by extracting specifications from implementations via single-path symbolic analysis. We demonstrate this approach through Teralizer, a prototype for Java that transforms JUnit tests into property-based jqwik tests. Unlike prior work that generalizes from input-output examples, Teralizer derives specifications from program semantics.
  We evaluated Teralizer on three progressively challenging datasets. On EvoSuite-generated tests for EqBench and Apache Commons utilities, Teralizer improved mutation scores by 1-4 percentage points. Generalization of mature developer-written tests from Apache Commons utilities showed only 0.05-0.07 percentage points improvement. Analysis of 632 real-world Java projects from RepoReapers highlights applicability barriers: only 1.7% of projects completed the generalization pipeline, with failures primarily due to type support limitations in symbolic analysis and static analysis limitations in our prototype. Based on the results, we provide a roadmap for future work, identifying research and engineering challenges that need to be tackled to advance the field of test generalization.
  Artifacts available at: https://doi.org/10.5281/zenodo.17950381

</details>


### [15] [MoT: A Model-Driven Low-Code Approach for Simplifying Cloud-of-Things Application Development](https://arxiv.org/abs/2512.14613)
*Cristiano Welter,Kleinner Farias*

Main category: cs.SE

TL;DR: MoT是一种基于模型的低代码方法，简化云物融合应用开发，提升易用性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前云物融合应用开发缺乏标准化、模型驱动的方法，技术门槛高且效率低。

Method: 提出Model of Things（MoT），结合UML定制化配置文件与低代码原则，并通过案例研究与TAM问卷评估其效果。

Result: 实验证明MoT可行，能降低开发复杂度、加速部署，用户感知易用性与实用性高。

Conclusion: MoT为云物融合提供高效灵活的模型驱动方案，有助于推动该技术更广泛采用。

Abstract: The integration of cloud computing and the Internet of Things (IoT) is essential for scalable, intelligent systems. However, developing cloud-of-things (CoT) applications remains challenging. It requires significant technical expertise and lacks standardized, model-driven methodologies. Current approaches fail to ensure interoperability, automation, and efficiency. This study introduces the Model of Things (MoT), a model-based approach that incorporates low-code principles to simplify CoT development. MoT reduces technical barriers by providing a custom UML profile designed for IoT and cloud services. To evaluate MoT, we conducted a case study and a Technology Acceptance Model (TAM) questionnaire. The results confirmed MoT's feasibility, demonstrating that it streamlines CoT application development and deployment. Users found MoT accessible, even with limited IoT experience, and reported high perceived ease of use and usefulness. Qualitative feedback highlighted MoT's ability to reduce complexity and speed up development. MoT offers a promising, model-driven solution for CoT application development. By lowering entry barriers and promoting automation, it enhances both efficiency and flexibility. This study represents a step toward a more user-friendly framework, enabling broader adoption of CoT technologies.

</details>


### [16] [Reconsidering Conversational Norms in LLM Chatbots for Sustainable AI](https://arxiv.org/abs/2512.14673)
*Ronnie de Souza Santos,Cleyton Magalhães,Italo Santos*

Main category: cs.SE

TL;DR: 该论文探讨用户交互行为对LLM聊天系统能耗的影响，提出从对话设计角度提升可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注模型架构与硬件效率，忽视用户交互对能耗的作用。

Method: 从四个维度分析交互行为如何影响能耗：对话长度、响应延迟期望、用户习惯、上下文累积。

Result: 发现用户交互模式显著影响系统能耗，需重新设计对话机制以实现节能。

Conclusion: 可持续性不仅依赖技术优化，也取决于用户对话规范的设计与引导。

Abstract: LLM based chatbots have become central interfaces in technical, educational, and analytical domains, supporting tasks such as code reasoning, problem solving, and information exploration. As these systems scale, sustainability concerns have intensified, with most assessments focusing on model architecture, hardware efficiency, and deployment infrastructure. However, existing mitigation efforts largely overlook how user interaction practices themselves shape the energy profile of LLM based systems. In this vision paper, we argue that interaction level behavior appears to be an underexamined factor shaping the environmental impact of LLM based systems, and we present this issue across four dimensions. First, extended conversational patterns increase token production and raise the computational cost of inference. Second, expectations of instant responses limit opportunities for energy aware scheduling and workload consolidation. Third, everyday user habits contribute to cumulative operational demand in ways that are rarely quantified. Fourth, the accumulation of context affects memory requirements and reduces the efficiency of long running dialogues. Addressing these challenges requires rethinking how chatbot interactions are designed and conceptualized, and adopting perspectives that recognize sustainability as partly dependent on the conversational norms through which users engage with LLM based systems.

</details>
