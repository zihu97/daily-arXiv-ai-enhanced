{"id": "2602.11362", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.11362", "abs": "https://arxiv.org/abs/2602.11362", "authors": ["Reginald Frank", "Soujanya Ponnapalli", "Octavio Lomeli", "Neil Giridharan", "Marcos K Aguilera", "Natacha Crooks"], "title": "Real Life Is Uncertain. Consensus Should Be Too!", "comment": "HotOS '25: Proceedings of the 2025 Workshop on Hot Topics in Operating Systems", "summary": "Modern distributed systems rely on consensus protocols to build a fault-tolerant-core upon which they can build applications. Consensus protocols are correct under a specific failure model, where up to $f$ machines can fail. We argue that this $f$-threshold failure model oversimplifies the real world and limits potential opportunities to optimize for cost or performance. We argue instead for a probabilistic failure model that captures the complex and nuanced nature of faults observed in practice. Probabilistic consensus protocols can explicitly leverage individual machine \\textit{failure curves} and explore side-stepping traditional bottlenecks such as majority quorum intersection, enabling systems that are more reliable, efficient, cost-effective, and sustainable.", "AI": {"tldr": "\u63d0\u51fa\u7528\u6982\u7387\u6545\u969c\u6a21\u578b\u66ff\u4ee3\u4f20\u7edff\u95e8\u9650\u6a21\u578b\uff0c\u4f18\u5316\u5171\u8bc6\u534f\u8bae\u7684\u53ef\u9760\u6027\u3001\u6548\u7387\u4e0e\u6210\u672c", "motivation": "\u4f20\u7edff\u95e8\u9650\u6545\u969c\u6a21\u578b\u8fc7\u5ea6\u7b80\u5316\u73b0\u5b9e\u6545\u969c\u573a\u666f\uff0c\u9650\u5236\u6210\u672c\u4e0e\u6027\u80fd\u4f18\u5316\u7a7a\u95f4", "method": "\u6784\u5efa\u6982\u7387\u6a21\u578b\u5229\u7528\u4e2a\u4f53\u673a\u5668\u6545\u969c\u66f2\u7ebf\uff0c\u89c4\u907f\u591a\u6570\u4ef2\u88c1\u4ea4\u96c6\u7b49 Hat\u4f20\u7edf\u74f6\u9888", "result": "\u5b9e\u73b0\u66f4\u9ad8\u53ef\u9760\u6027\u3001\u6548\u7387\u3001\u6210\u672c\u6548\u76ca\u548c\u53ef\u6301\u7eed\u6027\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf", "conclusion": "\u6982\u7387\u5171\u8bc6\u534f\u8bae\u80fd\u66f4\u7cbe\u51c6\u523b\u753b\u5b9e\u9645\u6545\u969c\u590d\u6742\u6027\uff0c\u89e3\u9501\u4f20\u7edf\u6a21\u578b\u65e0\u6cd5\u5b9e\u73b0\u7684\u6027\u80fd\u6f5c\u529b"}}
{"id": "2602.11544", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.11544", "abs": "https://arxiv.org/abs/2602.11544", "authors": ["Yiming Zhou", "Kaiping Xue", "Enhong Chen"], "title": "Differentially Private Perturbed Push-Sum Protocol and Its Application in Non-Convex Optimization", "comment": null, "summary": "In decentralized networks, nodes cannot ensure that their shared information will be securely preserved by their neighbors, making privacy vulnerable to inference by curious nodes. Adding calibrated random noise before communication to satisfy differential privacy offers a proven defense; however, most existing methods are tailored to specific downstream tasks and lack a general, protocol-level privacy-preserving solution. To bridge this gap, we propose Differentially Private Perturbed Push-Sum (DPPS), a lightweight differential privacy protocol for decentralized communication. Since protocol-level differential privacy introduces the unique challenge of obtaining the sensitivity for each communication round, DPPS introduces a novel sensitivity estimation mechanism that requires each node to compute and broadcast only one scalar per round, enabling rigorous differential privacy guarantees. This design allows DPPS to serve as a plug-and-play, low-cost privacy-preserving solution for downstream applications built on it. To provide a concrete instantiation of DPPS and better balance the privacy-utility trade-off, we design PartPSP, a privacy-preserving decentralized algorithm for non-convex optimization that integrates a partial communication mechanism. By partitioning model parameters into local and shared components and applying DPPS only to the shared parameters, PartPSP reduces the dimensionality of consensus data, thereby lowering the magnitude of injected noise and improving optimization performance. We theoretically prove that PartPSP converges under non-convex objectives and, with partial communication, achieves better optimization performance under the same privacy budget. Experimental results validate the effectiveness of DPPS's privacy-preserving and demonstrate that PartPSP outperforms existing privacy-preserving decentralized optimization algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDPPS\u534f\u8bae\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u901a\u4fe1\u4e2d\u8f7b\u578b\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\uff0c\u5e76\u8bbe\u8ba1PartPSP\u7b97\u6cd5\u4f18\u5316\u9690\u79c1-\u6548\u7528\u5e73\u8861\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u5728\u975e\u51f8\u76ee\u6807\u4e0b\u6536\u655b\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5fae\u5206\u9690\u79c1\u65b9\u6cd5\u591a\u4e3a\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u901a\u7528\u534f\u8bae\u7ea7\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\uff1b\u8fb9\u7f18\u8282\u70b9\u4fe1\u606f\u6613\u88ab\u597d\u5947\u8282\u70b9\u63a8\u6f14\uff0c\u5bfc\u81f4\u9690\u79c1\u6cc4\u6f0f\u3002", "method": "DPPS\u5f15\u5165\u654f\u611f\u5ea6\u4f30\u91cf\u673a\u5236\uff08\u6bcf\u8282\u70b9\u6bcf\u8f6e\u5e7f\u64ad\u4e00\u6807\u91cf\uff09\uff1bPartPSP\u5206\u5272\u6a21\u578b\u53c2\u6570\u4e3a\u672c\u5730\u4e0e\u5171\u4eab\u7ec4\u4ef6\uff0c\u4ec5\u5bf9\u5171\u4eab\u53c2\u6570\u5e94\u7528DPPS\uff0c\u964d\u4f4e\u566a\u58f0\u7ef4\u5ea6\u3002", "result": "DPPS\u63d0\u4f9b\u4e25\u683c\u5dee\u5206\u9690\u79c1\u4fdd\u969c\uff1bPartPSP\u7406\u8bba\u4e0a\u5728\u975e\u51f8\u76ee\u6807\u4e0b\u6536\u655b\uff0c\u76f8\u540c\u9690\u79c1\u9884\u7b97\u4e0b\u4f18\u5316\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\uff1b\u5b9e\u9a8c\u9a8c\u8bc1\u9690\u79c1\u4fdd\u62a4\u6709\u6548\u4e14\u7b97\u6cd5\u6027\u80fd\u4f18\u8d8a\u3002", "conclusion": "DPPS\u4f5c\u4e3a\u4f4e\u6210\u672c\u5373\u63d2\u5373\u7528\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\uff0cPartPSP\u901a\u8fc7\u90e8\u5206\u901a\u4fe1\u4f18\u5316\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u9002\u7528\u4e8e\u5404\u7c7b\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2602.11686", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11686", "abs": "https://arxiv.org/abs/2602.11686", "authors": ["Xinyi Liu", "Yujie Wang", "Fangcheng Fu", "Xuefeng Xiao", "Huixia Li", "Jiashi Li", "Bin Cui"], "title": "LAER-MoE: Load-Adaptive Expert Re-layout for Efficient Mixture-of-Experts Training", "comment": "19 pages, 12 figures, the paper will be presented at ASPLOS 2026", "summary": "Expert parallelism is vital for effectively training Mixture-of-Experts (MoE) models, enabling different devices to host distinct experts, with each device processing different input data. However, during expert parallel training, dynamic routing results in significant load imbalance among experts: a handful of overloaded experts hinder overall iteration, emerging as a training bottleneck.\n  In this paper, we introduce LAER-MoE, an efficient MoE training framework. The core of LAER-MoE is a novel parallel paradigm, Fully Sharded Expert Parallel (FSEP), which fully partitions each expert parameter by the number of devices and restores partial experts at expert granularity through All-to-All communication during training. This allows for flexible re-layout of expert parameters during training to enhance load balancing. In particular, we perform fine-grained scheduling of communication operations to minimize communication overhead. Additionally, we develop a load balancing planner to formulate re-layout strategies of experts and routing schemes for tokens during training. We perform experiments on an A100 cluster, and the results indicate that our system achieves up to 1.69x acceleration compared to the current state-of-the-art training systems. Source code available at https://github.com/PKU-DAIR/Hetu-Galvatron/tree/laer-moe.", "AI": {"tldr": "\u63d0\u51faLAER-MoE\u6846\u67b6\u89e3\u51b3\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u8bad\u7ec3\u4e2d\u56e0\u52a8\u6001\u8def\u7531\u5bfc\u81f4\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u6838\u5fc3\u662f\u901a\u8fc7\u5206\u7247\u4e13\u5bb6\u5e76\u884c\uff08FSEP\uff09\u548c\u8d1f\u8f7d\u8c03\u5ea6\u7b56\u7565\u5b9e\u73b0\u52a0\u901f\u8bad\u7ec3\u3002", "motivation": "\u4e13\u5bb6\u5e76\u884c\u8bad\u7ec3\u4e2d\u52a8\u6001\u8def\u7531\u5bfc\u81f4\u4e0d\u540c\u4e13\u5bb6\u95f4\u8d1f\u8f7d\u4e25\u91cd\u4e0d\u5747\u8861\uff0c\u8fc7\u8f7d\u4e13\u5bb6\u6210\u4e3a\u8bad\u7ec3\u74f6\u9888\uff0c\u964d\u4f4e\u4e86\u6574\u4f53\u8fed\u4ee3\u6548\u7387\u3002", "method": "\u91c7\u7528\u5b8c\u5168\u5206\u7247\u4e13\u5bb6\u5e76\u884c\uff08FSEP\uff09\u8303\u5f0f\uff1a1. \u5c06\u6bcf\u4e2a\u4e13\u5bb6\u53c2\u6570\u6309\u8bbe\u5907\u6570\u5b8c\u5168\u5206\u5272\uff1b2. \u8bad\u7ec3\u65f6\u901a\u8fc7All-to-All\u901a\u4fe1\u6309\u4e13\u5bb6\u7c92\u5ea6\u91cd\u7ec4\u53c2\u6570\uff1b3. \u7ec6\u7c92\u5ea6\u8c03\u5ea6\u901a\u4fe1\u64cd\u4f5c\u964d\u4f4e\u5f00\u9500\uff1b4. \u5f00\u53d1\u8d1f\u8f7d\u5747\u8861\u89c4\u5212\u5668\u52a8\u6001\u8c03\u6574\u4e13\u5bb6\u5e03\u5c40\u548c\u8def\u7531\u7b56\u7565\u3002", "result": "\u5728A100\u96c6\u7fa4\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u8bad\u7ec3\u7cfb\u7edf\u5b9e\u73b0\u4e86\u6700\u9ad81.69\u500d\u7684\u52a0\u901f\u3002", "conclusion": "LAER-MoE\u901a\u8fc7\u53c2\u6570\u91cd\u7ec4\u548c\u8d1f\u8f7d\u4f18\u5316\u6709\u6548\u63d0\u5347\u4e86MoE\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u4e13\u5bb6\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11357", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.11357", "abs": "https://arxiv.org/abs/2602.11357", "authors": ["Xiaoling Yi", "Ryan Antonio", "Yunhao Deng", "Fanchen Kong", "Joren Dumoulin", "Jun Yin", "Marian Verhelst"], "title": "A 16 nm 1.60TOPS/W High Utilization DNN Accelerator with 3D Spatial Data Reuse and Efficient Shared Memory Access", "comment": "Accepted at ISCAS 2026 (2026 IEEE International Symposium on Circuits and Systems)", "summary": "Achieving high compute utilization across a wide range of AI workloads is crucial for the efficiency of versatile DNN accelerators. This paper presents the Voltra chip and its utilization-optimised DNN accelerator architecture, which leverages 3-Dimensional (3D) spatial data reuse along with efficient and flexible shared memory access. The 3D spatial dataflow enables balanced spatial data reuse across three dimensions, improving spatial utilization by up to 2.0x compared to a conventional 2D design. Inside the shared memory access architecture, Voltra incorporates flexible data streamers that enable mixed-grained hardware data pre-fetching and dynamic memory allocation, further improving the temporal utilization by 2.12-2.94x and achieving 1.15-2.36x total latency speedup compared with the non-prefetching and separated memory architecture, respectively. Fabricated in 16nm technology, our chip achieves 1.60 TOPS/W peak system energy efficiency and 1.25 TOPS/mm2 system area efficiency, which is competitive with state-of-the-art solutions while achieving high utilization across diverse workloads.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVoltra\u82af\u7247\u53ca\u5176\u4f18\u5316DNN\u52a0\u901f\u5668\u67b6\u6784\uff0c\u91c7\u75283D\u7a7a\u95f4\u6570\u636e\u590d\u7528\u548c\u7075\u6d3b\u5185\u5b58\u8bbf\u95ee\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u80fd\u4e0e\u9ad8\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u901a\u7528DNN\u52a0\u901f\u5668\u5728\u591a\u6837\u5316AI\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u8ba1\u7b97\u5229\u7528\u7387\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u6548\u7387\u3002", "method": "\u7ed3\u5408\u4e09\u7ef4\u7a7a\u95f4\u6570\u636e\u6d41\u6280\u672f\u4f18\u5316\u6570\u636e\u590d\u7528\uff0c\u5e76\u5f15\u5165\u7075\u6d3b\u6570\u636e\u6d41\u5668\u5b9e\u73b0\u6df7\u5408\u7c92\u5ea6\u786c\u4ef6\u9884\u53d6\u548c\u52a8\u6001\u5185\u5b58\u5206\u914d\uff0c\u589e\u5f3a\u5171\u4eab\u5185\u5b58\u8bbf\u95ee\u6548\u7387\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf2D\u8bbe\u8ba1\u7a7a\u95f4\u5229\u7528\u7387\u63d0\u53472.0\u500d\uff1b\u9884\u53d6\u6280\u672f\u63d0\u9ad8\u65f6\u95f4\u5229\u7528\u73872.12-2.94\u500d\uff0c\u5ef6\u8fdf\u964d\u4f4e1.15-2.36\u500d\uff1b16nm\u82af\u7247\u5b9e\u73b01.60 TOPS/W\u80fd\u6548\u548c1.25 TOPS/mm\u00b2\u9762\u6548\uff0c\u6027\u80fd\u5ab2\u7f8e\u6700\u5148\u8fdb\u65b9\u6848\u3002", "conclusion": "Voltra\u82af\u7247\u5728\u9ad8\u8d1f\u8f7d\u9002\u5e94\u6027\u4e0b\u53d6\u5f97\u4e86\u9886\u5148\u7684\u80fd\u91cf\u548c\u9762\u79ef\u6548\u7387\uff0c\u4e3aDNN\u52a0\u901f\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11740", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.11740", "abs": "https://arxiv.org/abs/2602.11740", "authors": ["Ayhan Alp Aydeniz", "Robert Loftin", "Kagan Tumer"], "title": "Counterfactual Conditional Likelihood Rewards for Multiagent Exploration", "comment": "9 pages, 5 figures", "summary": "Efficient exploration is critical for multiagent systems to discover coordinated strategies, particularly in open-ended domains such as search and rescue or planetary surveying. However, when exploration is encouraged only at the individual agent level, it often leads to redundancy, as agents act without awareness of how their teammates are exploring. In this work, we introduce Counterfactual Conditional Likelihood (CCL) rewards, which score each agent's exploration by isolating its unique contribution to team exploration. Unlike prior methods that reward agents solely for the novelty of their individual observations, CCL emphasizes observations that are informative with respect to the joint exploration of the team. Experiments in continuous multiagent domains show that CCL rewards accelerate learning for domains with sparse team rewards, where most joint actions yield zero rewards, and are particularly effective in tasks that require tight coordination among agents.", "AI": {"tldr": "\u9488\u5bf9\u591a\u4eba\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4e2a\u4f53\u63a2\u7d22\u5bfc\u81f4\u5197\u4f59\u7684\u95ee\u9898\uff0c\u63d0\u51faCCL\u5956\u52b1\u673a\u5236\uff0c\u901a\u8fc7\u6d4b\u91cf\u4e2a\u4f53\u5bf9\u8054\u5408\u63a2\u7d22\u7684\u72ec\u7279\u8d21\u732e\uff0c\u63d0\u5347\u534f\u8c03\u6548\u7387\uff1b\u5728\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457\u3002", "motivation": "\u5f00\u653e\u578b\u9886\u57df\uff08\u5982\u641c\u7d22\u6551\u63f4\uff09\u4e2d\uff0c\u4e2a\u4f53\u63a2\u7d22\u5ffd\u89c6\u56e2\u961f\u534f\u4f5c\u5bfc\u81f4\u5197\u4f59\u884c\u52a8\uff0c\u59a8\u788d\u53d1\u73b0\u534f\u8c03\u7b56\u7565\u3002", "method": "\u5f15\u5165\u53cd\u4e8b\u5b9e\u6761\u4ef6\u4f3c\u7136\uff08CCL\uff09\u5956\u52b1\u673a\u5236\uff0c\u8bc4\u4ef7\u4e2a\u4f53\u89c2\u5bdf\u5bf9\u8054\u5408\u63a2\u7d22\u7684\u4fe1\u606f\u4ef7\u503c\uff0c\u800c\u975e\u4ec5\u8bc4\u4e2a\u4f53\u89c2\u5bdf\u65b0\u9896\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCCL\u5728\u7a00\u758f\u56e2\u961f\u5956\u52b1\u9886\u57df\u52a0\u901f\u5b66\u4e60\uff0c\u5bf9\u9700\u8981\u7d27\u5bc6\u534f\u4f5c\u7684\u4efb\u52a1\u6210\u6548\u7a81\u51fa\u3002", "conclusion": "CCL\u5956\u52b1\u901a\u8fc7\u5f3a\u8c03\u8054\u5408\u63a2\u7d22\uff0c\u6709\u6548\u4f18\u5316\u591a\u4eba\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u80fd\u529b\u3002"}}
{"id": "2602.11209", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.11209", "abs": "https://arxiv.org/abs/2602.11209", "authors": ["Ziyi Yang", "Kalit Inani", "Keshav Kabra", "Vima Gupta", "Anand Padmanabha Iyer"], "title": "SAFuzz: Semantic-Guided Adaptive Fuzzing for LLM-Generated Code", "comment": "11 pages, 6 figures, 4 tables", "summary": "While AI-coding assistants accelerate software development, current testing frameworks struggle to keep pace with the resulting volume of AI-generated code. Traditional fuzzing techniques often allocate resources uniformly and lack semantic awareness of algorithmic vulnerability patterns, leading to inefficient resource usage and missed vulnerabilities. To address these limitations, we present a hybrid testing framework that leverages LLM-guided adaptive fuzzing to detect algorithmic vulnerabilities efficiently. Our system SAFuzz integrates prompt-based behavioral diversification, harness generation with problem-specific oracles, and an LLM-based predictor to enable adaptive resource allocation and dynamic early stopping. Evaluating SAFuzz on CSES algorithmic problems, we improve vulnerability discrimination precision from 77.9% to 85.7% and achieve a 1.71x reduction in time cost compared to SOTA GreenFuzz while maintaining comparable recall. We further observe that combining our approach with existing unit test generation methods yields complementary gains, increasing the bug detection recall from 67.3% to 79.5%.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11476", "categories": ["cs.OS", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.11476", "abs": "https://arxiv.org/abs/2602.11476", "authors": ["R. Jay Martin"], "title": "Bounded Local Generator Classes for Deterministic State Evolution", "comment": "38 pages. Formal operator-class result", "summary": "We formalize a constructive subclass of locality-preserving deterministic operators acting on graph-indexed state systems. We define the class of Bounded Local Generator Classes (BLGC), consisting of finite-range generators operating on bounded state spaces under deterministic composition. Within this class, incremental update cost is independent of total system dimension. We prove that, under the BLGC assumptions, per-step operator work satisfies W_t = O(1) as the number of nodes M \\to \\infty, establishing a structural decoupling between global state size and incremental computational effort. The framework admits a Hilbert-space embedding in \\ell^2(V; \\mathbb{R}^d) and yields bounded operator norms on admissible subspaces. The result applies specifically to the defined subclass and does not claim universality beyond the stated locality and boundedness constraints.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11741", "categories": ["cs.DC", "cs.DB", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11741", "abs": "https://arxiv.org/abs/2602.11741", "authors": ["Bo Guan"], "title": "Designing Scalable Rate Limiting Systems: Algorithms, Architecture, and Distributed Solutions", "comment": "27 pages, 8 figures, 2 tables", "summary": "Designing a rate limiter that is simultaneously accurate, available, and scalable presents a fundamental challenge in distributed systems, primarily due to the trade-offs between algorithmic precision, availability, consistency, and partition tolerance. This article presents a concrete architecture for a distributed rate limiting system in a production-grade environment. Our design chooses the in-memory cache database, the Redis, along with its Sorted Set data structure, which provides $O(log (N))$ time complexity operation for the key-value pair dataset with efficiency and low latency, and maintains precision. The core contribution is quantifying the accuracy and memory cost trade-off of the chosen Rolling Window as the implemented rate limiting algorithm against the Token Bucket and Fixed Window algorithms. In addition, we explain how server-side Lua scripting is critical to bundling cleanup, counting, and insertion into a single atomic operation, thereby eliminating race conditions in concurrent environments. In the system architecture, we propose a three-layer architecture that manages the storage and updating of the limit rules. Through script load by hashing the rule parameters, rules can be changed without modifying the cached scripts. Furthermore, we analyze the deployment of this architecture on a Redis Cluster, which provides the availability and scalability by data sharding and replication. We explain the acceptance of AP (Availability and Partition Tolerance) from the CAP theorem as the pragmatic engineering trade-off for this use case.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u751f\u4ea7\u73af\u5883\u7684\u5206\u5e03\u5f0f\u9650\u901f\u7cfb\u7edf\u67b6\u6784\uff0c\u8212\u9002\u7cbe\u786e\u6027\u3001\u53ef\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u95f4\u7684\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u9650\u901f\u5668\u8bbe\u8ba1\u7684\u5173\u952e\u6311\u6218\u2014\u2014\u5728\u7b97\u6cd5\u7cbe\u5ea6\u3001\u53ef\u7528\u6027\u3001\u4e00\u81f4\u6027\u548c\u5206\u533a\u5bb9\u9519\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u57fa\u4e8eRedis Sorted Set\u5b9e\u73b0O(log N)\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u6eda\u52a8\u7a97\u53e3\u7b97\u6cd5\uff1b\u4f7f\u7528Lua\u811a\u672c\u786e\u4fdd\u539f\u5b50\u64cd\u4f5c\u6d88\u9664\u7ade\u4e89\u6761\u4ef6\uff1b\u8bbe\u8ba1\u4e09\u5c42\u89c4\u5219\u7ba1\u7406\u67b6\u6784\uff1b\u90e8\u7f72Redis\u96c6\u7fa4\u901a\u8fc7\u6570\u636e\u5206\u7247\u4e0e\u590d\u5236\u5b9e\u73b0\u6269\u5c55\u6027\uff1b\u91c7\u7528CAP\u7406\u8bba\u4e2d\u7684AP\u6743\u8861\u3002", "result": "\u91cf\u5316\u4e86\u6eda\u52a8\u7a97\u53e3\u7b97\u6cd5\u76f8\u5bf9\u4ee4\u724c\u6876\u548c\u56fa\u5b9a\u7a97\u53e3\u7684\u7cbe\u786e\u5ea6\u4e0e\u5185\u5b58\u6d88\u8017\u7684\u6743\u8861\uff1b\u6784\u5efa\u51fa\u652f\u6301\u52a8\u6001\u89c4\u5219 suszt", "conclusion": "\u8be5\u67b6\u6784\u6210\u529f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u53ef\u7528\u7684\u5206\u5e03\u5f0f\u9650\u901f\u5668\uff0c\u4ee5\u5de5\u7a0b\u5b9e\u7528\u4e3b\u4e49\u5316\u89e3CAP\u9650\u5236\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11210", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11210", "abs": "https://arxiv.org/abs/2602.11210", "authors": ["Danlong Yuan", "Wei Wu", "Zhengren Wang", "Xueliang Zhao", "Huishuai Zhang", "Dongyan Zhao"], "title": "SWE-MiniSandbox: Container-Free Reinforcement Learning for Building Software Engineering Agents", "comment": "ICML under review", "summary": "Reinforcement learning (RL) has become a key paradigm for training software engineering (SWE) agents, but existing pipelines typically rely on per-task containers for isolation. At scale, pre-built container images incur substantial storage overhead, slow environment setup, and require container-management privileges. We propose SWE-MiniSandbox, a lightweight, container-free method that enables scalable RL training of SWE agents without sacrificing isolation. Instead of relying on per-instance containers, SWE-MiniSandbox executes each task in an isolated workspace backed by kernel-level mechanisms, substantially reducing system overhead. It leverages lightweight environment pre-caching techniques to eliminate the need for bulky container images. As a result, our approach lowers disk usage to approximately 5\\% of that required by container-based pipelines and reduces environment preparation time to about 25\\% of the container baseline. Empirical results demonstrate that SWE-MiniSandbox achieves evaluation performance comparable to standard container-based pipelines. By removing the dependency on heavy container infrastructure, SWE-MiniSandbox offers a practical and accessible foundation for scaling RL-based SWE agents, particularly in resource-constrained research environments.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11998", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.11998", "abs": "https://arxiv.org/abs/2602.11998", "authors": ["Ramakant kumar"], "title": "An Auction-Based Mechanism for Optimal Task Allocation and Resource Aware Containerization", "comment": null, "summary": "Distributed computing has enabled cooperation between multiple computing devices for the simultaneous execution of resource-hungry tasks. Such execution also plays a pivotal role in the parallel execution of numerous tasks in the Internet of Things (IoT) environment. Leveraging the computing resources of multiple devices, the offloading and processing of computationintensive tasks can be carried out more efficiently. However, managing resources and optimizing costs remain challenging for successfully executing tasks in cloud-based containerization for IoT. This paper proposes AUC-RAC, an auction-based mechanism for efficient offloading of computation tasks among multiple local servers in the context of IoT devices. The approach leverages the concept of Docker swarm, which connects multiple local servers in the form of Manager Node (MN) and Worker Nodes (WNs). It uses Docker containerization to execute tasks simultaneously. In this system, IoT devices send tasks to the MN, which then sends the task details to all its WNs to participate in the auction-based bidding process. The auctionbased bidding process optimizes the allocation of computation tasks among multiple systems, considering their resource sufficiency. The experimental analysis establishes that the approach offers improved offloading and computation-intensive services for IoT devices by enabling cooperation between local servers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAUC-RAC\u62cd\u5356\u673a\u5236,\u7528\u4e8e\u7269\u8054\u7f51\u73af\u5883\u4e0b\u4f18\u5316\u8ba1\u7b97\u4efb\u52a1\u5378\u8f7d\u5230\u591a\u4e2a\u672c\u5730\u670d\u52a1\u5668\u7684\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u7269\u8054\u7f51\u8ba1\u7b97\u4efb\u52a1\u6267\u884c\u4e2d\u5b58\u5728\u7684\u8d44\u6e90\u7ba1\u7406\u56f0\u96be\u548c\u4f18\u5316\u6210\u672c\u6311\u6218\u3002", "method": "\u5229\u7528Docker\u96c6\u7fa4(\u7ba1\u7406\u8282\u70b9MN\u548c\u5de5\u4f5c\u8282\u70b9WNs)\u8fdb\u884c\u62cd\u5356\u7ade\u4ef7;\u8bbe\u5907\u53d1\u9001\u4efb\u52a1\u81f3MN,\u4efb\u52a1\u7ec6\u8282\u5e7f\u64ad\u81f3WNs\u540e\u6839\u636e\u8d44\u6e90\u5145\u8db3\u6027\u4ee5\u62cd\u5356\u5206\u914d\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u901a\u8fc7\u670d\u52a1\u5668\u5408\u4f5c\u63d0\u9ad8\u4e86\u5378\u8f7d\u670d\u52a1\u548c\u5bc6\u96c6\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "AUC-RAC\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u7269\u8054\u7f51\u8bbe\u5907\u7684\u4efb\u52a1\u6267\u884c\u6027\u80fd\u4e0e\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2602.11521", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.11521", "abs": "https://arxiv.org/abs/2602.11521", "authors": ["Lian Liu", "Shixin Zhao", "Yutian Zhou", "Yintao He", "Mengdi Wang", "Yinhe Han", "Ying Wang"], "title": "PAM: Processing Across Memory Hierarchy for Efficient KV-centric LLM Serving System", "comment": "15 pages, 13 figures", "summary": "The widespread adoption of Large Language Models (LLMs) has exponentially increased the demand for efficient serving systems. With growing requests and context lengths, key-value (KV)-related operations, including attention computation and KV cache storage, have emerged as critical bottlenecks. They require massive memory bandwidth and capacity. Unfortunately, existing LLM serving systems, optimized for compute-bound workloads, fail to handle these memory-intensive operations effectively. Even with Processing-In-Memory (PIM) technology, current single-level memory designs cannot simultaneously satisfy the bandwidth and capacity requirements.\n  To address these challenges, we propose Processing Across Memory (PAM), a KV-centric LLM serving system that coordinates heterogeneous PIM-enabled memory devices within a hierarchical architecture. PAM introduces a novel computing paradigm to balance high memory bandwidth with scalable capacity. First, PAM exploits the inherent context locality in KV access patterns to intelligently distribute KV tokens across the memory hierarchy. Second, to further exploit context locality, it introduces the PAMattention algorithm, enabling fine-grained parallel attention computation across heterogeneous PIM devices. Finally, PAM incorporates an intra-device KV mapping, inter-device KV migration interface, and an inter-device online KV scheduling algorithm to dynamically balance computational workloads. By addressing both bandwidth and capacity demands simultaneously, PAM significantly enhances the efficiency and scalability of LLM serving systems, paving the way for cost-effective, high-performance solutions in the era of large-scale AI.", "AI": {"tldr": "PAM\u63d0\u51fa\u5206\u5c42\u5185\u5b58\u67b6\u6784\uff0c\u5229\u7528\u5f02\u6784PIM\u8bbe\u5907\u89e3\u51b3LLM\u670d\u52a1\u4e2d\u7684KV\u64cd\u4f5c\u5185\u5b58\u74f6\u9888\uff0c\u901a\u8fc7\u6539\u8fdb\u6ce8\u610f\u529b\u8ba1\u7b97\u548c\u52a8\u6001\u8c03\u5ea6\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u968f\u7740LLM\u8bf7\u6c42\u91cf\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u957f\uff0cKV\u64cd\u4f5c\u6210\u4e3a\u5185\u5b58\u5e26\u5bbd\u548c\u5bb9\u91cf\u7684\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u57fa\u4e8ePIM\u7684\u5355\u5c42\u5185\u5b58\u8bbe\u8ba1\u65e0\u6cd5\u540c\u65f6\u9ad8\u4e09\u5e26\u5bbd\u548c\u5927\u5bb9\u91cf\u9700\u6c42\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6027\u80fd\u53d7\u9650\u3002", "method": "\u91c7\u7528\u5f02\u6784PIM\u8bbe\u5907\u7684\u5206\u5c42\u67b6\u6784\uff1a\u5148\u5229\u7528KV\u8bbf\u95ee\u7684\u4e0a\u4e0b\u6587\u5c40\u90e8\u6027\u5206\u5e03\u4ee4\u724c\uff1b\u521b\u65b0PAMattention\u7b97\u6cd5\u5b9e\u73b0\u8de8\u8bbe\u5907\u5e76\u884c\u6ce8\u610f\u529b\u8ba1\u7b97\uff1b\u5e76\u901a\u8fc7KV\u6620\u5c04\u3001\u8fc1\u79fb\u63a5\u53e3\u548c\u5728\u7ebf\u8c03\u5ea6\u52a8\u6001\u5e73\u8861\u8d1f\u8f7d\u3002", "result": "PAM\u540c\u6b65\u6ee1\u8db3\u5e26\u5bbd\u4e0e\u5bb9\u91cf\u9700\u6c42\uff0c\u663e\u8457\u63d0\u9ad8LLM\u670d\u52a1\u6548\u7387\u53ca\u53ef\u6269\u5c55\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21AI\u65f6\u4ee3\u63d0\u4f9b\u9ad8\u6027\u80fd\u3001\u4f4e\u6210\u672c\u7684LLM\u670d\u52a1\u89e3\u51b3\u65b9\u6848\uff0c\u5960\u5b9a\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2602.11977", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.11977", "abs": "https://arxiv.org/abs/2602.11977", "authors": ["Michael Otte", "Roderich Gro\u00df"], "title": "Multi-Defender Single-Attacker Perimeter Defense Game on a Cylinder: Special Case in which the Attacker Starts at the Boundary", "comment": "4 pages, 3 figures", "summary": "We describe a multi-agent perimeter defense game played on a cylinder. A team of n slow-moving defenders must prevent a single fast-moving attacker from crossing the boundary of a defensive perimeter. We describe the conditions necessary for the attacker to win in the special case that the intruder starts close to the boundary and in a region that is currently defended.", "AI": {"tldr": "\u7814\u7a76\u5706\u67f1\u6218\u573a\u4e0a\u7531n\u4e2a\u6162\u901f\u9632\u5fa1\u8005\u5bf9\u6297\u5355\u4e2a\u5feb\u901f\u653b\u51fb\u8005\u7684\u8fb9\u754c\u9632\u5fa1\u6e38\u620f\uff0c\u91cd\u70b9\u5206\u6790\u653b\u51fb\u8005\u8d77\u59cb\u4e8e\u8fb9\u754c\u9644\u8fd1\u4e14\u9632\u5fa1\u533a\u57df\u5185\u65f6\u7684\u83b7\u80dc\u6761\u4ef6\u3002", "motivation": "\u63a2\u7d22\u591a\u667a\u80fd\u4f53\u9632\u5fa1\u7cfb\u7edf\u4e2d\u8fb9\u754c\u9632\u62a4\u7684\u52a8\u6001\u673a\u5236\uff0c\u8bc6\u522b\u5f53\u5165\u4fb5\u8005\u5177\u5907\u4f4d\u7f6e\u4f18\u52bf\u65f6\u7cfb\u7edf\u6f5c\u5728\u7684\u8106\u5f31\u6027\u3002", "method": "\u6784\u5efa\u5706\u67f1\u7a7a\u95f4\u9632\u5fa1\u6a21\u578b\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63a8\u5bfc\u653b\u51fb\u8005\u83b7\u80dc\u7684\u6570\u5b66\u6761\u4ef6\uff0c\u91cd\u70b9\u5173\u6ce8\u521d\u59cb\u4f4d\u7f6e\u4f4d\u4e8e\u8fb9\u754c\u9644\u8fd1\u4e14\u9632\u5fa1\u8986\u76d6\u533a\u57df\u7684\u60c5\u51b5\u3002", "result": "\u786e\u5b9a\u4e86\u653b\u51fb\u8005\u80fd\u591f\u7a81\u7834\u9632\u7ebf\u6240\u9700\u7684\u5177\u4f53\u6761\u4ef6\uff0c\u63ed\u793a\u4e86\u5728\u7279\u5b9a\u521d\u59cb\u914d\u7f6e\u4e0b\u5feb\u901f\u653b\u51fb\u8005\u53ef\u80fd\u6218\u80dc\u6162\u901f\u9632\u5fa1\u56e2\u961f\u7684\u89c4\u5f8b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u591a\u667a\u80fd\u4f53\u9632\u5fa1\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u51f8\u663e\u8fb9\u754c\u9632\u62a4\u4e2d\u521d\u59cb\u5e03\u9632\u7b56\u7565\u5bf9\u7cfb\u7edf\u9c81\u68d2\u6027\u7684\u5173\u952e\u5f71\u54cd\u3002"}}
{"id": "2602.11223", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.11223", "abs": "https://arxiv.org/abs/2602.11223", "authors": ["Micheal P. Papazoglou", "Bernd J. Kr\u00e4mer", "Mira Raheem", "Amal Elgammal"], "title": "Patient Digital Twins for Chronic Care: Technical Hurdles, Lessons Learned, and the Road Ahead", "comment": "Feature Article, Patient Medical Digital Twins, Under Review in IEEE SOftware", "summary": "Chronic diseases constitute the principal burden of morbidity, mortality, and healthcare costs worldwide, yet current health systems remain fragmented and predominantly reactive. Patient Medical Digital Twins (PMDTs) offer a paradigm shift: holistic, continuously updated digital counterparts of patients that integrate clinical, genomic, lifestyle, and quality-of-life data. We report early implementations of PMDTs via ontology-driven modeling and federated analytics pilots. Insights from the QUALITOP oncology study and a distributed AI platform confirm both feasibility and challenges: aligning with HL7 FHIR and OMOP standards, embedding privacy governance, scaling federated queries, and designing intuitive clinician interfaces. We also highlight technical gains, such as automated reasoning over multimodal blueprints and predictive analytics for patient outcomes. By reflecting on these experiences, we outline actionable insights for software engineers and identify opportunities, such as DSLs and model-driven engineering, to advance PMDTs toward trustworthy, adaptive chronic care ecosystems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12246", "categories": ["cs.NI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.12246", "abs": "https://arxiv.org/abs/2602.12246", "authors": ["Mona Ghassemian", "Andr\u00e9s Meseguer Valenzuela", "Ana Garcia Armada", "Dejan Vukobratovic", "Periklis Chatzimisios", "Kaspar Althoefer", "Ranga Rao Venkatesha Prasad"], "title": "6G Empowering Future Robotics: A Vision for Next-Generation Autonomous Systems", "comment": "7 pages, 3 figures, 2 tables, submitted to IEEE magazine publication", "summary": "The convergence of robotics and next-generation communication is a critical driver of technological advancement. As the world transitions from 5G to 6G, the foundational capabilities of wireless networks are evolving to support increasingly complex and autonomous robotic systems. This paper examines the transformative impact of 6G on enhancing key robotics functionalities. It provides a systematic mapping of IMT-2030 key performance indicators to robotic functional blocks including sensing, perception, cognition, actuation and self-learning. Building upon this mapping, we propose a high-level architectural framework integrating robotic, intelligent, and network service planes, underscoring the need for a holistic approach. As an example use case, we present a real-time, dynamic safety framework enabled by IMT-2030 capabilities for safe and efficient human-robot collaboration in shared spaces.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12070", "categories": ["cs.DC", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.12070", "abs": "https://arxiv.org/abs/2602.12070", "authors": ["Zixi Cai", "Kuowen Chen", "Shengquan Du", "Tsvi Kopelowitz", "Seth Pettie", "Ben Plosk"], "title": "Contention Resolution, With and Without a Global Clock", "comment": null, "summary": "In the Contention Resolution problem $n$ parties each wish to have exclusive use of a shared resource for one unit of time. The problem has been studied since the early 1970s, under a variety of assumptions on feedback given to the parties, how the parties wake up, knowledge of $n$, and so on. The most consistent assumption is that parties do not have access to a global clock, only their local time since wake-up. This is surprising because the assumption of a global clock is both technologically realistic and algorithmically interesting. It enriches the problem, and opens the door to entirely new techniques. Our primary results are: [1] We design a new Contention Resolution protocol that guarantees latency $$O\\left(\\left(n\\log\\log n\\log^{(3)} n\\log^{(4)} n\\cdots \\log^{(\\log^* n)} n\\right)\\cdot 2^{\\log^* n}\\right) \\le n(\\log\\log n)^{1+o(1)}$$ in expectation and with high probability. This already establishes at least a roughly $\\log n$ complexity gap between randomized protocols in GlobalClock and LocalClock. [2] Prior analyses of randomized ContentionResolution protocols in LocalClock guaranteed a certain latency with high probability, i.e., with probability $1-1/\\text{poly}(n)$. We observe that it is just as natural to measure expected latency, and prove a $\\log n$-factor complexity gap between the two objectives for memoryless protocols. The In-Expectation complexity is $\u0398(n \\log n/\\log\\log n)$ whereas the With-High-Probability latency is $\u0398(n\\log^2 n/\\log\\log n)$. Three of these four upper and lower bounds are new. [3] Given the complexity separation above, one would naturally want a ContentionResolution protocol that is optimal under both the In-Expectation and With-High-Probability metrics. This is impossible! It is even impossible to achieve In-Expectation latency $o(n\\log^2 n/(\\log\\log n)^2)$ and With-High-Probability latency $n\\log^{O(1)} n$ simultaneously.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11580", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.11580", "abs": "https://arxiv.org/abs/2602.11580", "authors": ["Hao Zhen", "Qingxuan Kang", "Yungang Bao", "Trevor E. Carlson"], "title": "Benchmarking for Single Feature Attribution with Microarchitecture Cliffs", "comment": "12 pages, 14 figures, 4 tables", "summary": "Architectural simulators play a critical role in early microarchitectural exploration due to their flexibility and high productivity. However, their effectiveness is often constrained by fidelity: simulators may deviate from the behavior of the final RTL, leading to unreliable performance estimates. Consequently, model calibration, which aligns simulator behavior with the RTL as the ground-truth microarchitecture, becomes essential for achieving accurate performance modeling.\n  To facilitate model calibration accuracy, we propose Microarchitecture Cliffs, a benchmark generation methodology designed to expose mismatches in microarchitectural behavior between the simulator and RTL. After identifying the key architectural components that require calibration, the Cliff methodology enables precise attribution of microarchitectural differences to a single microarchitectural feature through a set of benchmarks. In addition, we develop a set of automated tools to improve the efficiency of the Cliff workflow.\n  We apply the Cliff methodology to calibrate the XiangShan version of gem5 (XS-GEM5) against the XiangShan open-source CPU (XS-RTL). We reduce the performance error of XS-GEM5 from 59.2% to just 1.4% on the Cliff benchmarks. Meanwhile, the calibration guided by Cliffs effectively reduces the relative error of a representative tightly coupled microarchitectural feature by 48.03%. It also substantially lowers the absolute performance error, with reductions of 15.1% and 21.0% on SPECint2017 and SPECfp2017, respectively.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12102", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.12102", "abs": "https://arxiv.org/abs/2602.12102", "authors": ["Zhijian Gao", "Shuxin Li", "Bo An"], "title": "DEpiABS: Differentiable Epidemic Agent-Based Simulator", "comment": "17 pages, 9 figures, to be published in AAMAS 2026", "summary": "The COVID-19 pandemic highlighted the limitations of existing epidemic simulation tools. These tools provide information that guides non-pharmaceutical interventions (NPIs), yet many struggle to capture complex dynamics while remaining computationally practical and interpretable. We introduce DEpiABS, a scalable, differentiable agent-based model (DABM) that balances mechanistic detail, computational efficiency and interpretability. DEpiABS captures individual-level heterogeneity in health status, behaviour, and resource constraints, while also modelling epidemic processes like viral mutation and reinfection dynamics. The model is fully differentiable, enabling fast simulation and gradient-based parameter calibration. Building on this foundation, we introduce a z-score-based scaling method that maps small-scale simulations to any real-world population sizes with negligible loss in output granularity, reducing the computational burden when modelling large populations. We validate DEpiABS through sensitivity analysis and calibration to COVID-19 and flu data from ten regions of varying scales. Compared to the baseline, DEpiABS is more detailed, fully interpretable, and has reduced the average normal deviation in forecasting from 0.97 to 0.92 on COVID-19 mortality data and from 0.41 to 0.32 on influenza-like-illness data. Critically, these improvements are achieved without relying on auxiliary data, making DEpiABS a reliable, generalisable, and data-efficient framework for future epidemic response modelling.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11224", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11224", "abs": "https://arxiv.org/abs/2602.11224", "authors": ["Hubert M. Pysklo", "Artem Zhuravel", "Patrick D. Watson"], "title": "Agent-Diff: Benchmarking LLM Agents on Enterprise API Tasks via Code Execution with State-Diff-Based Evaluation", "comment": "Pre-Print. Under review for KDD 2026", "summary": "We present Agent-Diff, a novel benchmarking framework for evaluating agentic Large Language Models (LLMs) on real-world tasks that execute code via external APIs. Agentic LLM performance varies due to differences in models, external tool access, prompt structures, and agentic frameworks. Benchmarks must make fundamental trade-offs between a sandboxed approach that controls for variation in software environments and more ecologically valid approaches employing real services. Agent-Diff attempts to capture the desirable features of both of these approaches by including access to the real API interfaces for software services while sandboxing the environment in which calls are made, processed, and evaluated. This approach relies on two key innovations. The first is a novel state-diff contract, which separates process from outcome - rather than fuzzy trace or parameter matching, we define task success as whether the expected change in environment state was achieved. The second is a novel sandbox that provides a standardized scripting layer that all models use to execute code against external APIs (Slack, Box, Linear, Google Calendar). Thus, we can evaluate different agentic LLMs against a standardized set of contracts using a unified sandbox while still evaluating their performance on real-world service interfaces. Using the Agent-Diff framework, we provide benchmarks for nine LLMs across 224 tasks utilizing enterprise software workflows. In addition, we evaluate the robustness of the framework with ablation experiments to assess the contribution of access to API documentation on benchmark performance. Code and data: https://github.com/agent-diff-bench/agent-diff.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12151", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.12151", "abs": "https://arxiv.org/abs/2602.12151", "authors": ["Youhe Jiang", "Fangcheng Fu", "Taiyi Wang", "Guoliang He", "Eiko Yoneki"], "title": "OServe: Accelerating LLM Serving via Spatial-Temporal Workload Orchestration", "comment": null, "summary": "Serving Large Language Models (LLMs) can benefit immensely from parallelizing both the model and input requests across multiple devices, but incoming workloads exhibit substantial spatial and temporal heterogeneity. Spatially, workloads comprise heterogeneous requests with varying compute and memory demands. Temporally, workload composition varies over time. Nevertheless, existing systems typically assume spatially uniform and temporally stable workloads, employing a homogeneous, static model deployment. This mismatch between the assumption and real-world spatial-temporal heterogeneity results in suboptimal performance. We present OServe, an LLM serving system with heterogeneous and flexible model deployment that addresses both spatial and temporal heterogeneity. First, OServe introduces a novel workload-aware scheduling algorithm that optimizes heterogeneous model deployments according to real-time workload characteristics. Second, OServe proposes an efficient workload-adaptive switching method that migrates model deployments in response to predicted workload changes. Experiments on real-world traces show that OServe improves performance by up to 2$\\times$ (average: 1.5$\\times$) compared to state-of-the-art serving systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11614", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.11614", "abs": "https://arxiv.org/abs/2602.11614", "authors": ["Yousuf Choudhary", "Tosiron Adegbija"], "title": "Device-Circuit Co-Design of Variation-Resilient Read and Write Drivers for Antiferromagnetic Tunnel Junction (AFMTJ) Memories", "comment": "International VLSI Symposium on Technology, Systems and Applications (VLSI-TSA) 2026", "summary": "Antiferromagnetic Tunnel Junctions (AFMTJs) offer picosecond switching and high integration density for in-memory computing, but their ultrafast dynamics and low tunnel magnetoresistance (TMR) make state-of-the-art MRAM interfaces unreliable. This work develops a device-circuit co-designed read/write interface optimized for AFMTJ behavior. Using a calibrated SPICE AFMTJ model as a baseline, we identify the limitations of conventional drivers and propose an asymmetric pulse driver (PD) for deterministic picosecond switching and a self-timed sense amplifier (STSA) with dynamic trip-point tuning for low-TMR sensing. Our experiments using SPICE and Monte Carlo evaluations demonstrate that the proposed circuits preserve AFMTJ latency and energy benefits while achieving robust read/write yield under realistic PVT and 3D integration parasitics, outperforming standard MRAM front-ends under the same conditions.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u9488\u5bf9AFMTJ\uff08\u53cd\u94c1\u78c1\u96a7\u9053\u7ed3\uff09\u884c\u4e3a\u7684\u8bbe\u5907\u7535\u8def\u534f\u540c\u8bbe\u8ba1\u8bfb\u5199\u63a5\u53e3\uff0c\u89e3\u51b3\u4e86\u5176\u8d85\u5feb\u52a8\u6001\u548c\u4f4e\u96a7\u7a7f\u78c1\u963b\uff08TMR\uff09\u5bfc\u81f4\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "AFMTJ\u5177\u5907\u76ae\u79d2\u7ea7\u5207\u6362\u548c\u9ad8\u96c6\u6210\u5bc6\u5ea6\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u5185\u5b58\u8ba1\u7b97\uff0c\u4f46\u5176\u8d85\u5feb\u52a8\u6001\u548c\u4f4eTMR\u4f7f\u73b0\u6709MRAM\u63a5\u53e3\u4e0d\u53ef\u9760\uff0c\u9700\u4f18\u5316\u63a5\u53e3\u4ee5\u786e\u4fdd\u6027\u80fd\u3002", "method": "\u4ee5\u6821\u51c6SPICE AFMTJ\u6a21\u578b\u4e3a\u57fa\u7840\uff0c\u8bc6\u522b\u5e38\u89c4\u9a71\u52a8\u5668\u9650\u5236\uff1b\u8bbe\u8ba1\u975e\u5bf9\u79f0\u8109\u51b2\u9a71\u52a8\u5668\u5b9e\u73b0\u786e\u5b9a\u6027\u76ae\u79d2\u5207\u6362\uff0c\u5e76\u4f7f\u7528\u5e26\u52a8\u6001\u89e6\u53d1\u70b9\u8c03\u6574\u7684\u81ea\u5b9a\u65f6\u611f\u5e94\u653e\u5927\u5668\u4f18\u5316\u4f4eTMR\u4f20\u611f\u3002", "result": "SPICE\u548c\u8499\u7279\u5361\u6d1b\u5b9e\u9a8c\u663e\u793a\uff0c\u6240\u63d0\u8bae\u7535\u8def\u5728\u4fdd\u6301AFMTJ\u5ef6\u8fdf\u4e0e\u80fd\u91cf\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u5728\u5b9e\u9645PVT\u53d8\u5f02\u548c3D\u5bc4\u751f\u6548\u5e94\u4e0b\u5b9e\u73b0\u53ef\u9760\u8bfb\u5199\u6210\u54c1\u7387\uff0c\u4f18\u4e8e\u6807\u51c6MRAM\u524d\u7aef\u3002", "conclusion": "\u8be5\u8bbe\u5907\u7535\u8def\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u514b\u670dAFMTJ\u6311\u6218\uff0c\u63d0\u5347\u5185\u5b58\u8ba1\u7b97\u7cfb\u7edf\u53ef\u9760\u6027\uff0c identifies the limitations of conventional drivers and propose an asymmetric pulse driver (PD) for deterministic picosecond switching and a self-timed sense amplifier (STSA) with dynamic trip-point tuning for low-TMR sensing. \u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.11411", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11411", "abs": "https://arxiv.org/abs/2602.11411", "authors": ["Yang Liu", "Armstrong Foundjem", "Xingfang Wu", "Heng Li", "Foutse Khomh"], "title": "Improving the Robustness of Large Language Models for Code Tasks via Fine-tuning with Perturbed Data", "comment": null, "summary": "Context: In the fast-paced evolution of software development, Large Language Models (LLMs) have become indispensable tools for tasks such as code generation, completion, analysis, and bug fixing. Ensuring the robustness of these models against potential vulnerabilities from handling diverse inputs is critical, as variations in input can lead to incorrect or insecure code outputs.\n  Objective: This work aims to improve the robustness of LLMs for coding-related tasks against potential adversarial inputs. Specifically, we investigate how fine-tuning LLMs with perturbed datasets impacts their robustness against input perturbations.\n  Method: We systematically evaluated LLM robustness by fine-tuning models using datasets perturbed at character-level, word-level, and sentence-level, comparing results against base models and models fine-tuned on unperturbed datasets.\n  Results: Fine-tuning LLMs with perturbed datasets significantly improves model robustness (RD usually drops around 4\\% - 6\\%), especially for models with relatively weak robustness. However, this fine-tuning process typically results in a slight performance decrease (pass@1 usually drops around 1\\% - 3\\%) compared to fine-tuning with unperturbed datasets, although occasional performance improvements are observed.\n  Conclusion \\& Implications: Fine-tuning LLMs for coding tasks with perturbed data effectively enhances their robustness at the cost of a minor performance reduction, emphasizing the importance of balancing the robustness and performance of LLMs for coding applications.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6270\u52a8\u6570\u636e\u96c6\u5fae\u8c03\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u5065\u58ee\u6027\uff0c\u663e\u8457\u6539\u5584\u6297\u6270\u52a8\u80fd\u529b\u4f46\u4f34\u96a8\u8f7b\u5fae\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u5728\u5feb\u901f\u6f14\u8fdb\u7684\u8f6f\u4ef6\u5f00\u53d1\u4e2d\uff0c\u9700\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5bf9\u6297\u6027\u8f93\u5165\u7684\u5065\u58ee\u6027\uff0c\u9632\u6b62\u8f93\u5165\u53d8\u5f02\u5bfc\u81f4\u4e0d\u5b89\u5168\u6216\u4e0d\u6b63\u786e\u4ee3\u7801\u8f93\u51fa\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u901a\u8fc7\u5b57\u7b26\u7ea7\u3001\u8bcd\u7ea7\u548c\u53e5\u7ea7\u6270\u52a8\u6570\u636e\u96c6\u5fae\u8c03\u7684\u6a21\u578b\uff0c\u5bf9\u6bd4\u57fa\u51c6\u6a21\u578b\u548c\u65e0\u6270\u52a8\u5fae arguing\u6a21\u578b\u7684\u6548\u679c\u3002", "result": "\u6270\u52a8\u5fae\u8c03\u663e\u8457\u589e\u5f3a\u5065\u58ee\u6027\uff08RD\u901a\u5e38\u964d\u4f4e4%-6%\uff09\uff0c\u5c24\u5176\u5bf9\u5f31\u6a21\u578b\u6709\u6548\uff0c\u4f46\u6027\u80fd\u7565\u964d1%-3%\uff0c\u5076\u6709\u63d0\u5347\u3002", "conclusion": "\u6270\u52a8\u6570\u636e\u5fae\u8c03\u6709\u6548\u63d0\u5347\u5065\u58ee\u6027\u5374\u727a\u7272\u5c11\u8bb8\u6027\u80fd\uff0c\u5f3a\u8c03\u7f16\u7801\u5e94\u7528\u4e2d\u9700\u5e73\u8861\u4e8c\u8005\u5173\u7cfb\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.11966", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.11966", "abs": "https://arxiv.org/abs/2602.11966", "authors": ["Jiahong Bi", "Lars Sch\u00fctze", "Jeronimo Castrillon"], "title": "MING: An Automated CNN-to-Edge MLIR HLS framework", "comment": null, "summary": "Driven by the increasing demand for low-latency and real-time processing, machine learning applications are steadily migrating toward edge computing platforms, where Field-Programmable Gate Arrays (FPGAs) are widely adopted for their energy efficiency compared to CPUs and GPUs. To generate high-performance and low-power FPGA designs, several frameworks built upon High Level Synthesis (HLS) vendor tools have been proposed, among which MLIR-based frameworks are gaining significant traction due to their extensibility and ease of use. However, existing state-of-the-art frameworks often overlook the stringent resource constraints of edge devices. To address this limitation, we propose MING, an Multi-Level Intermediate Representation (MLIR)-based framework that abstracts and automates the HLS design process. Within this framework, we adopt a streaming architecture with carefully managed buffers, specifically designed to handle resource constraints while ensuring low-latency. In comparison with recent frameworks, our approach achieves on average 15x speedup for standard Convolutional Neural Network (CNN) kernels with up to four layers, and up to 200x for single-layer kernels. For kernels with larger input sizes, MING is capable of generating efficient designs that respect hardware resource constraints, whereas state-of-the-art frameworks struggle to meet.", "AI": {"tldr": "\u63d0\u51faMING\u6846\u67b6\uff1a\u57fa\u4e8eMLIR\u7684FPGA\u786c\u4ef6\u7efc\u5408\uff08HLS\uff09\u89e3\u51b3\u65b9\u6848\uff0c\u9488\u5bf9\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u9650\u5236\u8bbe\u8ba1\u6d41\u5f0f\u67b6\u6784\u4e0e\u7ba1\u7406\u7f13\u51b2\u533a\uff0c\u663e\u8457\u63d0\u5347CNN\u5185\u6838\u6027\u80fd\u3002", "motivation": "\u73b0\u6709FPGA HLS\u6846\u67b6\u5ffd\u89c6\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u7ea6\u675f\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u4e0e\u5b9e\u65f6\u5904\u7406\u9700\u6c42\uff0c\u9700\u5f00\u53d1\u517c\u987e\u6548\u80fd\u4e0e\u8d44\u6e90\u9650\u5236\u7684\u65b9\u6848\u3002", "method": "\u6784\u5efaMLIR\u6846\u67b6\u5b9e\u73b0HLS\u8bbe\u8ba1\u81ea\u52a8\u5316\uff0c\u91c7\u7528\u6d41\u5f0f\u67b6\u6784\u4e0e\u7f13\u51b2\u533a\u7ba1\u7406\u673a\u5236\u4f18\u5316\u8d44\u6e90\u5206\u914d\uff0c\u786e\u4fdd\u5728\u786c\u4ef6\u9650\u5236\u4e0b\u8fd0\u884c\u3002", "result": "CNN\u5185\u6838\u5355\u5c42\u52a0\u901f\u8fbe200\u500d\uff08\u5e73\u57474\u5c42\u67b6\u678415\u500d\uff09\uff0c\u5904\u7406\u5927\u8f93\u5165\u65f6\u8d44\u6e90\u5229\u7528\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u6ee1\u8db3\u8fb9\u7f18\u8bbe\u5907\u7ea6\u675f\u3002", "conclusion": "MING\u6210\u529f\u89e3\u51b3\u8fb9\u7f18FPGA\u7684\u8d44\u6e90-\u6027\u80fd\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6ML\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u8bbe\u8ba1\u8303\u5f0f\u3002"}}
{"id": "2602.11435", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11435", "abs": "https://arxiv.org/abs/2602.11435", "authors": ["Haolin Li", "Michael Coblenz"], "title": "A Grounded Theory of Debugging in Professional Software Engineering Practice", "comment": "Accepted by FSE'26", "summary": "Debugging is a central yet complex activity in software engineering. Prior studies have documented debugging strategies and tool usage, but little theory explains how experienced developers reason about bugs in large, real-world codebases. We conducted a qualitative study using a grounded theory approach. We observed seven professional developers and five professional live-coding streamers working on 17 debugging tasks in their own codebases, capturing diverse contexts of debugging. We theorize debugging as a structured, iterative diagnostic process in which programmers update a mental model of the system to guide information gathering. Developers gather information by alternating between navigation and execution strategies, employing forward and backward tracing modes of reasoning and adapting these approaches according to codebase context, complexity, and familiarity. Developers also gather external resources to complement code-based evidence, with their experience enabling them to systematically construct a mental model. We contribute a grounded theory of professional debugging that surfaces the human-centered dimensions of the practice, with implications for tool design and software engineering education.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u624e\u6839\u7406\u8bba\u5206\u6790\u4e86\u4e13\u4e1a\u5f00\u53d1\u8005\u8c03\u8bd5\u5927\u578b\u4ee3\u7801\u5e93\u7684\u8fc7\u7a0b\uff0cJacob\u63ed\u793a\u8c03\u8bd5\u6d3b\u52a8\u7684\u4e3b\u8981\u7279\u5f81\u548c\u7b56\u7565\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u7814\u7a76\u63a2\u8ba8\u8c03\u8bd5\u7b56\u7565\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u4f46\u7f3a\u4e4f\u89e3\u91ca\u4e13\u5bb6\u5f00\u53d1\u8005\u5982\u4f55\u5728\u771f\u5b9e\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u63a8\u7406bug\u7684\u7406\u8bba\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u586b\u8865\u6b64\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5b9a\u6027\u624e\u6839\u7406\u8bba\u7814\u7a76\uff0c\u89c2\u5bdf7\u4f4d\u4e13\u4e1a\u5f00\u53d1\u8005\u548c5\u4f4d\u76f4\u64ad\u5f00\u53d1\u8005\u5728\u5176\u81ea\u6709\u4ee3\u7801\u5e93\u4e2d\u5b8c\u621017\u4e2a\u8c03\u8bd5\u4efb\u52a1\uff0c\u4ee5\u6355\u83b7\u591a\u6837\u5316\u8c03\u8bd5\u60c5\u5883\u3002", "result": "\u8c03\u8bd5\u88ab\u89c6\u4e3a\u7ed3\u6784\u5316\u8fed\u4ee3\u8bca\u65ad\u8fc7\u7a0b\uff0c\u5f00\u53d1\u8005\u901a\u8fc7\u4ea4\u66ff\u5bfc\u822a\u4e0e\u6267\u884c\u7b56\u7565\u6765\u66f4\u65b0\u7cfb\u7edf\u5fc3\u7406\u6a21\u578b\uff0c\u5e76\u89c6\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u3001\u590d\u6742\u5ea6\u7b49\u56e0\u7d20\u8c03\u6574\u6b63\u53cd\u5411\u8ffd\u8e2a\u63a8\u7406\u65b9\u5f0f\uff1b\u540c\u65f6\u501f\u52a9\u7ecf\u9a8c\u548c\u5916\u90e8\u8d44\u6e90\u6784\u5efa\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u8d21\u732e\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4e13\u4e1a\u8c03\u8bd5\u7406\u8bba\uff0c\u5f3a\u8c03\u5b9e\u8df5\u7ef4\u5ea6\u548c\u5fc3\u667a\u6a21\u578b\u5efa\u8bbe\uff0c\u5bf9\u5de5\u5177\u8bbe\u8ba1\u53ca\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\u6709\u91cd\u8981\u542f\u793a\u610f\u4e49\u3002"}}
{"id": "2602.11447", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11447", "abs": "https://arxiv.org/abs/2602.11447", "authors": ["Zixuan Feng", "Katie Kimura", "Bianca Trinkenreich", "Igor Steinmacher", "Marco Gerosa", "Anita Sarma"], "title": "Addressing OSS Community Managers' Challenges in Contributor Retention", "comment": null, "summary": "Open-source software (OSS) community managers face significant challenges in retaining contributors, as they must monitor activity and engagement while navigating complex dynamics of collaboration. Current tools designed for managing contributor retention (e.g., dashboards) fall short by providing retrospective rather than predictive insights to identify potential disengagement early. Without understanding how to anticipate and prevent disengagement, new solutions risk burdening community managers rather than supporting retention management. Following the Design Science Research paradigm, we employed a mixed-methods approach for problem identification and solution design to address contributor retention. To identify the challenges hindering retention management in OSS, we conducted semi-structured interviews, a multi-vocal literature review, and community surveys. Then through an iterative build-evaluate cycle, we developed and refined strategies for diagnosing retention risks and informing engagement efforts. We operationalized these strategies into a web-based prototype, incorporating feedback from 100+ OSS practitioners, and conducted an in situ evaluation across two OSS communities. Our study offers (1) empirical insights into the challenges of contributor retention management in OSS, (2) actionable strategies that support OSS community managers' retention efforts, and (3) a practical framework for future research in developing or validating theories about OSS sustainability.", "AI": {"tldr": "\u63d0\u51fa\u9884\u6d4b\u6027\u65b9\u6cd5\u89e3\u51b3\u5f00\u6e90\u793e\u533a\u8d21\u732e\u8005\u6d41\u5931\u95ee\u9898", "motivation": "\u73b0\u6709\u5de5\u5177\u4ec5\u63d0\u4f9b\u6ede\u540e\u6d1e\u5bdf\uff0c\u65e0\u6cd5\u9884\u6d4b\u8d21\u732e\u8005\u6d41\u5931\uff0c\u589e\u52a0\u793e\u533a\u7ba1\u7406\u8d1f\u62c5", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a\u8bbf\u8c08\u3001\u6587\u732e\u7efc\u8ff0\u3001\u793e\u533a\u8c03\u67e5\u8bc6\u522b\u95ee\u9898\uff1b\u8fed\u4ee3\u5f0f\u539f\u578b\u5f00\u53d1\u4e0e\u4e24\u4e2a\u793e\u533a\u7684\u73b0\u573a\u8bc4\u4f30", "result": "\u5f00\u53d1\u542b\u98ce\u9669\u8bca\u65ad\u7b56\u7565\u7684Web\u539f\u578b\uff0c\u7ecf100\u591a\u540d\u4ece\u4e1a\u8005\u9a8c\u8bc1\uff1b\u63d0\u4f9b\u5b9e\u8bc1\u89c1\u89e3\u3001\u64cd\u4f5c\u7b56\u7565\u548c\u7814\u7a76\u6846\u67b6", "conclusion": "\u4e3a\u5f00\u6e90\u53ef\u6301\u7eed\u6027\u7814\u7a76\u5efa\u7acb\u5b9e\u7528\u6846\u67b6\uff0c\u63d0\u5347\u793e\u533a\u7ba1\u7406\u8005\u9884\u6d4b\u6027\u7559\u5b58\u80fd\u529b"}}
{"id": "2602.11487", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11487", "abs": "https://arxiv.org/abs/2602.11487", "authors": ["Asmar Muqeet", "Shaukat Ali", "Paolo Arcaini"], "title": "Search-Based Quantum Program Testing via Commuting Pauli String", "comment": null, "summary": "Quantum software testing is important for reliable quantum software engineering. Despite recent advances, existing quantum software testing approaches rely on simple test inputs and statistical oracles, costly program specifications, and limited validation on real quantum computers. To address these challenges, we propose SB-QOPS, a search-based quantum program testing approach via commuting Pauli strings. SB-QOPS, as a direct extension to a previously proposed QOPS approach, redefines test cases in terms of Pauli strings and introduces a measurement-centric oracle that exploits their commutation properties, enabling effective testing of quantum programs while reducing the need for full program specifications. By systematically exploring the search space through an expectation-value-based fitness function, SB-QOPS improves test budget utilization and increases the likelihood of uncovering subtle faults. We conduct a large-scale empirical evaluation on quantum circuits of up to 29 qubits on real quantum computers and emulators. We assess three search strategies: Genetic Algorithm, Hill Climbing, and the (1+1) Evolutionary Algorithm, and evaluate SB-QOPS under both simulated and real noisy conditions. Experiments span three quantum computing platforms: IBM, IQM, and Quantinuum. Results show that SB-QOPS significantly outperforms QOPS, achieving a fault-detection score of 100% for circuits up to 29 qubits, and demonstrating portability across quantum platforms.", "AI": {"tldr": "\u63d0\u51faSB-QOPS\u65b9\u6cd5\uff0c\u901a\u8fc7Pauli\u5b57\u7b26\u4e32\u4f18\u5316\u91cf\u5b50\u7a0b\u5e8f\u6d4b\u8bd5\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u5728\u771f\u5b9e\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u9a8c\u8bc1\u8de8\u5e73\u53f0\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u91cf\u5b50\u8f6f\u4ef6\u6d4b\u8bd5\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u8f93\u5165\u548c\u7edf\u8ba1\u9884\u8a00\uff0c\u89c4\u8303\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u771f\u5b9e\u9a8c\u8bc1\uff0cSB-QOPS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u6269\u5c55QOPS\u65b9\u6cd5\uff0c\u4f7f\u7528Pauli\u5b57\u7b26\u4e32\u5b9a\u4e49\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5f15\u5165\u57fa\u4e8e\u9884\u671f\u503c\u7684\u9002\u5e94\u5ea6\u51fd\u6570\u548c\u4e09\u79cd\u641c\u7d22\u7b56\u7565\uff08\u9057\u4f20\u7b97\u6cd5\u3001\u722c\u5c71\u6cd5\u3001(1+1)\u8fdb\u5316\u7b97\u6cd5\uff09\uff0c\u5728IBM\u3001IQM\u3001Quantinuum\u5e73\u53f0 \ud5a5\uc0c1\u5b9e\u6d4b\u53ca\u6a21\u62df\u566a\u58f0\u73af\u5883\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "SB-QOPS\u572829\u91cf\u5b50\u6bd4\u7279\u7535\u8def\u4e0a\u5b9e\u73b0100%\u6545\u969c\u68c0\u51fa\u7387\uff0c\u663e\u8457\u4f18\u4e8eQOPS\uff0c\u4e14\u5728\u5404\u91cf\u5b50\u5e73\u53f0\u5c55\u73b0\u826f\u597d\u53ef\u79fb\u690d\u6027\u3002", "conclusion": "SB-QOPS\u9ad8\u6548\u63d0\u5347\u6d4b\u8bd5\u9884\u7b97\u5229\u7528\u7387\uff0c\u51cf\u5c11\u7a0b\u5e8f\u89c4\u8303\u9700\u6c42\uff0c\u589e\u5f3a\u4e86\u91cf\u5b50\u8f6f\u4ef6\u7684\u53ef\u9760\u6027\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2602.11514", "categories": ["cs.SE", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.11514", "abs": "https://arxiv.org/abs/2602.11514", "authors": ["Sidong Feng", "Chunyang Chen"], "title": "How Smart Is Your GUI Agent? A Framework for the Future of Software Interaction", "comment": null, "summary": "GUI agents are rapidly becoming a new interaction to software, allowing people to navigate web, desktop and mobile rather than execute them click by click. Yet ``agent'' is described with radically different degrees of autonomy, obscuring capability, responsibility and risk. We call for conceptual clarity through GUI Agent Autonomy Levels (GAL), a six-level framework that makes autonomy explicit and helps benchmark progress toward trustworthy software interaction.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11671", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11671", "abs": "https://arxiv.org/abs/2602.11671", "authors": ["Minh Le-Anh", "Huyen Nguyen", "Khanh An Tran", "Nam Le Hai", "Linh Ngo Van", "Nghi D. Q. Bui", "Bach Le"], "title": "Do Not Treat Code as Natural Language: Implications for Repository-Level Code Generation and Beyond", "comment": "Accepted to FSE 2026", "summary": "Large language models for code (CodeLLMs) have demonstrated remarkable success in standalone code completion and generation, sometimes even surpassing human performance, yet their effectiveness diminishes in repository-level settings where cross-file dependencies and structural context are essential. Existing Retrieval-Augmented Generation (RAG) approaches often borrow strategies from NLP, relying on chunking-based indexing and similarity-based retrieval. Chunking results in the loss of coherence between code units and overlooks structural relationships, while similarity-driven methods frequently miss functionally relevant dependencies such as helper functions, classes, or global variables. To address these limitations, we present Hydra, a repository-level code generation framework that treats code as structured code rather than natural language. Our approach introduces (i) a structure-aware indexing strategy that represents repositories as hierarchical trees of functions, classes, and variables, preserving code structure and dependencies, (ii) a lightweight dependency-aware retriever (DAR) that explicitly identifies and retrieves the true dependencies required by a target function, and (iii) a hybrid retrieval mechanism that combines DAR with similarity-based retrieval to provide both essential building blocks and practical usage examples. Extensive experiments on the challenging DevEval and RepoExec benchmarks, both requiring function implementation from real-world repositories with complex large repository context, show that Hydra achieves state-of-the-art performance across open- and closed-source CodeLLMs. Notably, our method establishes a new state of the art in repository-level code generation, surpassing strongest baseline by over 5% in Pass@1 and even enabling smaller models to match or exceed the performance of much larger ones that rely on existing retrievers.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11692", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11692", "abs": "https://arxiv.org/abs/2602.11692", "authors": ["Shashiwadana Nirmani", "Hourieh Khalajzadeh", "Mojtaba Shahin", "Xiao Liu"], "title": "Beyond Code: Empirical Insights into How Team Dynamics Influence OSS Project Selection", "comment": null, "summary": "Open-source software (OSS) development relies on effective collaboration among distributed contributors. Yet, current OSS project recommendation systems primarily emphasize technical attributes, overlooking the collaboration and community aspects that influence contributors' decisions to join and remain in projects. This study investigates how team dynamics within OSS communities influence project selection and how these preferences vary across contributors' motivations. We conducted an online survey with 198 OSS practitioners, combining quantitative and qualitative analyses to capture contributors' perceptions of team dynamics. The results reveal that communication-related team dynamics such as responsiveness, tone, and clarity of replies are consistently prioritized across practitioners. However, the relative importance of these team dynamics differs according to contributors' motivations. For instance, practitioners motivated by gaining reputation or networking preferred inclusive project communities that encouraged diverse participation. These findings highlight that understanding how team dynamics align with contributors' motivations provides valuable insights into practitioners' project selection behaviour. Those insights can inform the design of future human-aware project recommendation systems that better account for social collaboration quality and motivational fit.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11724", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11724", "abs": "https://arxiv.org/abs/2602.11724", "authors": ["Xiwen Teoh", "Yun Lin", "Duc-Minh Nguyen", "Ruofei Ren", "Wenjie Zhang", "Jin Song Dong"], "title": "WebTestPilot: Agentic End-to-End Web Testing against Natural Language Specification by Inferring Oracles with Symbolized GUI Elements", "comment": null, "summary": "Visual language model (VLM) agents show great promise in automating end-to-end (E2E) web testing against requirements in natural language. However, the probabilistic nature of language models can have inherent hallucinations. Therefore, given a detected inconsistency between the requirement and the web application, it is hard to distinguish whether it stems from the hallucination or a real application bug. Addressing this issue presents two core technical challenges: the implicit oracle inference challenge, where the agent must act as its own oracle to implicitly decide if the application's behavior is correct without guidance, and the probabilistic inference challenge, where an LLM's inconsistent reasoning undermines its trustworthiness as an oracle. Existing LLM-based approaches fail to capture such implicit oracles, either by treating any page navigation that doesn't crash as a success, or by checking each state in isolation, thus missing bugs dependent on context from prior steps.\n  We introduce WebTestPilot, an LLM-based agent designed to address these challenges. WebTestPilot uses (1) a symbolization layer which detects and symbolizes critical GUI elements on the web application into symbols (i.e., variables) and (2) translates natural language specification into a sequence of steps, each of which is equipped with inferred pre- and post-conditions over the symbols as an oracle. This oracle captures data, temporal, and causal dependencies, enabling the validation of implicit requirements. To advance research in this area, we build a benchmark of bug-injected web apps for evaluating NL-to-E2E testing. The results show that WebTestPilot achieves a task completion rate of 99%, with 96% precision and 96% recall in bug detection, outperforming the best baseline (+70 precision, +27 recall). The agent generalizes across diverse natural language inputs and model scales.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11746", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11746", "abs": "https://arxiv.org/abs/2602.11746", "authors": ["Nafiz Imtiaz Khan", "Vladimir Filkov"], "title": "Leveraging Language Models to Discover Evidence-Based Actions for OSS Sustainability", "comment": null, "summary": "When successful, Open Source Software (OSS) projects create enormous value, but most never reach a sustainable state. Recent work has produced accurate models that forecast OSS sustainability, yet these models rarely tell maintainers what to do: their features are often high-level socio-technical signals that are not directly actionable. Decades of empirical software engineering research have accumulated a large but underused body of evidence on concrete practices that improve project health.\n  We close this gap by using LLMs as evidence miners over the SE literature. We design a RAG-pipeline and a two-layer prompting strategy that extract researched actionables (ReACTs): concise, evidence-linked recommendations mapping to specific OSS practices. In the first layer, we systematically explore open LLMs and prompting techniques, selecting the best-performing combination to derive candidate ReACTs from 829 ICSE and FSE papers. In the second layer, we apply follow-up prompting to filter hallucinations, extract impact and evidence, and assess soundness and precision.\n  Our pipeline yields 1,922 ReACTs, of which 1,312 pass strict quality criteria and are organized into practice-oriented categories connectable to project signals from tools like APEX. The result is a reproducible, scalable approach turning scattered research findings into structured, evidence-based actions guiding OSS projects toward sustainability.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11887", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.11887", "abs": "https://arxiv.org/abs/2602.11887", "authors": ["Javier Ron", "Martin Monperrus"], "title": "Verifiable Provenance of Software Artifacts with Zero-Knowledge Compilation", "comment": null, "summary": "Verifying that a compiled binary originates from its claimed source code is a fundamental security requirement, called source code provenance. Achieving verifiable source code provenance in practice remains challenging. The most popular technique, called reproducible builds, requires difficult matching and reexecution of build toolchains and environments. We propose a novel approach to verifiable provenance based on compiling software with zero-knowledge virtual machines (zkVMs). By executing a compiler within a zkVM, our system produces both the compiled output and a cryptographic proof attesting that the compilation was performed on the claimed source code with the claimed compiler. We implement a proof-of-concept implementation using the RISC Zero zkVM and the ChibiCC C compiler, and evaluate it on 200 synthetic programs as well as 31 OpenSSL and 21 libsodium source files. Our results show that zk-compilation is applicable to real-world software and provides strong security guarantees: all adversarial tests targeting compiler substitution, source tampering, output manipulation, and replay attacks are successfully blocked.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u96f6\u77e5\u8bc6\u865a\u62df\u673alearn(zkVM)\u7f16\u8bd1\u8f6f\u4ef6\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7zkVM\u6267\u884c\u7f16\u8bd1\u5668\uff0c\u751f\u6210\u4e8c\u8fdb\u5236\u6587\u4ef6\u53ca\u5bc6\u7801\u5b66\u8bc1\u660e\uff0c\u786e\u4fdd\u6e90\u4ee3\u7801\u6765\u6e90\u771f\u5b9e\uff0c\u89e3\u51b3\u4e86\u53ef\u91cd\u73b0\u6784\u5efa\u7684\u96be\u9898\u3002", "motivation": "\u52a8\u673a\u662f\u9a8c\u8bc1\u4e8c\u8fdb\u5236\u6587\u4ef6\u662f\u5426\u6e90\u81ea\u5ba3\u79f0\u7684\u6e90\u4ee3\u7801\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u4f20\u7edf\u53ef\u91cd\u73b0\u6784\u5efa\u6280\u672f\u56e0\u9700\u5339\u914d\u5de5\u5177\u94fe\u548c\u73af\u5883\u800c\u64cd\u4f5c\u56f0\u96be\u3002", "method": "\u65b9\u6cd5\u662f\u5728zkVM\u4e2d\u6267\u884c\u7f16\u8bd1\u5668\uff08\u4f7f\u7528RISC Zero zkVM\u548cChibiCC\u7f16\u8bd1\u5668\uff09\uff0c\u540c\u65f6\u4ea7\u51fa\u7f16\u8bd1\u8f93\u51fa\u548c\u5bc6\u7801\u5b66\u8bc1\u660e\uff0c\u4ee5\u8bc1\u5b9e\u7f16\u8bd1\u57fa\u4e8e\u58f0\u79f0\u7684\u6e90\u4ee3\u7801\u548c\u7f16\u8bd1\u5668\u8fdb\u884c\u3002", "result": "\u7ed3\u679c\u8bc1\u660e\u8be5\u7cfb\u7edf\u9002\u7528\u4e8e\u771f\u5b9e\u8f6f\u4ef6\uff08\u6d4b\u8bd5\u5bf9\u8c61\u5305\u62ec200\u4e2a\u5408\u6210\u7a0b\u5e8f\u53caOpenSSL\u3001libsodium\u6587\u4ef6\uff09\uff0c\u6210\u529f\u963b\u6b62\u6240\u6709\u9488\u5bf9\u7f16\u8bd1\u5668\u66ff\u6362\u3001\u6e90\u4ee3\u7801\u7be1\u6539\u3001\u8f93\u51fa\u64cd\u7eb5\u548c\u91cd\u653e\u653b\u51fb\u7684\u5bf9\u6297\u6d4b\u8bd5\u3002", "conclusion": "\u7ed3\u8bba\u662fzk\u7f16\u8bd1\u65b9\u6cd5\u63d0\u4f9b\u5f3a\u5b89\u5168\u4fdd\u8bc1\uff0c\u5145\u5206\u6ee1\u8db3\u73b0\u5b9e\u8f6f\u4ef6\u9700\u6c42\u4e14\u6709\u6548\u9632\u5fa1\u653b\u51fb\u3002"}}
{"id": "2602.11904", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11904", "abs": "https://arxiv.org/abs/2602.11904", "authors": ["Weixing Zhang", "Bowen Jiang", "Yuhong Fu", "Anne Koziolek", "Regina Hebig", "Daniel Str\u00fcber"], "title": "Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs: A Systematic Evaluation", "comment": null, "summary": "Software languages evolve over time for reasons such as feature additions. When grammars evolve, textual instances that originally conformed to them may become outdated. While model-driven engineering provides many techniques for co-evolving models with metamodel changes, these approaches are not designed for textual DSLs and may lose human-relevant information such as layout and comments. This study systematically evaluates the potential of large language models (LLMs) for co-evolving grammars and instances of textual DSLs. Using Claude Sonnet 4.5 and GPT-5.2 across ten case languages with ten runs each, we assess both correctness and preservation of human-oriented information. Results show strong performance on small-scale cases ($\\geq$94% precision and recall for instances requiring fewer than 20 modified lines), but performance degraded with scale: Claude maintains 85% recall at 40 lines, while GPT fails on the largest instances. Response time increases substantially with instance size, and grammar evolution complexity and deletion granularity affect performance more than change type. These findings clarify when LLM-based co-evolution is effective and where current limitations remain.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11911", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11911", "abs": "https://arxiv.org/abs/2602.11911", "authors": ["Giuseppe Crupi", "Rosalia Tufano", "Gabriele Bavota"], "title": "Improving Code Generation via Small Language Model-as-a-judge", "comment": "Accepted to the 48th International Conference on Software Engineering (ICSE 2026)", "summary": "Large language models (LLMs) have shown remarkable capabilities in automated code generation. While effective for mainstream languages, they may underperform on less common or domain-specific languages, prompting companies to develop in-house code generators. While open-source models can be trained for this, only LLMs with tens of billions of parameters match the performance of commercial tools, demanding costly training and deployment. Recent work proposed supporting code generation with smaller models (SLMs) by generating multiple candidate solutions and using another SLM to select the most likely correct one. The most recent work in this area is the one by Sun et al. [29] presenting RankEF, a T5 model trained to rank code solutions using both execution-based and non-execution-based information. However, Sun et al. do not assess the T5 ranker's classification accuracy, that is, how often it misjudges correct implementations as incorrect or vice versa, leaving open questions about the reliability of LMs as code correctness judges for other tasks (e.g., automated code review). Moreover, their experiments involve relatively old models, making it unclear the extent to which such a methodology would still help companies in cheaply training their own code generators with performance comparable to those of massive LLMs. We present a study addressing these limitations. We train several state-of-the-art SLMs as code correctness judges and assess their ability to discriminate between correct and wrong implementations. We show that modern SLMs outperform RankEF, even without exploiting execution-based information. When used as code rankers, they achieve higher performance gains than RankEF and perform competitively with LLMs 5-25x larger, at a fraction of the cost.", "AI": {"tldr": "\u73b0\u4ee3\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u4ee3\u7801\u6b63\u786e\u6027\u5224\u65ad\u4e0a\u8d85\u8d8aRankEF\uff0c\u65e0\u9700\u6267\u884c\u4fe1\u606f\uff0c\u5e76\u4ee5\u4f4e\u6210\u672c\u5ab2\u7f8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9Sun\u7b49\u4eba[29]\u7814\u7a76\u4e2dRankEF\u672a\u8bc4\u4f30\u5206\u7c7b\u51c6\u786e\u6027\u53ca\u4f7f\u7528\u8fc7\u65f6\u6a21\u578b\u7684\u5c40\u9650\uff0c\u672c\u7814\u7a76\u63a2\u8ba8SLMs\u4f5c\u4e3a\u4ee3\u7801\u6b63\u786e\u6027\u8bc4\u5224\u5668\u7684\u53ef\u9760\u6027\u3002", "method": "\u8bad\u7ec3\u591a\u4e2a\u5148\u8fdbSLMs\u4f5c\u4e3a\u4ee3\u7801\u6b63\u786e\u6027\u8bc4\u5224\u5668\uff0c\u8bc4\u4f30\u5176\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u4ee3\u7801\u7684\u80fd\u529b\u3002", "result": "\u73b0\u4ee3SLMs\u5728\u4e0d\u4f9d\u8d56\u6267\u884c\u4fe1\u606f\u65f6\u8868\u73b0\u4f18\u4e8eRankEF\uff0c\u6392\u540d\u4ee3\u7801\u65f6\u63d0\u5347\u66f4\u591a\u6027\u80fd\uff0c\u4ee5\u4f4e\u6210\u672c\u4e0e\u59275-25\u500d\u7684LLMs\u7ade\u4e89\u3002", "conclusion": "SLMs\u53ef\u5b9e\u73b0\u4f4e\u6210\u672c\u8bad\u7ec3\u81ea\u5b9a\u4e49\u4ee3\u7801\u751f\u6210\u5668\uff0c\u6027\u80fd\u5ab2\u7f8e\u5927\u578bLLMs\uff0c\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u9ad8\u6548\u66ff\u4ee3EDER\u65b9\u6848\u3002"}}
{"id": "2602.11925", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11925", "abs": "https://arxiv.org/abs/2602.11925", "authors": ["Giuseppe Crupi", "Rosalia Tufano", "Gabriele Bavota"], "title": "Studying Quality Improvements Recommended via Manual and Automated Code Review", "comment": "Accepted at the 34th International Conference on Program Comprehension (ICPC 2026)", "summary": "Several Deep Learning (DL)-based techniques have been proposed to automate code review. Still, it is unclear the extent to which these approaches can recommend quality improvements as a human reviewer. We study the similarities and differences between code reviews performed by humans and those automatically generated by DL models, using ChatGPT-4 as representative of the latter. In particular, we run a mining-based study in which we collect and manually inspect 739 comments posted by human reviewers to suggest code changes in 240 PRs. The manual inspection aims at classifying the type of quality improvement recommended by human reviewers (e.g., rename variable/constant). Then, we ask ChatGPT to perform a code review on the same PRs and we compare the quality improvements it recommends against those suggested by the human reviewers. We show that while, on average, ChatGPT tends to recommend a higher number of code changes as compared to human reviewers (~2.4x more), it can only spot 10% of the quality issues reported by humans. However, ~40% of the additional comments generated by the LLM point to meaningful quality issues. In short, our findings show the complementarity of manual and AI-based code review. This finding suggests that, in its current state, DL-based code review can be used as a further quality check on top of the one performed by humans, but should not be considered as a valid alternative to them nor as a mean to save code review time, since human reviewers would still need to perform their manual inspection while also validating the quality issues reported by the DL-based technique.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.11988", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11988", "abs": "https://arxiv.org/abs/2602.11988", "authors": ["Thibaud Gloaguen", "Niels M\u00fcndler", "Mark M\u00fcller", "Veselin Raychev", "Martin Vechev"], "title": "Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?", "comment": null, "summary": "A widespread practice in software development is to tailor coding agents to repositories using context files, such as AGENTS.md, by either manually or automatically generating them. Although this practice is strongly encouraged by agent developers, there is currently no rigorous investigation into whether such context files are actually effective for real-world tasks. In this work, we study this question and evaluate coding agents' task completion performance in two complementary settings: established SWE-bench tasks from popular repositories, with LLM-generated context files following agent-developer recommendations, and a novel collection of issues from repositories containing developer-committed context files.\n  Across multiple coding agents and LLMs, we find that context files tend to reduce task success rates compared to providing no repository context, while also increasing inference cost by over 20%. Behaviorally, both LLM-generated and developer-provided context files encourage broader exploration (e.g., more thorough testing and file traversal), and coding agents tend to respect their instructions. Ultimately, we conclude that unnecessary requirements from context files make tasks harder, and human-written context files should describe only minimal requirements.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12038", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12038", "abs": "https://arxiv.org/abs/2602.12038", "authors": ["Yuejun Guo", "Qiang Hu", "Qiang Tang", "Yves Le Traon"], "title": "An Empirical Study of the Imbalance Issue in Software Vulnerability Detection", "comment": "This paper was accepted by the 28th European Symposium on Research in Computer Security (ESORICS), 2023", "summary": "Vulnerability detection is crucial to protect software security. Nowadays, deep learning (DL) is the most promising technique to automate this detection task, leveraging its superior ability to extract patterns and representations within extensive code volumes. Despite its promise, DL-based vulnerability detection remains in its early stages, with model performance exhibiting variability across datasets. Drawing insights from other well-explored application areas like computer vision, we conjecture that the imbalance issue (the number of vulnerable code is extremely small) is at the core of the phenomenon. To validate this, we conduct a comprehensive empirical study involving nine open-source datasets and two state-of-the-art DL models. The results confirm our conjecture. We also obtain insightful findings on how existing imbalance solutions perform in vulnerability detection. It turns out that these solutions perform differently as well across datasets and evaluation metrics. Specifically: 1) Focal loss is more suitable to improve the precision, 2) mean false error and class-balanced loss encourages the recall, and 3) random over-sampling facilitates the F1-measure. However, none of them excels across all metrics. To delve deeper, we explore external influences on these solutions and offer insights for developing new solutions.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12058", "categories": ["cs.SE", "cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2602.12058", "abs": "https://arxiv.org/abs/2602.12058", "authors": ["Zhiyong Chen", "Jialun Cao", "Chang Xu", "Shing-Chi Cheung"], "title": "ModelWisdom: An Integrated Toolkit for TLA+ Model Visualization, Digest and Repair", "comment": "Accepted by FM 2026 Research Track (Tool)", "summary": "Model checking in TLA+ provides strong correctness guarantees, yet practitioners continue to face significant challenges in interpreting counterexamples, understanding large state-transition graphs, and repairing faulty models. These difficulties stem from the limited explainability of raw model-checker output and the substantial manual effort required to trace violations back to source specifications. Although the TLA+ Toolbox includes a state diagram viewer, it offers only a static, fully expanded graph without folding, color highlighting, or semantic explanations, which limits its scalability and interpretability. We present ModelWisdom, an interactive environment that uses visualization and large language models to make TLA+ model checking more interpretable and actionable. ModelWisdom offers: (i) Model Visualization, with colorized violation highlighting, click-through links from transitions to TLA+ code, and mapping between violating states and broken properties; (ii) Graph Optimization, including tree-based structuring and node/edge folding to manage large models; (iii) Model Digest, which summarizes and explains subgraphs via large language models (LLMs) and performs preprocessing and partial explanations; and (iv) Model Repair, which extracts error information and supports iterative debugging. Together, these capabilities turn raw model-checker output into an interactive, explainable workflow, improving understanding and reducing debugging effort for nontrivial TLA+ specifications. The website to ModelWisdom is available: https://model-wisdom.pages.dev. A demonstrative video can be found at https://www.youtube.com/watch?v=plyZo30VShA.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12079", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.12079", "abs": "https://arxiv.org/abs/2602.12079", "authors": ["Alessandro Aneggi", "Vincenzo Stoico", "Andrea Janes"], "title": "Performance Antipatterns: Angel or Devil for Power Consumption?", "comment": null, "summary": "Performance antipatterns are known to degrade the responsiveness of microservice-based systems, but their impact on energy consumption remains largely unexplored. This paper empirically investigates whether widely studied performance antipatterns defined by Smith and Williams also negatively influence power usage. We implement ten antipatterns as isolated microservices and evaluate them under controlled load conditions, collecting synchronized measurements of performance, CPU and DRAM power consumption, and resource utilization across 30 repeated runs per antipattern. The results show that while all antipatterns degrade performance as expected, only a subset exhibit a statistically significant relationship between response time and increased power consumption. Specifically, several antipatterns reach CPU saturation, capping power draw regardless of rising response time, whereas others (\\eg Unnecessary Processing, The Ramp) demonstrate energy-performance coupling indicative of inefficiency. Our results show that, while all injected performance antipatterns increase response time as expected, only a subset also behaves as clear energy antipatterns, with several cases reaching a nearly constant CPU power level where additional slowdowns mainly translate into longer execution time rather than higher instantaneous power consumption. The study provides a systematic foundation for identifying performance antipatterns that also behave as energy antipatterns and offers actionable insights for designing more energy-efficient microservices architectures.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12081", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.12081", "abs": "https://arxiv.org/abs/2602.12081", "authors": ["Alessandro Aneggi", "Xiaozhou Li", "Andrea Janes"], "title": "PPTAM$\u03b7$: Energy Aware CI/CD Pipeline for Container Based Applications", "comment": null, "summary": "Modern container-based microservices evolve through rapid deployment cycles, but CI/CD pipelines still rarely measure energy consumption, even though prior work shows that design patterns, code smells and refactorings affect energy efficiency. We present PPTAM$\u03b7$, an automated pipeline that integrates power and energy measurement into GitLab CI for containerised API systems, coordinating load generation, container monitoring and hardware power probes to collect comparable metrics at each commit. The pipeline makes energy visible to developers, supports version comparison for test engineers and enables trend analysis for researchers. We evaluate PPTAM$\u03b7$ on a JWT-authenticated API across four commits, collecting performance and energy metrics and summarising the architecture, measurement methodology and validation.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.12256", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.12256", "abs": "https://arxiv.org/abs/2602.12256", "authors": ["Alex Chudic", "G\u00fcl \u00c7al\u0131kl\u0131"], "title": "Automated Test Suite Enhancement Using Large Language Models with Few-shot Prompting", "comment": "13 pages, 3 figures, accepted to ICPC 2026 (34th International Conference on Program Comprehension)", "summary": "Unit testing is essential for verifying the functional correctness of code modules (e.g., classes, methods), but manually writing unit tests is often labor-intensive and time-consuming. Unit tests generated by tools that employ traditional approaches, such as search-based software testing (SBST), lack readability, naturalness, and practical usability. LLMs have recently provided promising results and become integral to developers' daily practices. Consequently, software repositories now include a mix of human-written tests, LLM-generated tests, and those from tools employing traditional approaches such as SBST. While LLMs' zero-shot capabilities have been widely studied, their few-shot learning potential for unit test generation remains underexplored. Few-shot prompting enables LLMs to learn from examples in the prompt, and automatically retrieving such examples could enhance test suites. This paper empirically investigates how few-shot prompting with different test artifact sources, comprising human, SBST, or LLM, affects the quality of LLM-generated unit tests as program comprehension artifacts and their contribution to improving existing test suites by evaluating not only correctness and coverage but also readability, cognitive complexity, and maintainability in hybrid human-AI codebases. We conducted experiments on HumanEval and ClassEval datasets using GPT-4o, which is integrated into GitHub Copilot and widely used among developers. We also assessed retrieval-based methods for selecting relevant examples. Our results show that LLMs can generate high-quality tests via few-shot prompting, with human-written examples producing the best coverage and correctness. Additionally, selecting examples based on the combined similarity of problem description and code consistently yields the most effective few-shot prompts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5c11\u6837\u672c\u63d0\u793a\uff08\u4f7f\u7528\u4eba\u7c7b\u3001SBST\u6216LLM\u751f\u6210\u7684\u6d4b\u8bd5\u793a\u4f8b\uff09\u5bf9LLM\u751f\u6210\u5355\u5143\u6d4b\u8bd5\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4eba\u5de5\u7f16\u5199\u7684\u793a\u4f8b\u5728\u8986\u76d6\u7387\u548c\u6b63\u786e\u6027\u4e0a\u6548\u679c\u6700\u4f73\uff0c\u4e14\u7ed3\u5408\u95ee\u9898\u548c\u4ee3\u7801\u76f8\u4f3c\u5ea6\u7684\u793a\u4f8b\u68c0\u7d22\u65b9\u6cd5\u6700\u6709\u6548\u3002", "motivation": "\u624b\u52a8\u7f16\u5199\u5355\u5143\u6d4b\u8bd5\u8d39\u65f6\u8d39\u529b\uff0c\u4f20\u7edf\u5de5\u5177\uff08\u5982SBST\uff09\u751f\u6210\u7684\u6d4b\u8bd5\u7f3a\u4e4f\u53ef\u8bfb\u6027\u548c\u5b9e\u7528\u6027\uff1bLLM\u867d\u5f15\u4eba\u6ce8\u76ee\uff0c\u4f46\u5176\u5c11\u6837\u672c\u5b66\u4e60\u5728\u0e2b\u0e25\u0e31\u0e07\u6d4b\u8bd5\u751f\u6210\u4e2d\u5e94\u7528\u4e0d\u8db3\uff0c\u56e0\u6b64\u7814\u7a76\u4e86\u4e0d\u540c\u793a\u4f8b\u6e90\u5982\u4f55\u63d0\u5347\u6d4b\u8bd5\u8d28\u91cf\u3002", "method": "\u5728HumanEval\u548cClassEval\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u96c6\u6210\u4e8e\u0e15\u0e31\u0e14\u0159\u00edkladcutCopilot\u7684GPT-4o\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86\u6d4b\u8bd5\u7684\u6b63\u786e\u6027\u3001\u8986\u76d6\u7387\u3001\u53ef\u8bfb\u6027\u3001\u8ba4\u77e5\u590d\u6742\u5ea6\u548c\u53ef\u7ef4\u62a4\u6027\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u68c0\u7d22\u7684\u65b9\u6cd5\u4e3a\u5c11\u6837\u672c\u63d0\u793a\u9009\u62e9\u793a\u4f8b\u3002", "result": "LLM\u53ef\u901a\u8fc7\u5c11\u6837\u672c\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5355\u5143\u6d4b\u8bd5\uff0c\u5176\u4e2d\u4eba\u5de5\u793a\u4f8b\u4ea7\u751f\u7684\u6700\u4f73\u8986\u76d6\u7387\u548c\u6b63\u786e\u6027\uff1b\u7ed3\u5408\u95ee\u9898\u548c\u4ee3\u7801\u76f8\u4f3c\u5ea6\u8fdb\u884c\u68c0\u7d22\u7684\u63d0\u793a\u65b9\u6cd5\u5728\u6240\u6709\u60c5\u5883\u4e2d\u6548\u679c\u6700\u4f18\u3002", "conclusion": "\u5c11\u6837\u672c\u63d0\u793a\u663e\u8457\u6539\u5584LLM\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u91c7\u7528\u4eba\u5de5\u793a\u4f8b\u548c\u7efc\u5408\u76f8\u4f3c\u5ea6\u68c0\u7d22\u65f6\uff0c\u8fd9\u5bf9\u4e8e\u63d0\u5347\u6df7\u5408\u4eba-AI\u4ee3\u7801\u5e93\u7684\u6d4b\u8bd5\u5957\u4ef6\u81f3\u5173\u91cd\u8981\u3002"}}
