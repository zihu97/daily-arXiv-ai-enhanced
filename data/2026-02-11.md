<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 8]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.SE](#cs.SE) [Total: 14]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [XLB: A High Performance Layer-7 Load Balancer for Microservices using eBPF-based In-kernel Interposition](https://arxiv.org/abs/2602.09473)
*Yuejie Wang,Chenchen Shou,Jiaxu Qian,Guyue Liu*

Main category: cs.NI

TL;DR: 提出XLB架构，一种基于eBPF的内核层L7负载均衡器，大幅提升微服务性能


<details>
  <summary>Details</summary>
Motivation: 传统负载均衡器在微服务场景中性能不足，无法满足高实例数、长服务链等需求而导致性能下降

Method: 重新设计负载均衡架构，利用eBPF在内核层实现核心逻辑，通过套接字重定向和嵌套eBPF映射解决连接管理问题

Result: 比Istio和Cilium性能显著提升：50实例环境下吞吐量最高增加1.5倍，端到端延迟降低60%

Conclusion: XLB提供更轻量、可扩展且高效的负载均衡架构，消除了传统方案的调度和通信开销

Abstract: L7 load balancers are a fundamental building block in microservices as they enable fine-grained traffic distribution. Compared to monolithic applications, microservices demand higher performance and stricter isolation from load balancers. This is due to the increased number of instances, longer service chains, and the necessity for co-location with services on the same host. Traditional sidecar-based load balancers are ill-equipped to meet these demands, often resulting in significant performance degradation.
  In this work, we present XLB, a novel architecture that reshapes L7 load balancers as in-kernel interposition operating on the socket layer. We leverage eBPF to implement the core load balancing logic in the kernel, and address the connection management and state maintenance challenges through novel socket layer redirection and nested eBPF maps designs. XLB eliminates the extra overhead of scheduling, communication, and data movement, resulting in a more lightweight, scalable, and efficient L7 load balancer architecture. Compared to the widely used microservices load balancers (Istio and Cilium), over 50 microservice instances, XLB achieves up to 1.5x higher throughput and 60% lower end-to-end latency.

</details>


### [2] [Probabilistic Fair Ordering of Events](https://arxiv.org/abs/2602.09148)
*Muhammad Haseeb,Jinkun Geng,Aurojit Panda,Radhika Mittal,Nirav Atre,Srinivas Narayana,Anirudh Sivaraman*

Main category: cs.NI

TL;DR: 论文提出Tommy系统，利用时钟误差统计模型和社交选择理论实现概率公平的事件排序


<details>
  <summary>Details</summary>
Motivation: 在时钟同步存在固有误差的背景下，常规时间戳无法可靠排序事件，需要解决短暂时间窗内事件的公平排序问题

Method: 建立时钟同步误差统计模型，通过概率比较时间戳；结合社交选择理论的排名机制处理概率比较的非传递性问题，生成事件偏序关系

Result: 相较于Spanner TrueTime基准方法，Tommy显著提升公平性，实现高效部分排序

Conclusion: 接受时钟误差而非消除它，通过统计学与社会选择理论结合，实现了优于传统方法的公平事件序列

Abstract: A growing class of applications depends on fair ordering, where events that occur earlier should be processed before later ones. Providing such guarantees is difficult in practice because clock synchronization is inherently imperfect: events generated at different clients within a short time window may carry timestamps that cannot be reliably ordered. Rather than attempting to eliminate synchronization error, we embrace it and establish a probabilistically fair sequencing process. Tommy is a sequencer that uses a statistical model of per-clock synchronization error to compare noisy timestamps probabilistically. Although this enables ordering of two events, the probabilistic comparator is intransitive, making global ordering non-trivial. We address this challenge by mapping the sequencing problem to a classical ranking problem from social choice theory, which offers principled mechanisms for reasoning with intransitive comparisons. Using this formulation, Tommy produces a partial order of events, achieving significantly better fairness than a Spanner TrueTime-based baseline approach.

</details>


### [3] [Harvest: Adaptive Photonic Switching Schedules for Collective Communication in Scale-up Domains](https://arxiv.org/abs/2602.09188)
*Mahir Rahman,Samuel Joseph,Nihar Kodkani,Behnaz Arzani,Vamsi Addanki*

Main category: cs.NI

TL;DR: Harvest是一种合成光子互连拓扑重新配置调度的系统方法，旨在最小化集体通信完成时间。


<details>
  <summary>Details</summary>
Motivation: 光子互连具有高带宽和能效，但每次重新配置产生显著开销，导致每次步骤都重新配置在高效集体通信中不切实际。

Method: 利用动态规划框架结合拓扑优化子问题，合成调度平衡重新配置延迟、拥塞和传播延迟；并针对递归加倍算法内部优化生成最优调度，无需外部优化器。

Result: 通过在包级、流级评估及GPU硬件仿真测试中，相比静态互连和每次步骤重新配置基准，显著减少多种集体算法的完成时间。

Conclusion: Harvest能自适应不同光子技术，有效提升集体通信效率，并减少完成时间。

Abstract: As chip-to-chip silicon photonics gain traction for their bandwidth and energy efficiency, their circuit-switched nature raises a fundamental question for collective communication: when and how should the interconnect be reconfigured to realize these benefits? Establishing direct optical paths can reduce congestion and propagation delay, but each reconfiguration incurs non-negligible overhead, making naive per-step reconfiguration impractical.
  We present Harvest, a systematic approach for synthesizing topology reconfiguration schedules that minimize collective completion time in photonic interconnects. Given a collective communication algorithm and its fixed communication schedule, Harvest determines how the interconnect should evolve over the course of the collective, explicitly balancing reconfiguration delay against congestion and propagation delay. We reduce the synthesis problem into a dynamic program with an underlying topology optimization subproblem and show that the approach applies to arbitrary collective communication algorithms. Furthermore, we exploit the algorithmic structure of a well-known AllReduce algorithm (Recursive Doubling) to synthesize optimal reconfiguration schedules without using any optimizers. By parameterizing the formulation using reconfiguration delay, Harvest naturally adapts to various photonic technologies. Using packet-level and flow-level evaluations, as well as hardware emulation on commercial GPUs, we show that the schedules synthesized by Harvest significantly reduce collective completion time across multiple collective algorithms compared to static interconnects and reconfigure-every-step baselines.

</details>


### [4] [QoS Identifier and Slice Mapping in 5G and Non-Terrestrial Network Interconnected Systems](https://arxiv.org/abs/2602.09493)
*Yuma Abe,Mariko Sekiguchi,Amane Miura*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The interconnection of 5G and non-terrestrial networks (NTNs) has been actively studied to expand connectivity beyond conventional terrestrial infrastructure. In the 3GPP standardization of 5G systems, the 5G Quality of Service (QoS) Identifier (5QI) is defined to characterize the QoS requirements of different traffic requirements. However, it falls short in capturing the diverse latency, capacity, and reliability profiles of NTN environments, particularly when NTNs are used as backhaul. Furthermore, it is difficult to manage individual traffic flows and perform efficient resource allocation and routing when a large number of 5G traffic flows are present in NTN systems. To address these challenges, we propose an optimization framework that enhances QoS handling by introducing an NTN QoS Identifier (NQI) and grouping 5G traffic into NTN slices based on similar requirements. This enables unified resource control and routing for a large number of 5G flows in NTN systems. In this paper, we present the detailed procedure of the proposed framework, which consists of 5QI to NQI mapping, NTN traffic to NTN slice mapping, and slice-level flow and routing optimization. We evaluate the framework by comparing multiple mapping schemes through numerical simulations and analyze their impact on overall network performance.

</details>


### [5] [ISO FastLane: Faster ISO 11783 with Dual Stack Approach as a Short Term Solution](https://arxiv.org/abs/2602.09633)
*Timo Oksanen*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The agricultural industry has been searching for a high-speed successor to the 250~kbit/s CAN bus backbone of ISO~11783 (ISOBUS) for over a decade, yet no protocol-level solution has reached standardization. Meanwhile, modern planters, sprayers, and Virtual Terminals are already constrained by the bus bandwidth. This paper presents ISO FastLane, a gateway-less dual-stack approach that routes point-to-point ISOBUS traffic over Ethernet while keeping broadcast messages on the existing CAN bus. The solution requires no new state machines, no middleware, and no changes to application layer code: only a simple Layer~3 routing decision and a lightweight peer discovery mechanism called Augmented Address Claim (AACL). Legacy devices continue to operate unmodified and unaware of FastLane traffic. Preliminary tests reported on the paper demonstrate that ISO FastLane accelerates Virtual Terminal object pool uploads by factor of 8 and sustains Task Controller message rates over 100 times beyond the current specification limit. Because ISO FastLane builds entirely on existing J1939 and ISO~11783 conventions, it can be implemented by ISOBUS engineers in a matter of weeks. This is delivering tangible performance gains today, without waiting for the long-term High Speed ISOBUS solution.

</details>


### [6] [6G NTN Waveforms: A Comparison of OTFS, AFDM and OCDM in LEO Satellite Channels](https://arxiv.org/abs/2602.09834)
*Baidyanath Mandal,Aniruddha Chandra,Rastislav Roka,Jarosław Wojtun,Jan Kelner,Cezary Ziołkowski*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Sixth generation (6G) physical layer (PHY) is evolving beyond the legacy orthogonal frequency division multiplexing (OFDM)-based waveforms. In this paper, we compare the bit error rate (BER) performance of three beyond-OFDM waveforms, namely, orthogonal time-frequency-space (OTFS) modulation, affine frequency division multiplexing (AFDM), and orthogonal chirp division multiplexing (OCDM), which are particularly suitable for the highly mobile non-terrestrial network (NTN) vertical of 6G. In order to characterize the effect of mobility and Doppler shift in low Earth orbit (LEO) satellites, we performed BER comparisons over four different NTN tapped-delay-line (TDL) models, TDL-A, TDL-B, TDL-C, and TDL-D, as specified in the 3rd generation partnership project (3GPP) technical report TR 38.811. After channel equalization, a minimum mean squared error with successive detection (MMSE-SD) algorithm was used to enhance the BER performance. It was found that AFDM and OTFS consistently outperformed OCDM across all TDL models, while AFDM performed better than OTFS in TDL-B and TDL-C, in the high signal-to-noise ratio (SNR) regime. The complete simulation framework is made available as an open-source code for quick validation and further development.

</details>


### [7] [SCOPE: A Training-Free Online 3D Deployment for UAV-BSs with Theoretical Analysis and Comparative Study](https://arxiv.org/abs/2602.09971)
*Chuan-Chi Lai*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Unmanned Aerial Vehicle (UAV)-mounted Base Stations (UAV-BSs) offer a flexible solution for serving ground users in temporary hotspot scenarios. However, efficiently deploying UAV-BSs to satisfy heterogeneous user distributions remains a challenging optimization problem. While recent data-driven approaches, particularly Deep Reinforcement Learning (DRL), have shown promise in dynamic environments, they often suffer from prohibitive training overhead, poor generalization to topology changes, and high computational complexity. To address these limitations, this paper proposes Satisfaction-driven Coverage Optimization via Perimeter Extraction (SCOPE), a training-free and online 3D deployment framework. Unlike heuristic baselines that rely on fixed-altitude assumptions, SCOPE integrates a perimeter extraction mechanism with the Smallest Enclosing Circle (SEC) algorithm to dynamically optimize 3D UAV positions. Theoretically, we provide a rigorous convergence proof of the proposed algorithm and derive its polynomial time complexity of $O(N^2 \log N)$. Experimentally, we conduct a comprehensive comparative study against state-of-the-art DRL baselines (e.g., PPO). Simulation results demonstrate that SCOPE achieves comparable user satisfaction to DRL methods but significantly lower computational latency (milliseconds vs. hours of training) and superior energy efficiency, making it an ideal solution for real-time, on-demand emergency deployment.

</details>


### [8] [ORCHID: Fairness-Aware Orchestration in Mission-Critical Air-Ground Integrated Networks](https://arxiv.org/abs/2602.09994)
*Chuan-Chi Lai,Chi Jai Choy*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In the era of 6G Air-Ground Integrated Networks (AGINs), Unmanned Aerial Vehicles (UAVs) are pivotal for providing on-demand wireless coverage in mission-critical environments, such as post-disaster rescue operations. However, traditional Deep Reinforcement Learning (DRL) approaches for multi-UAV orchestration often face critical challenges: instability due to the non-stationarity of multi-agent environments and the difficulty of balancing energy efficiency with service equity. To address these issues, this paper proposes ORCHID (Orchestration of Resilient Coverage via Hybrid Intelligent Deployment), a novel stability-enhanced two-stage learning framework. First, ORCHID leverages a GBS-aware topology partitioning strategy to mitigate the exploration cold-start problem. Second, we introduce a Reset-and-Finetune (R\&F) mechanism within the MAPPO architecture that stabilizes the learning process via synchronized learning rate decay and optimizer state resetting. This mechanism effectively suppresses gradient variance to prevent policy degradation, thereby ensuring algorithmic resilience in dynamic environments. Furthermore, we uncover a counter-intuitive efficiency-fairness synergy: contrary to the conventional trade-off, our results demonstrate that the proposed Max-Min Fairness (MMF) design not only guarantees service for cell-edge users but also achieves superior energy efficiency compared to Proportional Fairness (PF), which tends to converge to suboptimal greedy equilibria. Extensive experiments confirm that ORCHID occupies a superior Pareto-dominant position compared to state-of-the-art baselines, ensuring robust convergence and resilient connectivity in mission-critical scenarios.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [9] [LingxiDiagBench: A Multi-Agent Framework for Benchmarking LLMs in Chinese Psychiatric Consultation and Diagnosis](https://arxiv.org/abs/2602.09379)
*Shihao Xu,Tiancheng Zhou,Jiatong Ma,Yanli Ding,Yiming Yan,Ming Xiao,Guoyi Li,Haiyang Geng,Yunyun Han,Jianhua Chen,Yafeng Deng*

Main category: cs.MA

TL;DR: 针对精神疾病诊断中AI应用瓶颈，研究者推出LingxiDiagBench基准测试和LingxiDiag-16K合成对话数据集，评估LLM在诊断中的表现，发现二元分类效果佳但复杂场景能力不足，动态咨询效率低下且提问质量与诊断准确度关联有限。


<details>
  <summary>Details</summary>
Motivation: 全球精神障碍高发，但精神科医生短缺和主观诊断导致评估延误。AI辅助诊断缺乏同时支持真实患者模拟、临床认证标签及动态对话的基准测试，阻碍进展。

Method: 构建LingxiDiagBench，包含16,000条EMR对齐的合成咨询对话数据集LingxiDiag-16K，覆盖12类ICD-10疾病；评估LLM在静态诊断推理和动态多轮咨询的性能，并进行广泛实验比较最新LLM。

Result: LLM在抑郁-焦虑二元分类准确率达92.3%，但共病识别仅43.0%、12类鉴别诊断仅28.5%；动态咨询效果不如静态评估，显示信息收集策略无效；LLM评估的咨询质量与诊断准确度仅中度相关，良好结构提问不足以保证正确诊断。

Conclusion: LLM在精神疾病诊断中面临复杂场景挑战，需优化信息收集方法；发布数据集和评估框架促进可复现研究，强调未来应提升模型在处理共病和多轮互动中的能力。

Abstract: Mental disorders are highly prevalent worldwide, but the shortage of psychiatrists and the inherent subjectivity of interview-based diagnosis create substantial barriers to timely and consistent mental-health assessment. Progress in AI-assisted psychiatric diagnosis is constrained by the absence of benchmarks that simultaneously provide realistic patient simulation, clinician-verified diagnostic labels, and support for dynamic multi-turn consultation. We present LingxiDiagBench, a large-scale multi-agent benchmark that evaluates LLMs on both static diagnostic inference and dynamic multi-turn psychiatric consultation in Chinese. At its core is LingxiDiag-16K, a dataset of 16,000 EMR-aligned synthetic consultation dialogues designed to reproduce real clinical demographic and diagnostic distributions across 12 ICD-10 psychiatric categories. Through extensive experiments across state-of-the-art LLMs, we establish key findings: (1) although LLMs achieve high accuracy on binary depression--anxiety classification (up to 92.3%), performance deteriorates substantially for depression--anxiety comorbidity recognition (43.0%) and 12-way differential diagnosis (28.5%); (2) dynamic consultation often underperforms static evaluation, indicating that ineffective information-gathering strategies significantly impair downstream diagnostic reasoning; (3) consultation quality assessed by LLM-as-a-Judge shows only moderate correlation with diagnostic accuracy, suggesting that well-structured questioning alone does not ensure correct diagnostic decisions. We release LingxiDiag-16K and the full evaluation framework to support reproducible research at https://github.com/Lingxi-mental-health/LingxiDiagBench.

</details>


### [10] [Dieu khien he da tac tu](https://arxiv.org/abs/2602.09412)
*Minh Hoang Trinh,Hieu Minh Nguyen*

Main category: cs.MA

TL;DR: 本书系统介绍多智能体系统控制基础原理，填补该领域教材空缺。内容源自河内理工大学教学实践，涵盖图论基础、线性共识算法及应用研究三大部分。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统控制研究虽发展迅速且应用广泛，但系统化阐述基础原理的教材匮乏，尤其非英语资源稀缺。本书旨在填补这一空白并提供教学支持。

Method: 采用渐进式教学法：先通过图论引入系统基础；再解析线性共识算法设计与分析；最后探讨编队控制、分布式优化等应用。各章附研究注释、习题及延伸阅读。

Result: 形成分层教材框架，理论深度趋近研究水平但保持逐步解析的易懂性。已在河内理工大学课程中验证教学有效性，覆盖持续热点的核心议题。

Conclusion: 本书为多智能体控制领域提供系统性教学资源，未来将通过读者反馈持续优化。强调从原理到应用的完整知识链条构建。

Abstract: Since the early 2000s, control of multiagent systems has attracted significant research interest, with applications ranging from natural collective behaviors and social dynamics to engineered systems such as autonomous vehicles, sensor networks, and smart grids. Although research on multi-agent systems has diversified into numerous specialized directions, textbooks -- including those in English -- that provide a systematic treatment of the fundamental principles of multi-agent system control remain scarce. The material presented in this book has been developed and used in teaching since 2021, initially as a concise Vietnamese-language reference for the courses Networked Control Systems and Control of Multi-Agent Systems at Hanoi University of Science and Technology. The book focuses on a selection of fundamental topics of broad and continuing interest in the field. The complexity of several topics is asymptotic to that encountered in research-level studies, however, the analysis is presented in a step-by-step manner to facilitate access to commonly used methods and tools.
  The material is divided into three main parts. Part I introduces multiagent systems and basic graph-theoretic concepts. Part II addresses the design and analysis of linear consensus algorithms. Part III covers selected applications and research directions, including formation control, network localization, distributed optimization, opinion dynamics, and matrix-weighted networks. Each chapter concludes with notes on notable researchers in this field, further reading, and exercises.
  This book cannot be completed without the encouragement, support and suggestions from families, colleagues and friends. The authors appreciate feedback from readers to further improve the content of the book.

</details>


### [11] [Tiny Moves: Game-based Hypothesis Refinement](https://arxiv.org/abs/2602.09801)
*Agnieszka Dobrowolska,Rogier Hintzen,Martin Balla,Karl Gemayel,Sabine Reichert,Thomas Charman,Jen Ning Lim,Lindsay Edwards,Anna Gogleva*

Main category: cs.MA

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Most machine learning approaches to scientific discovery frame hypotheses as end-to-end predictions, obscuring the incremental structure of scientific reasoning. We propose The Hypothesis Game, a symbolic formalism for hypothesis refinement in which LLM agents operate on a shared hypothesis state using a fixed grammar of reasoning moves. The framework is motivated by the observation that scientific progress often proceeds through small, localized revisions, grounded in domain context, rather than extensive rewrites. We instantiate a minimal game with LLM agents and evaluate it on pathway-level mechanistic refinement tasks. In the primary setting of corruption recovery, where hypotheses contain controlled errors, the game-based approach consistently removes more errors and achieves higher precision than strong prompting baselines, while preserving valid structure through incremental edits. In a secondary reconstruction setting from partial cues, it performs comparably to the strongest baseline, indicating that explicit move-based refinement remains competitive even when ground-truth recovery is difficult. These findings support game-based reasoning as a principled route to more controllable, interpretable, and transferable hypothesis refinement systems for scientific discovery.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [Accelerating Post-Quantum Cryptography via LLM-Driven Hardware-Software Co-Design](https://arxiv.org/abs/2602.09410)
*Yuchao Liao,Tosiron Adegbija,Roman Lysecky*

Main category: cs.AR

TL;DR: 本文提出利用大型语言模型加速后量子密码的FPGA硬件设计框架，针对FALCON签名方案实现软硬件协同设计自动化，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 后量子密码算法计算复杂，硬件实现效率低下，难以应对量子威胁，需创新方法加速设计与优化。

Method: 开发新型框架，利用LLM分析PQC算法结构，识别性能瓶颈组件，自动化生成FPGA硬件描述候选方案，并与传统HLS方法在FALCON内核上进行比较。

Result: LLM驱动合成的加速器内核执行时间提速最高达2.6倍，临界路径缩短，但资源利用率和功耗存在权衡，需人工干预优化。

Conclusion: LLM可大幅减少设计迭代时间和开发成本，自动化FPGA加速器设计，为后量子密码提供快速适应性的新方向，提升安全性效率。

Abstract: Post-quantum cryptography (PQC) is crucial for securing data against emerging quantum threats. However, its algorithms are computationally complex and difficult to implement efficiently on hardware. In this paper, we explore the potential of Large Language Models (LLMs) to accelerate the hardware-software co-design process for PQC, with a focus on the FALCON digital signature scheme. We present a novel framework that leverages LLMs to analyze PQC algorithms, identify performance-critical components, and generate candidate hardware descriptions for FPGA implementation. We present the first quantitative comparison between LLM-driven synthesis and conventional HLS-based approaches for low-level compute-intensive kernels in FALCON, showing that human-in-the-loop LLM-generated accelerators can achieve up to 2.6x speedup in kernel execution time with shorter critical paths, while highlighting trade-offs in resource utilization and power consumption. Our results suggest that LLMs can minimize design effort and development time by automating FPGA accelerator design iterations for PQC algorithms, offering a promising new direction for rapid and adaptive PQC accelerator design on FPGAs.

</details>


### [13] [Development of an Energy-Efficient and Real-Time Data Movement Strategy for Next-Generation Heterogeneous Mixed-Criticality Systems](https://arxiv.org/abs/2602.09554)
*Thomas Benz*

Main category: cs.AR

TL;DR: 工业ACES领域面临摩尔定律停滞的挑战，需通过异构加速器和内存协同设计实现高效能计算。


<details>
  <summary>Details</summary>
Motivation: 汽车、机器人与航天等工业领域对自主性、连接性等需求激增，但硬件技术缩放受限，导致计算资源压力增大，亟需提升能效并解决混合关键性系统问题。

Method: 提出内存系统与用例、计算单元及加速器的协同设计方案，优化资源分配以减少关键任务间的资源竞争。

Result: 该设计预期提升计算效能与能效，支撑大规模不规则数据处理，满足混合关键性系统对可预测性的严苛需求。

Conclusion: 协同设计是应对ACES计算挑战的核心策略，须整合内存、计算单元及用例以实现高性能与高效率目标。

Abstract: Industrial domains such as automotive, robotics, and aerospace are rapidly evolving to satisfy the increasing demand for machine-learning-driven Autonomy, Connectivity, Electrification, and Shared mobility (ACES). This paradigm shift inherently and significantly increases the requirement for onboard computing performance and high-performance communication infrastructure. At the same time, Moore's Law and Dennard Scaling are grinding to a halt, in turn, driving computing systems to larger scales and higher levels of heterogeneity and specialization, through application-specific hardware accelerators, instead of relying on technological scaling only. Approaching ACES requires this substantial amount of compute at an increasingly high energy-efficiency, since most use cases are fundamentally resource-bound. This increase in compute performance and heterogeneity goes hand in hand with a growing demand for high memory bandwidth and capacity as the driving applications grow in complexity, operating on huge and progressively irregular data sets and further requiring a steady influx of sensor data, increasing pressure both on on-chip and off-chip interconnect systems. Further, ACES combines real-time time-critical with general compute tasks on the same physical platform, sharing communication, storage, and micro-architectural resources. These heterogeneous mixed-criticality systems (MCSs) place additional pressure on the interconnect, demanding minimal contention between the different criticality levels to sustain a high degree of predictability. Fulfilling the performance and energy-efficiency requirements across a wide range of industrial applications requires a carefully co-designed process of the memory system with the use cases as well as the compute units and accelerators.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [14] [ALPHA-PIM: Analysis of Linear Algebraic Processing for High-Performance Graph Applications on a Real Processing-In-Memory System](https://arxiv.org/abs/2602.09174)
*Marzieh Barkhordar,Alireza Tabatabaeian,Mohammad Sadrosadati,Christina Giannoula,Juan Gomez Luna,Izzat El Hajj,Onur Mutlu,Alaa R. Alameldeen*

Main category: cs.DC

TL;DR: 本文探讨在UPMEM真实内存计算系统上运行图算法的性能，与CPU/GPU对比，揭示硬件限制并指明优化方向


<details>
  <summary>Details</summary>
Motivation: 传统CPU/GPU处理图数据时存在内存瓶颈，内存计算(PIM)虽能缓解但缺乏真实系统验证

Method: 在UPMEM的PIM架构上实现典型图算法，分析性能瓶颈并与CPU/GPU基准对比

Result: 发现优化PIM核心间数据划分至关重要，指出硬件需提升指令级并行、改进DMA引擎、建立核心间直接互联

Conclusion:  upgraded未来的PIM硬件设计需强化计算/存储/通信子系统，重点降低数据传输开销

Abstract: Processing large-scale graph datasets is computationally intensive and time-consuming. Processor-centric CPU and GPU architectures, commonly used for graph applications, often face bottlenecks caused by extensive data movement between the processor and memory units due to low data reuse. As a result, these applications are often memory-bound, limiting both performance and energy efficiency due to excessive data transfers. Processing-In-Memory (PIM) offers a promising approach to mitigate data movement bottlenecks by integrating computation directly within or near memory. Although several previous studies have introduced custom PIM proposals for graph processing, they do not leverage real-world PIM systems.
  This work aims to explore the capabilities and characteristics of common graph algorithms on a real-world PIM system to accelerate data-intensive graph workloads. To this end, we (1) implement representative graph algorithms on UPMEM's general-purpose PIM architecture; (2) characterize their performance and identify key bottlenecks; (3) compare results against CPU and GPU baselines; and (4) derive insights to guide future PIM hardware design.
  Our study underscores the importance of selecting optimal data partitioning strategies across PIM cores to maximize performance. Additionally, we identify critical hardware limitations in current PIM architectures and emphasize the need for future enhancements across computation, memory, and communication subsystems. Key opportunities for improvement include increasing instruction-level parallelism, developing improved DMA engines with non-blocking capabilities, and enabling direct interconnection networks among PIM cores to reduce data transfer overheads.

</details>


### [15] [LLM-CoOpt: A Co-Design and Optimization Framework for Efficient LLM Inference on Heterogeneous Platforms](https://arxiv.org/abs/2602.09323)
*Jie Kong,Wei Wang,Jiehan Zhou,Chen Yu*

Main category: cs.DC

TL;DR: 提出LLM-CoOpt框架，通过算法硬件协同设计优化大型语言模型推理性能，包括键值缓存优化、分组查询注意力和分页注意力策略，提升吞吐uture能力量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型推理中的内存带宽瓶颈、计算冗余和长序列处理效率低下问题。

Method: 集成三大策略：Opt-KV优化KV缓存读写路径并使用FP8量化减少内存占用；Opt-GQA重构查询分组注意力降低计算复杂度；Opt-Pa分块管理长序列并通过懒计算缓解内存压力。

Result: 在LLaMa-13BGPTQ模型上实验显示：推理吞吐量最高提升13.43%，延迟降低16.79%，模型准确性维持不变。

Conclusion: LLM-CoOpt为大规模语言模型实际应用提供了高效实用的优化路径。

Abstract: Major challenges in LLMs inference remain frequent memory bandwidth bottlenecks, computational redundancy, and inefficiencies in long-sequence processing. To address these issues, we propose LLM-CoOpt, a comprehensive algorithmhardware co-design framework aimed at improving both throughput and latency in LLM inference. LLM-CoOpt integrates three key strategies: (1) Key-Value Cache Optimization, termed Opt-KV, which improves memory access efficiency by optimizing both KV cache write and read paths, and introduces FP8 quantization to reduce memory footprint while maintaining accuracy; (2) Grouped-Query Attention for Computational Efficiency, termed Opt-GQA, which reduces the overall computational complexity by restructuring multi-head self-attention into grouped-query attention with shared key-value projections, enabling higher throughput and lower resource consumption; (3) Paged Attention for Long- Sequence Processing, termed Opt-Pa, which adopts a two-step strategy to first segment long sequences into manageable chunks and then apply lazy memory mapping and computation, significantly reducing memory pressure and improving performance on long-context inputs.Experiments on the LLaMa-13BGPTQ model demonstrate that LLM-CoOpt increases inference throughput by up to 13.43%, reduces latency by up to 16.79%, and maintains model accuracy. These results confirm that LLM-CoOpt provides a practical, high-performance optimization path for real-world inference of large-scale language models.

</details>


### [16] [The Coordination Criterion](https://arxiv.org/abs/2602.09435)
*Joseph M. Hellerstein*

Main category: cs.DC

TL;DR: 论文提出协调性标准：当分布式规范在历史扩展下具有单调性时，方可实现无协调执行，此标准统一了CAP定理、CALM框架等经典结论。


<details>
  <summary>Details</summary>
Motivation: 探究分布式规范的协调核心需求：明确何时协调是规范本身固有属性，而非特定协议或实现策略的附加要求。

Method: 基于异步消息传递模型，利用Lamport历史（happens-before偏序执行）和规范定义的观察结果，构建与实现无关的协调性判定框架。

Result: 确立充要条件：规范可无协调实现当且仅当其在观察结果序关系下满足历史扩展单调性，为协调存在性划定严格边界。

Conclusion: 该协调标准揭示了CAP定理、共识任务、快照等经典问题本质同源，均为同一语义现象的不同实例，为分布式系统设计提供统一理论基石。

Abstract: When is coordination intrinsically required by a distributed specification, rather than imposed by a particular protocol or implementation strategy? We give a general answer using minimal assumptions. In an asynchronous message-passing model, we show that a specification admits a coordination-free implementation if and only if it is monotone with respect to history extension under an appropriate order on observable outcomes.
  This Coordination Criterion is stated directly over Lamport histories -- partially ordered executions under happens-before -- and specification-defined observable outcomes, without assuming any particular programming language, object implementation, or protocol structure. It yields a sharp boundary between specifications that can be implemented without coordination and those for which coordination is unavoidable. The criterion provides a uniform explanation for a range of classical results, including CAP-style impossibility, CALM-style coordination-freedom, agreement and snapshot tasks, transactional isolation levels, and invariant confluence -- all instances of the same underlying semantic phenomenon.

</details>


### [17] [High-performance Vector-length Agnostic Quantum Circuit Simulations on ARM Processors](https://arxiv.org/abs/2602.09604)
*Ruimin Shi,Gabin Schieffer,Pei-Hung Lin,Maya Gokhale,Andreas Herten,Ivy Peng*

Main category: cs.DC

TL;DR: 研究针对ARM SVE和RISC-V RVV向量架构，开发向量长度无关设计以提升量子态模拟性能，在三种ARM处理器上实现最高4.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 探索量子态模拟在向量长度无关架构中是否具备高性能可移植性，解决新兴处理器架构下的向量化效率问题。

Method: 提出向量长度自适应内存布局调整、负载缓冲、细粒度循环控制和门融合算术强度适应等优化技术，在Google Qsim实现并评估36量子位电路于NVIDIA Grace、AWS Graviton3和Fujitsu A64FX平台，定义新指标量化向量化活动。

Result: 在A64FX实现最高4.5倍加速，Grace获得2.5倍加速，Graviton达1.5倍加速，验证了向量长度无关设计的性能可移植性并为未来架构提供通用启示。

Conclusion: 单代码源向量长度无关量子模拟实现了跨平台显著性能提升，证明该设计可有效应对多样化向量架构挑战。

Abstract: ARM SVE and RISC-V RVV are emerging vector architectures in high-end processors that support vectorization of flexible vector length. In this work, we leverage an important workload for quantum computing, quantum state-vector simulations, to understand whether high-performance portability can be achieved in a vector-length agnostic (VLA) design. We propose a VLA design and optimization techniques critical for achieving high performance, including VLEN-adaptive memory layout adjustment, load buffering, fine-grained loop control, and gate fusion-based arithmetic intensity adaptation. We provide an implementation in Google's Qsim and evaluate five quantum circuits of up to 36 qubits on three ARM processors, including NVIDIA Grace, AWS Graviton3, and Fujitsu A64FX. By defining new metrics and PMU events to quantify vectorization activities, we draw generic insights for future VLA designs. Our single-source implementation of VLA quantum simulations achieves up to 4.5x speedup on A64FX, 2.5x speedup on Grace, and 1.5x speedup on Graviton.

</details>


### [18] [Revealing the Challenges of Attention-FFN Disaggregation for Modern MoE Models and Hardware Systems](https://arxiv.org/abs/2602.09721)
*Guowei Liu,Hongming Li,Yaning Guo,Yongxi Lyu,Mo Zhou,Yi Liu,Zhaogeng Li,Yanpeng Wang*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deploying large-scale MoE models presents challenges in memory capacity and bandwidth for expert activation. While Attention-FFN Disaggregation (AFD) has emerged as a potential architecture to decouple compute and memory resources, its performance boundaries compared to standard large-scale Expert Parallelism (EP) remain underexplored. In this paper, we conduct a systematic analysis of AFD by extending the roofline model to the communication level, correlating interconnect bandwidth, arithmetic intensity, and Hardware FLOPS Utilization (HFU). Our analysis reveals a dead zone on standard clusters: increasing FFN instance count fails to improve HFU as computational workload is capped by scale-out bandwidth, causing operator active time to shrink relative to the fixed latency budget. We further show that AFD's discrete node-level scaling incurs higher imbalance penalties than EP's continuous batch adjustment. Nevertheless, these limitations diminish under specific conditions: Superpod-class hardware with abundant interconnect bandwidth and models with coarse-grained experts and lower sparsity are more likely to benefit from AFD. These findings position AFD as a promising approach for specific hardware-model combinations rather than a universal solution.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [RuleFlow : Generating Reusable Program Optimizations with LLMs](https://arxiv.org/abs/2602.09051)
*Avaljot Singh,Dushyant Bharadwaj,Stefanos Baziotis,Kaushik Varadharajan,Charith Mendis*

Main category: cs.SE

TL;DR: 论文提出RuleFlow混合框架，通过三阶段方法优化Pandas程序：探索单程序优化→转换为通用规则→编译器自动部署，在PandasBench上实现最高4.3倍和1914.9倍加速


<details>
  <summary>Details</summary>
Motivation: 现有Pandas优化技术存在两极化缺陷：编译器方案轻量但优化有限，系统方案全面但笨重；LLM可实现复杂优化但不可靠、成本高且效率低下

Method: 三阶段混合架构：1）探索阶段利用LLM生成单程序优化方案；2）桥接阶段将优化转化为通用重写规则；3）部署阶段通过编译器自动应用规则，避免重复依赖LLM

Result: 在PandasBench测试中：相比编译器最优方案Dias加速4.3倍，超越系统最优方案Modin 1914.9倍，成为新SOTA框架

Conclusion: RuleFlow通过分离优化发现与部署流程并建立规则转化机制，平衡了优化能力与执行效率，为Pandas程序提供了可靠且高效的自动化优化方案

Abstract: Optimizing Pandas programs is a challenging problem. Existing systems and compiler-based approaches offer reliability but are either heavyweight or support only a limited set of optimizations. Conversely, using LLMs in a per-program optimization methodology can synthesize nontrivial optimizations, but is unreliable, expensive, and offers a low yield. In this work, we introduce a hybrid approach that works in a 3-stage manner that decouples discovery from deployment and connects them via a novel bridge. First, it discovers per-program optimizations (discovery). Second, they are converted into generalised rewrite rules (bridge). Finally, these rules are incorporated into a compiler that can automatically apply them wherever applicable, eliminating repeated reliance on LLMs (deployment). We demonstrate that RuleFlow is the new state-of-the-art (SOTA) Pandas optimization framework on PandasBench, a challenging Pandas benchmark consisting of Python notebooks. Across these notebooks, we achieve a speedup of up to 4.3x over Dias, the previous compiler-based SOTA, and 1914.9x over Modin, the previous systems-based SOTA.
  Our code is available at https://github.com/ADAPT-uiuc/RuleFlow.

</details>


### [20] [Predicting Open Source Software Sustainability with Deep Temporal Neural Hierarchical Architectures and Explainable AI](https://arxiv.org/abs/2602.09064)
*S M Rakib Ul Karim,Wenyi Lu,Enock Kasaadha,Sean Goggins*

Main category: cs.SE

TL;DR: 提出一个分层预测框架，建模开放源码软件项目的生命周期阶段，结合多维指标和可解释AI技术，实现高精度分类并强调贡献和社区的动态作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖项目年龄或累计活动等静态指标，无法揭示OSS项目随时间的可持续发展轨迹；亟需评估其健康和动态组织，以支持利益相关者进行规模化管理。

Method: 使用分层框架，基于社会技术分类将OSS项目划分为不同生命周期阶段；整合工程化表格指标与24个月时间序列数据，采用分类管道并融入可解释AI分析特征贡献。

Result: 在大规模OSS仓库语料库评估中，生命周期阶段分类准确率超过94%；特征归因分析一致表明贡献活动和社区相关特征为主导信号。

Conclusion: 该方法突出了集体参与动态在OSS可持续性中的核心角色，推动了多维度项目组织的理解，为实践提供了透明且高效的评估工具。

Abstract: Open Source Software (OSS) projects follow diverse lifecycle trajectories shaped by evolving patterns of contribution, coordination, and community engagement. Understanding these trajectories is essential for stakeholders seeking to assess project organization and health at scale. However, prior work has largely relied on static or aggregated metrics, such as project age or cumulative activity, providing limited insight into how OSS sustainability unfolds over time. In this paper, we propose a hierarchical predictive framework that models OSS projects as belonging to distinct lifecycle stages grounded in established socio-technical categorizations of OSS development. Rather than treating sustainability solely as project longevity, these lifecycle stages operationalize sustainability as a multidimensional construct integrating contribution activity, community participation, and maintenance dynamics. The framework combines engineered tabular indicators with 24-month temporal activity sequences and employs a multi-stage classification pipeline to distinguish lifecycle stages associated with different coordination and participation regimes. To support transparency, we incorporate explainable AI techniques to examine the relative contribution of feature categories to model predictions. Evaluated on a large corpus of OSS repositories, the proposed approach achieves over 94\% overall accuracy in lifecycle stage classification. Attribution analyses consistently identify contribution activity and community-related features as dominant signals, highlighting the central role of collective participation dynamics.

</details>


### [21] [DRAGON: Robust Classification for Very Large Collections of Software Repositories](https://arxiv.org/abs/2602.09071)
*Stefano Balla,Stefano Zacchiroli,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes*

Main category: cs.SE

TL;DR: 本文提出DRAGON仓库分类器，利用轻量信号改进大规模软件库分类，在README缺失下仍有效，性能提升显著并发布大型数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖README文件和元数据，但这些常缺失，导致大规模软件集合中的应用受限。

Method: 设计DRAGON分类器，仅基于版本控制系统中的文件和目录名称（可选README），兼顾轻量信号。

Result: F1@5指标从54.8%提升至60.8%，优于现有技术；无README时性能仅下降6%；错误多为语义接近的误判；同时发布迄今最大开放数据集（82.5万仓库）。

Conclusion: DRAGON提高了文档稀缺环境下的鲁棒性，误判标签的语义接近特性增强了实际搜索价值，适用于大型软件集合的自动化管理。

Abstract: The ability to automatically classify source code repositories with ''topics'' that reflect their content and purpose is very useful, especially when navigating or searching through large software collections. However, existing approaches often rely heavily on README files and other metadata, which are frequently missing, limiting their applicability in real-world large-scale settings. We present DRAGON, a repository classifier designed for very large and diverse software collections. It operates entirely on lightweight signals commonly stored in version control systems: file and directory names, and optionally the README when available. In repository classification at scale, DRAGON improves F1@5 from 54.8% to 60.8%, surpassing the state of the art. DRAGON remains effective even when README files are absent, with performance degrading by only 6% w.r.t. when they are present. This robustness makes it practical for real-world settings where documentation is sparse or inconsistent. Furthermore, many of the remaining classification errors are near misses, where predicted labels are semantically close to the correct topics. This property increases the practical value of the predictions in real-world software collections, where suggesting a few related topics can still guide search and discovery. As a byproduct of developing DRAGON, we also release the largest open dataset to date for repository classification, consisting of 825 thousand repositories with associated ground-truth topics, sourced from the Software Heritage archive, providing a foundation for future large-scale and language-agnostic research on software repository understanding.

</details>


### [22] [Towards an OSF-based Registered Report Template for Software Engineering Controlled Experiments](https://arxiv.org/abs/2602.09292)
*Ana B. M. Bett,Thais S. Nepomuceno,Edson OliveiraJr,Maria Teresa Baldassarre,Valdemar V. Graciano Neto,Marcos Kalinowski*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Context: The empirical software engineering (ESE) community has contributed to improving experimentation over the years. However, there is still a lack of rigor in describing controlled experiments, hindering reproducibility and transparency. Registered Reports (RR) have been discussed in the ESE community to address these issues. A RR registers a study's hypotheses, methods, and/or analyses before execution, involving peer review and potential acceptance before data collection. This helps mitigate problematic practices such as p-hacking, publication bias, and inappropriate post hoc analysis. Objective: This paper presents initial results toward establishing an RR template for Software Engineering controlled experiments using the Open Science Framework (OSF). Method: We analyzed templates of selected OSF RR types in light of documentation guidelines for controlled experiments. Results: The observed lack of rigor motivated our investigation of OSF-based RR types. Our analysis showed that, although one of the RR types aligned with many of the documentation suggestions contained in the guidelines, none of them covered the guidelines comprehensively. The study also highlights limitations in OSF RR template customization. Conclusion: Despite progress in ESE, planning and documenting experiments still lack rigor, compromising reproducibility. Adopting OSF-based RRs is proposed. However, no currently available RR type fully satisfies the guidelines. Establishing RR-specific guidelines for SE is deemed essential.

</details>


### [23] [Cross-Project Flakiness: A Case Study of the OpenStack Ecosystem](https://arxiv.org/abs/2602.09311)
*Tao Xiao,Dong Wang,Shane McIntosh,Hideaki Hata,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 本文通过分析649个OpenStack项目，揭示了测试不稳定性在生态系统层面的广泛影响，包括55%的项目受跨项目浮动影响及单元测试的常见浮动问题。


<details>
  <summary>Details</summary>
Motivation: 探索测试不稳定性在更广泛生态系统中的影响，传统研究多集中于单个项目内部，而跨项目和不一致的浮动问题鲜有涉及，这损害开发信任并浪费资源。

Method: 对OpenStack生态系统进行实证研究，分析649个项目，聚焦跨项目浮动（影响多个项目）和不一致浮动（不同项目表现不一），定性分析CI竞态条件、构建配置不一致和依赖不匹配等原因。

Result: 识别1535个跨项目浮动测试和1105个不一致浮动测试；55%的项目受影响，显著增加审查时间和计算成本；70%的单元测试显示浮动，挑战隔离稳定性；主要原因包括CI竞态问题、配置差异和依赖问题。

Conclusion: 强调生态系统协作的重要性，建议标准化CI配置、改进测试隔离策略，以增强CI可靠性和资源效率。

Abstract: Automated regression testing is a cornerstone of modern software development, often contributing directly to code review and Continuous Integration (CI). Yet some tests suffer from flakiness, where their outcomes vary non-deterministically. Flakiness erodes developer trust in test results, wastes computational resources, and undermines CI reliability. While prior research has examined test flakiness within individual projects, its broader ecosystem-wide impact remains largely unexplored. In this paper, we present an empirical study of test flakiness in the OpenStack ecosystem, which focuses on (1) cross-project flakiness, where flaky tests impact multiple projects, and (2) inconsistent flakiness, where a test exhibits flakiness in some projects but remains stable in others. By analyzing 649 OpenStack projects, we identify 1,535 cross-project flaky tests and 1,105 inconsistently flaky tests. We find that cross-project flakiness affects 55% of OpenStack projects and significantly increases both review time and computational costs. Surprisingly, 70% of unit tests exhibit cross-project flakiness, challenging the assumption that unit tests are inherently insulated from issues that span modules like integration and system-level tests. Through qualitative analysis, we observe that race conditions in CI, inconsistent build configurations, and dependency mismatches are the primary causes of inconsistent flakiness. These findings underline the need for better coordination across complex ecosystems, standardized CI configurations, and improved test isolation strategies.

</details>


### [24] [SWE-AGI: Benchmarking Specification-Driven Software Construction with MoonBit in the Era of Autonomous Agents](https://arxiv.org/abs/2602.09447)
*Zhirui Zhang,Hongbo Zhang,Haoxiang Fei,Zhiyuan Bao,Yubin Chen,Zhengyu Lei,Ziyue Liu,Yixuan Sun,Mingkun Xiao,Zihang Ye,Yu Zhang,Hongcheng Zhu,Yuxiang Wen,Heung-Yeung Shum*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Although large language models (LLMs) have demonstrated impressive coding capabilities, their ability to autonomously build production-scale software from explicit specifications remains an open question. We introduce SWE-AGI, an open-source benchmark for evaluating end-to-end, specification-driven construction of software systems written in MoonBit. SWE-AGI tasks require LLM-based agents to implement parsers, interpreters, binary decoders, and SAT solvers strictly from authoritative standards and RFCs under a fixed API scaffold. Each task involves implementing 1,000-10,000 lines of core logic, corresponding to weeks or months of engineering effort for an experienced human developer. By leveraging the nascent MoonBit ecosystem, SWE-AGI minimizes data leakage, forcing agents to rely on long-horizon architectural reasoning rather than code retrieval. Across frontier models, gpt-5.3-codex achieves the best overall performance (solving 19/22 tasks, 86.4%), outperforming claude-opus-4.6 (15/22, 68.2%), and kimi-2.5 exhibits the strongest performance among open-source models. Performance degrades sharply with increasing task difficulty, particularly on hard, specification-intensive systems. Behavioral analysis further reveals that as codebases scale, code reading, rather than writing, becomes the dominant bottleneck in AI-assisted development. Overall, while specification-driven autonomous software engineering is increasingly viable, substantial challenges remain before it can reliably support production-scale development.

</details>


### [25] [Toward Linking Declined Proposals and Source Code: An Exploratory Study on the Go Repository](https://arxiv.org/abs/2602.09467)
*Sota Nakashima,Masanari Kondo,Mahmoud Alfadel,Aly Ahmad,Toshihiro Nakae,Hidenori Matsuzaki*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Traceability links are key information sources for software developers, connecting software artifacts (e.g., linking requirements to the corresponding source code). In open-source software (OSS) projects, such links play an important role, particularly between the contributions (e.g., GitHub issues) and the corresponding source code. Through these links, developers can trace the discussions in contributions and uncover design rationales, constraints, and security concerns. Previous studies have mainly examined accepted contributions, while those declined after discussion have been overlooked. The discussions behind declined contributions contain valuable design rationales and implicit knowledge about software decision-making, as the reasons behind the decline often reveal the criteria used to judge what should or should not be implemented. In this study, we present the first attempt to establish traceability links between declined contributions and related source code. We propose an initial linking approach and conduct an empirical analysis of the generated links to discuss factors affecting link generation. As our dataset, we use proposals from the official Go repository, which are GitHub issues used to propose new features or language changes. To link declined proposals to source code, we designed an LLM-driven pipeline. Our results showed that the pipeline selected the correct granularity for each declined proposal with an accuracy of 0.836, and generated correct links at that granularity with a mean precision of 0.643. To clarify the challenges of linking declined proposals, we performed a failure analysis. In the declined proposals where the pipeline failed to generate links, the discussions were often redundant and lacked concrete information (e.g., how the feature should be implemented).

</details>


### [26] [SWE-Bench Mobile: Can Large Language Model Agents Develop Industry-Level Mobile Applications?](https://arxiv.org/abs/2602.09540)
*Muxin Tian,Zhe Wang,Blair Yang,Zhenwei Tang,Kunlun Zhu,Honghua Dong,Hanchen Li,Xinni Xie,Guangjing Wang,Jiaxuan You*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Can large language model agents develop industry-level mobile applications? We introduce \textbf{SWE-Bench Mobile}, a benchmark for evaluating coding agents on realistic software engineering tasks derived from a production iOS codebase. Unlike existing benchmarks that focus on isolated problems or bug fixes, SWE-Bench Mobile captures the full complexity of industrial development: multi-modal inputs (PRDs and Figma designs), a large-scale mixed Swift/Objective-C codebase, and comprehensive test suites. We evaluate 22 agent-model configurations across four coding agents -- three commercial (Cursor, Codex, Claude Code) and one open-source (OpenCode) -- and find that even the best configurations achieve only 12\% task success rate. Our analysis reveals that (1) agent design matters as much as model capability -- the same model shows up to 6$\times$ performance gap across agents, (2) commercial agents consistently outperform open-source alternatives, and (3) simple ``Defensive Programming'' prompts outperform complex ones by 7.4\%. These findings highlight a significant gap between current agent capabilities and industrial requirements, while providing actionable insights for practitioners and researchers. We release SWE-Bench Mobile as a \textit{hosted benchmark challenge} to prevent data contamination and ensure fair evaluation. The public leaderboard and development toolkit are available at https://swebenchmobile.com.

</details>


### [27] [Generative AI Adoption in an Energy Company: Exploring Challenges and Use Cases](https://arxiv.org/abs/2602.09846)
*Malik Abdul Sami,Zeeshan Rasheed,Meri Olenius,Muhammad Waseem,Kai-Kristian Kemell,Jussi Rasku,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Organisations are examining how generative AI can support their operational work and decision-making processes. This study investigates how employees in a energy company understand AI adoption and identify areas where AI and LLMs-based agentic workflows could assist daily activities. Data was collected in four weeks through sixteen semi-structured interviews across nine departments, supported by internal documents and researcher observations. The analysis identified areas where employees positioned AI as useful, including reporting work, forecasting, data handling, maintenance-related tasks, and anomaly detection. Participants also described how GenAI and LLM-based tools could be introduced through incremental steps that align with existing workflows. The study provides an overview view of AI adoption in the energy sector and offers a structured basis for identifying entry points for practical implementation and comparative research across industries.

</details>


### [28] [Immersion in the GitHub Universe: Scaling Coding Agents to Mastery](https://arxiv.org/abs/2602.09892)
*Jiale Zhao,Guoxin Chen,Fanzhe Meng,Minghao Li,Jie Chen,Hui Xu,Yongshuai Sun,Xin Zhao,Ruihua Song,Yuan Zhang,Peng Wang,Cheng Chen,Jirong Wen,Kai Jia*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Achieving mastery in real world software engineering tasks is fundamentally bottlenecked by the scarcity of large scale, high quality training data. Scaling such data has been limited by the complexity of environment setup, unit test generation, and problem statement curation. In this paper, we propose ScaleSWE, an automated, sandboxed multi agent workflow designed to construct high quality SWE data at scale. The system coordinates three specialized agents for environment setup, test creation, and problem description synthesis to process 6 million pull requests across 5200 repositories, producing Scale SWE Data: 100k verified SWE instances, the largest such dataset to date. It substantially surpasses existing real world datasets in repository diversity and reflects realistic task complexity. We further demonstrate the dataset utility for training by distilling 71498 high quality trajectories and finetuning Qwen30BA3BInstruct to produce ScaleSWE Agent. Our agent achieves a 64 resolve rate on SWE Bench Verified a nearly three fold improvement over the base model. ScaleSWE provides a scalable, reproducible approach for data construction to advance LLM based software engineering. Scale SWE will be publicly available.

</details>


### [29] [Operationalizing Human Values in the Requirements Engineering Process of Ethics-Aware Autonomous Systems](https://arxiv.org/abs/2602.09921)
*Everaldo Silva Júnior,Lina Marsso,Ricardo Caldas,Marsha Chechik,Genaína Nunes Rodrigues*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Operationalizing human values alongside functional and adaptation requirements remains challenging due to their ambiguous, pluralistic, and context-dependent nature. Explicit representations are needed to support the elicitation, analysis, and negotiation of value conflicts beyond traditional software engineering abstractions. In this work, we propose a requirements engineering approach for ethics-aware autonomous systems that captures human values as normative goals and aligns them with functional and adaptation goals. These goals are systematically operationalized into Social, Legal, Ethical, Empathetic, and Cultural (SLEEC) requirements, enabling automated well-formedness checking, conflict detection, and early design-time negotiation. We demonstrate the feasibility of the approach through a medical Body Sensor Network case study.

</details>


### [30] [JMigBench: A Benchmark for Evaluating LLMs on Source Code Migration (Java 8 to Java 11)](https://arxiv.org/abs/2602.09930)
*Nishil Amin,Zhiwei Fei,Xiang Li,Justyna Petke,He Ye*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We build a benchmark to evaluate large language models (LLMs) for source code migration tasks, specifically upgrading functions from Java 8 to Java 11. We first collected a dataset of function pairs from open-source repositories, but limitations in data quality led us to construct a refined dataset covering eight categories of deprecated APIs. Using this dataset, the Mistral Codestral model was evaluated with CodeBLEU and keyword-based metrics to measure lexical and semantic similarity as well as migration correctness. Results show that the evaluated model (Mistral Codestral) can handle trivial one-to-one API substitutions with moderate success, achieving identical migrations in 11.11% of the cases, but it struggles with more complex migrations such as CORBA or JAX-WS. These findings suggest Mistral Codestral can partially reduce developer effort by automating repetitive migration tasks but cannot yet replace humans within the scope of the JMigBench benchmark. The benchmark and analysis provide a foundation for future work on expanding datasets, refining prompting strategies, and improving migration performance across different LLMs.

</details>


### [31] [QEMI: A Quantum Software Stacks Testing Framework via Equivalence Modulo Inputs](https://arxiv.org/abs/2602.09942)
*Junjie Luo,Shangzhou Xia,Fuyuan Zhang,Jianjun Zhao*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As quantum algorithms and hardware continue to evolve, ensuring the correctness of the quantum software stack (QSS) has become increasingly important. However, testing QSSes remains challenging due to the oracle problem, i.e., the lack of a reliable ground truth for expected program behavior. Existing metamorphic testing approaches often rely on equivalent circuit transformations, backend modifications, or parameter tuning to address this issue. In this work, inspired by Equivalence Modulo Inputs (EMI), we propose Quantum EMI (QEMI), a new testing approach for QSSes. Our key contributions include: (1) a random quantum program generator that produces code with dead code based on quantum control-flow structures, and (2) an adaptation of the EMI technique from classical compiler testing to generate variants by removing dead code. By comparing the behavior of these variants, we can detect potential bugs in QSS implementations. We applied QEMI to Qiskit, Q#, and Cirq, and successfully identified 11 crash bugs and 1 behavioral inconsistency. QEMI expands the limited set of testing techniques available for quantum software stacks by going beyond structural transformations and incorporating semantics-preserving ones into quantum program analysis.

</details>


### [32] [Environment-in-the-Loop: Rethinking Code Migration with LLM-based Agents](https://arxiv.org/abs/2602.09944)
*Xiang Li,Zhiwei Fei,Ying Ma,Jerry Zhang,Sarro Federica,He Ye*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern software systems continuously undergo code upgrades to enhance functionality, security, and performance, and Large Language Models (LLMs) have demonstrated remarkable capabilities in code migration tasks. However, while research on automated code migration which including refactoring, API adaptation, and dependency updates has advanced rapidly, the exploration of the automated environment interaction that must accompany it remains relatively scarce. In practice, code and its environment are intricately intertwined. Relying solely on static analysis of the environment leads to an inadequate understanding of the target setting, prolongs feedback cycles, and consequently causes significant rework and project delays, thereby reducing overall efficiency. We contend that successful software evolution demands a holistic perspective that integrates both code and environment migration. To understand the current landscape and challenges, we first provide an overview of the status of automated environment construction. We then propose a novel framework paradigm that tightly integrates automated environment setup with the code migration workflow. Finally, we explore the challenges and future directions for automated environment interaction within the code migration domain. Our findings emphasize that without automated environment interaction, the automation of code migration is only half complete.

</details>
