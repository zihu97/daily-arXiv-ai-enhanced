{"id": "2602.21278", "categories": ["cs.AR", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.21278", "abs": "https://arxiv.org/abs/2602.21278", "authors": ["Xinxin Wang", "Lixian Yan", "Shuhan Liu", "Luke Upton", "Zhuoqi Cai", "Yiming Tan", "Shengman Li", "Koustav Jana", "Peijing Li", "Jesse Cirimelli-Low", "Thierry Tambe", "Matthew Guthaus", "H. -S. Philip Wong"], "title": "Heterogeneous Memory Design Exploration for AI Accelerators with a Gain Cell Memory Compiler", "comment": null, "summary": "As memory increasingly dominates system cost and energy, heterogeneous on-chip memory systems that combine technologies with complementary characteristics are becoming essential. Gain Cell RAM (GCRAM) offers higher density, lower power, and tunable retention, expanding the design space beyond conventional SRAM. To this end, we create an OpenGCRAM compiler supporting both SRAM and GCRAM. It generates macro-level designs and layouts for commercial CMOS processes and characterizes area, delay, and power across user-defined configurations. The tool enables systematic identification of optimal heterogeneous memory configurations for AI tasks under specified performance metrics."}
{"id": "2602.21251", "categories": ["cs.SE", "cs.AI", "cs.MA", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.21251", "abs": "https://arxiv.org/abs/2602.21251", "authors": ["Clemens Pohle"], "title": "AgenticTyper: Automated Typing of Legacy Software Projects Using Agentic AI", "comment": "Accepted at ICSE 2026 Student Research Competition (SRC)", "summary": "Legacy JavaScript systems lack type safety, making maintenance risky. While TypeScript can help, manually adding types is expensive. Previous automated typing research focuses on type inference but rarely addresses type checking setup, definition generation, bug identification, or behavioral correctness at repository scale. We present AgenticTyper, a Large Language Model (LLM)-based agentic system that addresses these gaps through iterative error correction and behavior preservation via transpilation comparison. Evaluation on two proprietary repositories (81K LOC) shows that AgenticTyper resolves all 633 initial type errors in 20 minutes, reducing manual effort from one working day."}
{"id": "2602.21343", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.21343", "abs": "https://arxiv.org/abs/2602.21343", "authors": ["Chao Feng", "Thomas Grubl", "Jan von der Assen", "Sandrin Raphael Hunkeler", "Linn Anna Spitz", "Gerome Bovet", "Burkhard Stiller"], "title": "UnlinkableDFL: a Practical Mixnet Protocol for Churn-Tolerant Decentralized FL Model Sharing", "comment": null, "summary": "Decentralized Federated Learning (DFL) eliminates the need for a central aggregator, but it can expose communication patterns that reveal participant identities. This work presents UnlinkableDFL, a DFL framework that combines a peer-based mixnet with fragment-based model aggregation to ensure unlinkability in fully decentralized settings. Model updates are divided into encrypted fragments, sent over separate multi-hop paths, and aggregated without using any identity information. A theoretical analysis indicates that relay and end-to-end unlinkability improve with larger mixing sets and longer paths, while convergence remains similar to standard FedAvg. A prototype implementation evaluates learning performance, latency, unlinkability, and resource usage. The results show that UnlinkableDFL converges reliably and adapts to node churn. Communication latency emerges as the main overhead, while memory and CPU usage stay moderate. These findings illustrate the balance between anonymity and system efficiency, demonstrating that strong unlinkability can be maintained in decentralized learning workflows."}
{"id": "2602.21404", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21404", "abs": "https://arxiv.org/abs/2602.21404", "authors": ["Shanshan Mao", "Peter Tino"], "title": "From Cooperation to Hierarchy: A Study of Dynamics of Hierarchy Emergence in a Multi-Agent System", "comment": "16 pages, 8 figures", "summary": "A central premise in evolutionary biology is that individual variation can generate information asymmetries that facilitate the emergence of hierarchical organisation. To examine this process, we develop an agent-based model (ABM) to identify the minimal conditions under which hierarchy arises in dynamic multi-agent systems, focusing on the roles of initial heterogeneity and mutation amplitude across generations. Hierarchical organisation is quantified using the Trophic Incoherence (TI) metric, which captures directional asymmetries in interaction networks. Our results show that even small individual differences can be amplified through repeated local interactions involving reproduction, competition, and cooperation, but that hierarchical order is markedly more sensitive to mutation amplitude than to initial heterogeneity. Across repeated trials, stable hierarchies reliably emerge only when mutation amplitude is sufficiently high, while initial heterogeneity primarily affects early formation rather than long-term persistence. Overall, these findings demonstrate how simple interaction rules can give rise to both the emergence and persistence of hierarchical organisation, providing a quantitative account of how structured inequality can develop from initially homogeneous populations."}
{"id": "2602.21411", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21411", "abs": "https://arxiv.org/abs/2602.21411", "authors": ["Marc Dufay", "Diana Ghinea", "Anton Paramonov"], "title": "General Convex Agreement with Near-Optimal Communication", "comment": "Working paper", "summary": "Convex Agreement (CA) strengthens Byzantine Agreement (BA) by requiring the output agreed upon to lie in the convex hull of the honest parties' inputs. This validity condition is motivated by practical aggregation tasks (e.g., robust learning or sensor fusion) where honest inputs need not coincide but should still constrain the decision. CA inherits BA lower bounds, and optimal synchronous round complexity is easy to obtain (e.g., via Byzantine Broadcast). The main challenge is \\emph{communication}: standard approaches for CA have a communication complexity of $Θ(Ln^2)$ for large $L$-bit inputs, leaving a gap in contrast to BA's lower bound of $Ω(Ln)$ bits. While recent work achieves optimal communication complexity of $O(Ln)$ for sufficiently large $L$ [GLW,PODC'25], translating this result to general convexity spaces remained an open problem.\n  We investigate this gap for abstract convexity spaces, and we present deterministic synchronous CA protocols with near-optimal communication complexity: when $L = Ω(n \\cdot κ)$, where $κ$ is a security parameter, we achieve $O(L\\cdot n\\log n)$ communication for finite convexity spaces and $O(L\\cdot n^{1+o(1)})$ communication for Euclidean spaces $\\mathbb{R}^d$. Our protocols have asymptotically optimal round complexity $O(n)$ and, when a bound on the inputs' lengths $L$ is fixed a priori, we achieve near-optimal resilience $t < n/(ω+\\varepsilon)$ for any constant $\\varepsilon>0$, where $ω$ is the Helly number of the convexity space. If $L$ is unknown, we still achieve resilience $t<n/(ω+\\varepsilon+1)$ for any constant $\\varepsilon > 0$. We further note that our protocols can be leveraged to efficiently solve parallel BA.\n  Our main technical contribution is the use of extractor graphs to obtain a deterministic assignment of parties to committees, which is resilient against adaptive adversaries."}
{"id": "2602.21568", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21568", "abs": "https://arxiv.org/abs/2602.21568", "authors": ["Yuvraj Agrawal", "Pallav Jain"], "title": "From Ad-Hoc Scripts to Orchestrated Pipelines: Architecting a Resilient ELT Framework for Developer Productivity Metrics", "comment": null, "summary": "Developer Productivity Dashboards are essential for visualizing DevOps performance metrics such as Deployment Frequency and Change Failure Rate (DORA). However, the utility of these dashboards is frequently undermined by data reliability issues. In early iterations of our platform, ad-hoc ingestion scripts (Cron jobs) led to \"silent failures,\" where data gaps went undetected for days, eroding organizational trust. This paper reports on our experience migrating from legacy scheduling to a robust Extract-Load-Transform (ELT) pipeline using Directed Acyclic Graph (DAG) orchestration and Medallion Architecture. We detail the operational benefits of decoupling data extraction from transformation, the necessity of immutable raw history for metric redefinition, and the implementation of state-based dependency management. Our experience suggests that treating the metrics pipeline as a production-grade distributed system is a prerequisite for sustainable engineering analytics."}
{"id": "2602.21444", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.21444", "abs": "https://arxiv.org/abs/2602.21444", "authors": ["Marilet De Andrade", "Joachim Sachs", "Lucas Haug", "Simon Egger", "Frank Dürr", "Balázs Varga", "Janos Farkas", "György Miklós"], "title": "Compensating the Packet Delay Variation for 6G Integrated with IEEE Time-Sensitive Networking", "comment": "Accepted at the RTNS 2025 conference", "summary": "6G is deemed as a key technology to support emerging applications with stringent requirements for highly dependable and timecritical communication. In this paper, we investigate 6G networks integrated with TSN and how to compensate for wireless stochastic behavior which involves a large intrinsic packet delay variation. We evaluate a 6G solution to reduce packet delay variation that is based on de-jittering. For this, we propose to use virtual timeslots for providing the required time-awareness. We discuss the benefits of the proposed solution while evaluating the impact of the timeslot size on the number of schedulable TSN streams."}
{"id": "2602.21477", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21477", "abs": "https://arxiv.org/abs/2602.21477", "authors": ["Zhengding Hu", "Zaifeng Pan", "Prabhleen Kaur", "Vibha Murthy", "Zhongkai Yu", "Yue Guan", "Zhen Wang", "Steven Swanson", "Yufei Ding"], "title": "Pancake: Hierarchical Memory System for Multi-Agent LLM Serving", "comment": null, "summary": "In this work, we identify and address the core challenges of agentic memory management in LLM serving, where large-scale storage, frequent updates, and multiple coexisting agents jointly introduce complex and high-cost approximate nearest neighbor (ANN) searching problems. We present Pancake, a multi-tier agentic memory system that unifies three key techniques: (i) multi-level index caching for single agents, (ii) coordinated index management across multiple agents, and (iii) collaborative GPU-CPU acceleration. Pancake exposes easy-to-use interface that can be integrated into memory-based agents like Mem-GPT, and is compatible with agentic frameworks such as LangChain and LlamaIndex. Experiments on realistic agent workloads show that Pancake substantially outperforms existing frameworks, achieving more than 4.29x end-to-end throughput improvement."}
{"id": "2602.21548", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21548", "abs": "https://arxiv.org/abs/2602.21548", "authors": ["Yongtong Wu", "Shaoyuan Chen", "Yinmin Zhong", "Rilin Huang", "Yixuan Tan", "Wentao Zhang", "Liyue Zhang", "Shangyan Zhou", "Yuxuan Liu", "Shunfeng Zhou", "Mingxing Zhang", "Xin Jin", "Panpan Huang"], "title": "DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference", "comment": null, "summary": "The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cache storage I/O rather than computation. In prevalent disaggregated architectures, loading the massive KV-Cache from external storage creates a fundamental imbalance: storage NICs on prefill engines become bandwidth-saturated, while those on decoding engines remain idle. This asymmetry severely constrains overall system throughput.\n  We present DualPath, an inference system that breaks this bottleneck by introducing dual-path KV-Cache loading. Beyond the traditional storage-to-prefill path, DualPath enables a novel storage-to-decode path, in which the KV-Cache is loaded into decoding engines and then efficiently transferred to prefill engines via RDMA over the compute network. DualPath combines this optimized data path -- which inherently avoids network congestion and avoids interference with latency-critical model execution communications -- with a global scheduler that dynamically balances load across prefill and decode engines.\n  Our evaluation on three models with production agentic workloads demonstrates that DualPath improves offline inference throughput by up to 1.87$\\times$ on our in-house inference system. It can also improve online serving throughput by an average factor of 1.96$\\times$ without violating SLO."}
{"id": "2602.21611", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21611", "abs": "https://arxiv.org/abs/2602.21611", "authors": ["Kangning Shen", "Jingyuan Zhang", "Chenxi Sun", "Wencong Zeng", "Yang Yue"], "title": "Structurally Aligned Subtask-Level Memory for Software Engineering Agents", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant potential as autonomous software engineering (SWE) agents. Recent work has further explored augmenting these agents with memory mechanisms to support long-horizon reasoning. However, these approaches typically operate at a coarse instance granularity, treating the entire problem-solving episode as the atomic unit of storage and retrieval. We empirically demonstrate that instance-level memory suffers from a fundamental granularity mismatch, resulting in misguided retrieval when tasks with similar surface descriptions require distinct reasoning logic at specific stages. To address this, we propose Structurally Aligned Subtask-Level Memory, a method that aligns memory storage, retrieval, and updating with the agent's functional decomposition. Extensive experiments on SWE-bench Verified demonstrate that our method consistently outperforms both vanilla agents and strong instance-level memory baselines across diverse backbones, improving mean Pass@1 over the vanilla agent by +4.7 pp on average (e.g., +6.8 pp on Gemini 2.5 Pro). Performance gains grow with more interaction steps, showing that leveraging past experience benefits long-horizon reasoning in complex software engineering tasks."}
{"id": "2602.21891", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2602.21891", "abs": "https://arxiv.org/abs/2602.21891", "authors": ["Fabio Palmese", "Gabriele Merlach", "Damiano Ravalico", "Martino Trevisan", "Alessandro E. C. Redondi"], "title": "Lossy Compression of Network Feature Data: When Less Is Enough", "comment": "Paper submitted to IEEE Communications Magazine", "summary": "Network traffic analysis increasingly relies on feature-based representations to support monitoring and security in the presence of pervasive encryption. Although features are more compact than raw packet traces, their storage has become a scalability bottleneck from large-scale core networks to resource-constrained Internet of Things (IoT) environments. This article investigates task-aware lossy compression strategies that reduce the storage footprint of traffic features while preserving analytics accuracy. Using website classification in core networks and device identification in IoT environments as representative use cases, we show that simple, semantics-preserving compression techniques expose stable operating regions that balance storage efficiency and task performance. These results highlight compression as a first-class design dimension in scalable network monitoring systems."}
{"id": "2602.22041", "categories": ["cs.MA", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.22041", "abs": "https://arxiv.org/abs/2602.22041", "authors": ["Vassil Guenov", "Ashwin George", "Arkady Zgonnikov", "David A. Abbink", "Luciano Cavalcante Siebert"], "title": "Using Feasible Action-Space Reduction by Groups to fill Causal Responsibility Gaps in Spatial Interactions", "comment": null, "summary": "Heralding the advent of autonomous vehicles and mobile robots that interact with humans, responsibility in spatial interaction is burgeoning as a research topic. Even though metrics of responsibility tailored to spatial interactions have been proposed, they are mostly focused on the responsibility of individual agents. Metrics of causal responsibility focusing on individuals fail in cases of causal overdeterminism -- when many actors simultaneously cause an outcome. To fill the gaps in causal responsibility left by individual-focused metrics, we formulate a metric for the causal responsibility of groups. To identify assertive agents that are causally responsible for the trajectory of an affected agent, we further formalise the types of assertive influences and propose a tiering algorithm for systematically identifying assertive agents. Finally, we use scenario-based simulations to illustrate the benefits of considering groups and how the emergence of group effects vary with interaction dynamics and the proximity of agents."}
{"id": "2602.21626", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21626", "abs": "https://arxiv.org/abs/2602.21626", "authors": ["Yifan Sun", "Gholamreza Haffar", "Minxian Xu", "Rajkumar Buyya", "Adel N. Toosi"], "title": "Multi-Layer Scheduling for MoE-Based LLM Reasoning", "comment": "12 pages, 10 figures", "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide range of tasks, but serving them efficiently at scale remains a critical challenge due to their substantial computational and latency demands. While most existing inference frameworks rely on simple scheduling strategies such as First-Come-First-Serve (FCFS) at the engine level and Round-Robin (RR) at the scheduler or coordinator level, they often fail to fully utilize system resources and may suffer from issues such as head-of-line blocking and load imbalance. Recent advances in Mixture-of-Experts (MoE) models have also introduced new challenges in scheduling arising from expert parallelism and routing complexity. This research proposes a multi-layer scheduling framework tailored for MoE-based LLM serving. It targets scheduling at three levels: request-level, enginelevel, and expert-level. At the request level, we explore algorithms such as Shortest-Job-First (SJF) and priority-aware aging to improve throughput and reduce latency. At the engine level, we design load-aware dispatching strategies that account for the current prefix token load, KV cache utilization, and user stickiness to achieve better resource matching. At the expert level, we focus on alleviating expert hotspots and strategically placing inter-layer expert dependencies to balance load and improve routing efficiency. Extensive experimental results from more than 100 experiments conducted under diverse workload distributions show that our approach consistently outperforms the state-of-theart inference framework vLLM, achieving up to 17.8% reduction in Time To First Token (TTFT) latency and 13.3% reduction in Time-Per-Output-Token (TPOT) latency."}
{"id": "2602.21641", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21641", "abs": "https://arxiv.org/abs/2602.21641", "authors": ["Man Zhang", "Yunyang Li", "Tao Yue"], "title": "Uncertainty Modeling for SysML v2", "comment": null, "summary": "Uncertainty is inherent in modern engineered systems, including cyber-physical systems, autonomous systems, and large-scale software-intensive infrastructures (such as microservice-based systems) operating in dynamic and partially observable environments. The recent publication of Precise Semantics for Uncertainty Modeling (PSUM) by the Object Management Group represents the first standardized specification for uncertainty modeling within the Model-Based Systems Engineering (MBSE) community, providing formally defined semantics for representing and reasoning about uncertainty in models. In parallel, the second version of Systems Modeling Language (SysML v2) was released as the next-generation systems modeling language, offering improved semantic rigor and reusability, yet lacking native constructs aligned with PSUM for first-class uncertainty representation. This paper proposes a systematic extension of SysML v2 that incorporates the PSUM metamodel into its modeling framework. The extension enables explicit specification of indeterminacy sources, structured characterization of uncertainties, and consistent propagation of uncertainty within system models, while preserving conformance with SysML v2 syntax and semantics. We validate the approach through seven case studies. Results demonstrate that the proposed extension (PSUM-SysMLv2) is expressive and applicable for uncertainty-aware MBSE, and potentially enables uncertainty and uncertainty propagation analyses."}
{"id": "2602.21251", "categories": ["cs.SE", "cs.AI", "cs.MA", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.21251", "abs": "https://arxiv.org/abs/2602.21251", "authors": ["Clemens Pohle"], "title": "AgenticTyper: Automated Typing of Legacy Software Projects Using Agentic AI", "comment": "Accepted at ICSE 2026 Student Research Competition (SRC)", "summary": "Legacy JavaScript systems lack type safety, making maintenance risky. While TypeScript can help, manually adding types is expensive. Previous automated typing research focuses on type inference but rarely addresses type checking setup, definition generation, bug identification, or behavioral correctness at repository scale. We present AgenticTyper, a Large Language Model (LLM)-based agentic system that addresses these gaps through iterative error correction and behavior preservation via transpilation comparison. Evaluation on two proprietary repositories (81K LOC) shows that AgenticTyper resolves all 633 initial type errors in 20 minutes, reducing manual effort from one working day."}
{"id": "2602.21788", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21788", "abs": "https://arxiv.org/abs/2602.21788", "authors": ["Yifan Niu", "Han Xiao", "Dongyi Liu", "Wei Zhou", "Jia Li"], "title": "DHP: Efficient Scaling of MLLM Training with Dynamic Hybrid Parallelism", "comment": null, "summary": "Scaling long-context capabilities is crucial for Multimodal Large Language Models (MLLMs). However, real-world multimodal datasets are extremely heterogeneous. Existing training frameworks predominantly rely on static parallelism strategies, which suffer from severe load imbalance, redundant communication, and suboptimal hardware utilization under data heterogeneity. In this work, we propose Dynamic Hybrid Parallelism (DHP), an efficient parallelism strategy that adaptively reconfigures communication groups and parallelism degrees during MLLM training. We generalize the non-power-of-two parallelism degrees and develop a polynomial-time algorithm to generate near-optimal parallelism strategies with only millisecond-level overhead per training batch. DHP is able to maintain high hardware efficiency even under extreme data variability. Experimental results demonstrate that DHP significantly outperforms Megatron-LM and DeepSpeed, achieving up to 1.36 $\\times$ speedup in training throughput while maintaining near-linear scaling efficiency across large-scale NPU clusters."}
{"id": "2602.22158", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.22158", "abs": "https://arxiv.org/abs/2602.22158", "authors": ["Minqiu Sun", "Xin Huang", "Luanzheng Guo", "Nathan R. Tallent", "Kento Sato", "Dong Dai"], "title": "LLMTailor: A Layer-wise Tailoring Tool for Efficient Checkpointing of Large Language Models", "comment": "9 pages, 3 figures, accepted at PDSW'25", "summary": "Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. Recent studies reveal that updates across LLM layers are highly non-uniform. Across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. This suggests that selectively checkpointing only layers with significant updates could reduce overhead without harming training. Implementing such selective strategies requires fine-grained control over both weights and optimizer states, which no current tool provides. To address this gap, we propose \\texttt{LLMTailor}, a checkpoint-merging framework that filters and assembles layers from different checkpoints to form a composite checkpoint. Our evaluation indicates that LLMTailor can work with different selective checkpointing strategies and effectively reduce checkpoint size (e.g., 4.3 times smaller for Llama3.1-8B) and checkpoint time (e.g., 2.8 times faster for Qwen2.5-7B) while maintaining model quality."}
