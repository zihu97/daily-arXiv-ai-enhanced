<div id=toc></div>

# Table of Contents

- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [1] [Reducing Compute Waste in LLMs through Kernel-Level DVFS](https://arxiv.org/abs/2601.08539)
*Jeffrey Spaan,Kuan-Hsun Chen,Ana-Lucia Varbanescu*

Main category: cs.PF

TL;DR: 本文提出一种细粒度、内核级的动态电压频率调节（DVFS）方法，显著降低大语言模型训练能耗且几乎不影响性能。


<details>
  <summary>Details</summary>
Motivation: AI数据中心能耗激增已成为可持续发展的瓶颈，需在不牺牲性能前提下减少能源浪费。

Method: 采用内核级DVFS策略，探索新频率配置，并验证其优于以往的pass级或迭代级方案。

Result: 在GPT-3训练中实现14.6%能耗节省（仅0.6%性能下降），且适用于数据与张量并行场景。

Conclusion: 内核级DVFS是减少LLM操作能耗的有效技术，兼顾节能与性能。

Abstract: The rapid growth of AI has fueled the expansion of accelerator- or GPU-based data centers. However, the rising operational energy consumption has emerged as a critical bottleneck and a major sustainability concern. Dynamic Voltage and Frequency Scaling (DVFS) is a well-known technique used to reduce energy consumption, and thus improve energy-efficiency, since it requires little effort and works with existing hardware. Reducing the energy consumption of training and inference of Large Language Models (LLMs) through DVFS or power capping is feasible: related work has shown energy savings can be significant, but at the cost of significant slowdowns. In this work, we focus on reducing waste in LLM operations: i.e., reducing energy consumption without losing performance. We propose a fine-grained, kernel-level, DVFS approach that explores new frequency configurations, and prove these save more energy than previous, pass- or iteration-level solutions. For example, for a GPT-3 training run, a pass-level approach could reduce energy consumption by 2% (without losing performance), while our kernel-level approach saves as much as 14.6% (with a 0.6% slowdown). We further investigate the effect of data and tensor parallelism, and show our discovered clock frequencies translate well for both. We conclude that kernel-level DVFS is a suitable technique to reduce waste in LLM operations, providing significant energy savings with negligible slow-down.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [2] [Hierarchical Online-Scheduling for Energy-Efficient Split Inference with Progressive Transmission](https://arxiv.org/abs/2601.08135)
*Zengzipeng Tang,Yuxuan Sun,Wei Chen,Jianwen Ding,Bo Ai,Yulin Shao*

Main category: cs.NI

TL;DR: ENACHI框架通过任务级和包级联合调度优化，在能耗与延迟约束下显著提升推理精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有设备边缘协同推理中任务级调度与信道动态不匹配、任务复杂度感知不足的问题。

Method: 提出两层Lyapunov框架，外层进行DNN分割与带宽分配，内层采用渐进传输机制动态调整发射功率。

Result: 在ImageNet数据集上，ENACHI相比现有方法推理精度提升43.12%，能耗降低62.13%，且在多用户场景保持稳定能耗。

Conclusion: ENACHI有效平衡精度、能耗与延迟，具备高适应性与可扩展性。

Abstract: Device-edge collaborative inference with Deep Neural Networks (DNNs) faces fundamental trade-offs among accuracy, latency and energy consumption. Current scheduling exhibits two drawbacks: a granularity mismatch between coarse, task-level decisions and fine-grained, packet-level channel dynamics, and insufficient awareness of per-task complexity. Consequently, scheduling solely at the task level leads to inefficient resource utilization. This paper proposes a novel ENergy-ACcuracy Hierarchical optimization framework for split Inference, named ENACHI, that jointly optimizes task- and packet-level scheduling to maximize accuracy under energy and delay constraints. A two-tier Lyapunov-based framework is developed for ENACHI, with a progressive transmission technique further integrated to enhance adaptivity. At the task level, an outer drift-plus-penalty loop makes online decisions for DNN partitioning and bandwidth allocation, and establishes a reference power budget to manage the long-term energy-accuracy trade-off. At the packet level, an uncertainty-aware progressive transmission mechanism is employed to adaptively manage per-sample task complexity. This is integrated with a nested inner control loop implementing a novel reference-tracking policy, which dynamically adjusts per-slot transmit power to adapt to fluctuating channel conditions. Experiments on ImageNet dataset demonstrate that ENACHI outperforms state-of-the-art benchmarks under varying deadlines and bandwidths, achieving a 43.12\% gain in inference accuracy with a 62.13\% reduction in energy consumption under stringent deadlines, and exhibits high scalability by maintaining stable energy consumption in congested multi-user scenarios.

</details>


### [3] [Joint Communication and Sensing in RIS-Assisted MIMO System Under Mutual Coupling](https://arxiv.org/abs/2601.08142)
*Dilki Wijekoon,Amine Mezghani,Ekram Hossain*

Main category: cs.NI

TL;DR: 本文研究了考虑互耦效应的RIS辅助通感一体化系统，通过联合优化RIS反射系数与基站波束成形，在单用户单目标场景下提升通信与感知性能。


<details>
  <summary>Details</summary>
Motivation: 现有RIS辅助JCAS模型忽略元件间互耦效应，导致性能评估不真实，本文旨在构建物理一致模型以更贴近实际系统。

Method: 采用费雪信息与互信息分别量化感知精度与通信效率，联合优化RIS反射系数与BS波束成形，并引入互耦模型与信号量化分析自干扰影响。

Result: 数值结果表明，考虑互耦可显著提升通信与感知性能，且单/双基地雷达配置对系统表现有不同影响，揭示了FI与MI之间的基本权衡关系。

Conclusion: 物理一致建模对RIS-JCAS系统设计至关重要，互耦效应不可忽略，系统配置需根据应用场景权衡通信与感知需求。

Abstract: This paper considers a downlink Reconfigurable Intelligent Surface (RIS)-assisted Joint Communication and Sensing (JCAS) system within a physically-consistent setting, accounting for the effect of mutual coupling between RIS elements arising due to sub-element spacing. The system features a multiple-input multiple-output (MIMO) terrestrial base station (BS) and explores both monostatic and bistatic radar configurations to enable joint communication and sensing. In the monostatic configuration, both the transmitter and receiver are at the same location, while the bistatic configuration separates the transmitter and receiver spatially. System performance is evaluated using Fisher Information (FI) to quantify sensing accuracy and Mutual Information (MI) to measure communication efficiency. To achieve an optimal balance between communication and sensing, the RIS reflective coefficients and BS transmit beamforming are jointly optimized by maximizing a weighted sum of FI and MI. A novel solution approach is proposed for a single-user, single-object scenario, leveraging the mutual coupling model to enhance system realism. The impact of self-interference on sensing performance is also investigated through signal quantization. Numerical results reveal a fundamental trade-off between FI and MI and demonstrate that incorporating mutual coupling within a physically-consistent framework significantly improves both communication and sensing performance compared to conventional RIS-assisted JCAS models. Additionally, the analysis highlights how the choice of monostatic versus bistatic radar configuration affects system performance, offering valuable insights for the design of RIS-assisted JCAS systems.

</details>


### [4] [Multi-Objective Optimization for Joint Communication and Sensing in Multi-user MIMO Systems: Characterizing the Pareto Boundary](https://arxiv.org/abs/2601.08152)
*Thakshila Perera,Amine Mezghani,Ekram Hossain*

Main category: cs.NI

TL;DR: 本文研究了联合通信与感知（JCAS）系统的帕累托边界性能，提出了一种适用于多用户MIMO场景的集成波束成形方法，并通过优化加权互信息与费舍尔信息之和，验证了联合波束成形优于独立波束成形。


<details>
  <summary>Details</summary>
Motivation: 同时满足通信与感知功能的JCAS系统需要在资源受限条件下实现性能平衡，探索其帕累托边界有助于指导实际系统设计。

Method: 构建以互信息和费舍尔信息为指标的多目标优化问题，针对多用户和单用户场景分别采用上行-下行对偶性结合拉格朗日优化、线搜索与块坐标上升法，以及投影梯度下降法求解。

Result: 数值结果表明，在多用户场景中联合波束成形最优；同时揭示了不同天线数、用户数及EIRP限制下的帕累托边界特性。

Conclusion: 联合波束成形能有效提升JCAS系统整体性能，帕累托边界分析为系统参数配置与性能权衡提供了理论依据。

Abstract: This paper investigates the Pareto boundary performance of a joint communication and sensing (JCAS) system that addresses both sensing and communication functions at the same time. In this scenario, a multiple-antenna base station (BS) transmits information to multiple single-antenna communication users while concurrently estimating the parameters of a single sensing object using the echo signal. We present an integrated beamforming approach for JCAS in a multi-user multiple-input and multiple-output (MIMO) system. The performance measures for communication and sensing are Fisher information (FI) and mutual information (MI). Our research considers two scenarios: multiple communication users with a single sensing object and a single communication user with a single sensing object. We formulate a multi-objective optimization problem to maximize the weighted sum of MI and FI, subject to a total transmit power budget for both cases. As a particular case, we address the equivalent isotropic radiated power (EIRP) for the single communication user scenario. We use the uplink-downlink duality for the multi-user case to simplify the problem and apply Lagrangian optimization and line search methods with a block-coordinate ascending technique. We use projected gradient descent (PGD) to solve the optimization problem in the single-user case. Our numerical results demonstrate that joint beamforming is optimal for the multi-user JCAS system, as opposed to independent beamforming for each user and the sensing object. Furthermore, we reveal the Pareto boundary for the multi-user case, with variations in the number of communication users and the number of transmitting and receiving antennas. We provide the Pareto boundary depending on EIRP limitations for the single-user case.

</details>


### [5] [Tiny-Twin: A CPU-Native Full-stack Digital Twin for NextG Cellular Networks](https://arxiv.org/abs/2601.08217)
*Ali Mamaghani,Ushasi Ghosh,Ish Kumar Jain,Srinivas Shakkottai,Dinesh Bharadia*

Main category: cs.NI

TL;DR: Tiny-Twin 是一个基于通用 CPU 的全栈数字孪生框架，支持高保真、可扩展的 5G 实验。


<details>
  <summary>Details</summary>
Motivation: 现有数字孪生方案在物理层保真度、可扩展性或硬件依赖性上存在妥协，亟需更实用的解决方案。

Method: 整合时变多抽头卷积与完整 5G 协议栈，通过软件架构重构和系统级优化实现在纯软件中执行细粒度卷积。

Result: Tiny-Twin 可支持多个并发用户设备，保持协议时序与端到端行为，平衡了低保真模拟器与高成本硬件仿真器之间的需求。

Conclusion: Tiny-Twin 作为开源平台，为下一代蜂窝研究提供了高保真、易访问的实验环境。

Abstract: Modern wireless applications demand testing environments that capture the full complexity of next-generation (NextG) cellular networks. While digital twins promise realistic emulation, existing solutions often compromise on physical-layer fidelity and scalability or depend on specialized hardware. We present Tiny-Twin, a CPU-Native, full-stack digital twin framework that enables realistic, repeatable 5G experimentation on commodity CPUs. Tiny-Twin integrates time-varying multi-tap convolution with a complete 5G protocol stack, supporting plug-and-play replay of diverse channel traces. Through a redesigned software architecture and system-level optimizations, Tiny-Twin supports fine-grained convolution entirely in software. With built-in real-time RIC integration and per User Equipment(UE) channel isolation, it facilitates rigorous testing of network algorithms and protocol designs. Our evaluation shows that Tiny-Twin scales to multiple concurrent UEs while preserving protocol timing and end-to-end behavior, delivering a practical middle ground between low-fidelity simulators and high-cost hardware emulators. We release Tiny-Twin as an open-source platform to enable accessible, high-fidelity experimentation for NextG cellular research.

</details>


### [6] [Unleashing Tool Engineering and Intelligence for Agentic AI in Next-Generation Communication Networks](https://arxiv.org/abs/2601.08259)
*Yinqiu Liu,Ruichen Zhang,Dusit Niyato,Abbas Jamalipour,Trung Q. Duong,Dong In Kim*

Main category: cs.NI

TL;DR: 本文探讨了工具智能在通信领域中的应用，提出了一套完整的工具工程生命周期，并通过无人机轨迹规划案例验证其有效性，为6G时代构建工具增强型智能体提供路线图。


<details>
  <summary>Details</summary>
Motivation: 弥合抽象推理与物理执行之间的鸿沟，推动大语言模型从被动聊天机器人向自主操作者演进。

Method: 系统性回顾工具工程生命周期（创建、发现、选择、学习、评估），并结合教师引导强化学习与可行性屏蔽机制进行案例研究。

Result: 智能体能利用外部工具消除导航不确定性，并在严格能耗约束下实现成本感知调度。

Conclusion: 工具智能是实现6G时代自主通信智能体的关键，需持续推进工具工程体系化研究。

Abstract: Nowadays, agentic AI is emerging as a transformative paradigm for next-generation communication networks, promising to evolve large language models (LLMs) from passive chatbots into autonomous operators. However, unleashing this potential requires bridging the critical gap between abstract reasoning and physical actuation, a capability we term tool intelligence. In this article, we explore the landscape of tool engineering to empower agentic AI in communications. We first analyze the functionalities of tool intelligence and its effects on communications. We then propose a systematic review for tool engineering, covering the entire lifecycle from tool creation and discovery to selection, learning, and benchmarking. Furthermore, we present a case study on tool-assisted uncrewed aerial vehicles (UAV) trajectory planning to demonstrate the realization of tool intelligence in communications. By introducing a teacher-guided reinforcement learning approach with a feasibility shield, we enable agents to intelligently operate tools. They utilize external tools to eliminate navigational uncertainty while mastering cost-aware scheduling under strict energy constraints. This article aims to provide a roadmap for building the tool-augmented intelligent agents of the 6G era.

</details>


### [7] [A decentralized academic certificate issuance system using smart contracts on the tron network](https://arxiv.org/abs/2601.08513)
*Ana Julia Evangelista Andrade,Flavio Cezar Amate*

Main category: cs.NI

TL;DR: 该论文提出了一种基于TRON区块链的去中心化学术证书颁发与验证系统，结合智能合约与IPFS存储，实现防伪、高效验证与低成本运行。


<details>
  <summary>Details</summary>
Motivation: 解决传统学术证书易伪造、依赖中心化机构、验证效率低等问题。

Method: 基于TRON区块链开发智能合约，搭配IPFS存储元数据，构建dApp前端，并进行功能、安全、性能及可用性评估。

Result: 系统支持正确发证与公开验证，具备访问控制和抗滥用能力；确认延迟低、交易成本可忽略；SUS评分为76.67，用户接受度良好。

Conclusion: 方案在技术上可行且实用性强，TRON区块链是构建去中心化学术认证系统的高效低成本基础设施。

Abstract: This paper presents the design, implementation, and evaluation of a decentralized system for issuing and verifying academic certificates based on blockchain technology. The proposed solution addresses common limitations of traditional certification models, such as susceptibility to forgery, reliance on centralized infrastructures, and inefficient verification processes. The system is built on the TRON blockchain and integrates smart contracts written in Solidity, a decentralized web application (dApp) for user interaction, and the InterPlanetary File System (IPFS) for decentralized storage of certificate metadata. The methodology comprised architectural design, smart contract development, and the implementation of a web-based interface, followed by functional, security, performance, and usability evaluations. Experimental results show that the system correctly supports certificate issuance and public verification, enforces access control, and resists common misuse scenarios. Performance analysis indicates low confirmation latency and negligible transaction costs, making the solution suitable for large-scale academic environments. Additionally, usability assessment using the System Usability Scale (SUS) resulted in a score of 76.67, indicating good user acceptance. Overall, the results demonstrate the technical feasibility and practical viability of the proposed approach, highlighting the TRON blockchain as an effective and cost-efficient infrastructure for decentralized academic certification systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [Matrix-PIC: Harnessing Matrix Outer-product for High-Performance Particle-in-Cell Simulations](https://arxiv.org/abs/2601.08277)
*Yizhuo Rao,Xingjian Cui,Jiabin Xie,Shangzhi Pang,Guangnan Feng,Jinhui Wei,Zhiguang Chen,Yutong Lu*

Main category: cs.DC

TL;DR: MatrixPIC通过矩阵中心化设计，在现代CPU上显著加速粒子网格交互，尤其在LWFA模拟中实现最高2.63倍总加速。


<details>
  <summary>Details</summary>
Motivation: 传统多核CPU上细粒度原子更新成为PIC模拟的瓶颈，而新型CPU集成的矩阵处理单元（MPU）提供了突破机会。

Method: 提出MatrixPIC，结合块矩阵算法、MPU-VPU混合执行流水线和增量排序器，适配现代CPU架构。

Result: 在LWFA模拟中总运行时间加速达2.63倍，核心核函数比基线快8.7倍，比最优VPU实现快2倍，达到CPU峰值性能83.08%。

Conclusion: 矩阵导向协同设计能有效利用新兴CPU架构加速PIC模拟，性能甚至超越数据中心GPU。

Abstract: Particle-in-Cell (PIC) simulations spend most of their execution time on particle--grid interactions, where fine-grained atomic updates become a major bottleneck on traditional many-core CPUs. Recent CPU architectures integrate specialized Matrix Processing Units (MPUs) that efficiently support matrix outer-product operations, offering new opportunities to overcome this limitation. Leveraging this architectural shift, this work focuses on redesigning the current deposition step of PIC simulations under a matrix-centric execution model.
  We present MatrixPIC, the first holistic co-design of the deposition kernel, data layout, and incremental particle sorting tailored to the hybrid MPU--VPU SIMD model on modern CPUs. MatrixPIC introduces: (i)~a block-matrix formulation of the current deposition algorithm that maps naturally to MPU outer-product primitives; (ii)~a hybrid execution pipeline that combines MPU-based high-density accumulation with VPU-based data preparation and control flow; and (iii)~an $O(1)$-amortized incremental sorter based on a gapped packed-memory array to preserve data locality for efficient MPU execution.
  Evaluated on a next-generation HPC platform, MatrixPIC achieves significant performance gains. In Laser-Wakefield Acceleration (LWFA) simulations, it delivers up to $2.63\times$ speedup in total runtime. For third-order deposition, the core kernel is accelerated by $8.7\times$ over the baseline and $2.0\times$ over the best hand-optimized VPU implementation. Moreover, MatrixPIC reaches $83.08\%$ of theoretical CPU peak performance, nearly $2.8\times$ higher than a highly optimized CUDA kernel on a data center GPU. These results demonstrate the effectiveness of matrix-oriented co-design for accelerating PIC simulations on emerging CPU architectures.

</details>


### [9] [Shifting the Sweet Spot: High-Performance Matrix-Free Method for High-Order Elasticity](https://arxiv.org/abs/2601.08374)
*Dali Chang,Chong Zhang,Kaiqi Zhang,Mingguan Yang,Huiyuan Li,Weiqiang Kong*

Main category: cs.DC

TL;DR: 本文通过优化矩阵自由方法，在MFEM框架中实现了高性能弹性力学高阶有限元计算，显著提升CPU硬件上的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有矩阵自由方法未能充分利用现代CPU架构和张量积单元结构，导致性能最优阶数偏低，限制了高阶方法的潜力。

Method: 设计多级优化策略：采用O(p^4)张量分解算法、利用Voigt对称性减少冗余计算、宏内核融合提升数据局部性。

Result: 在x86和ARM架构上，核心算子加速7x至83x，端到端流程提速3.6x至16.8x，性能最优阶数提升至p≥6。

Conclusion: 本研究为在主流CPU上开展大规模高阶弹性力学模拟提供了高效可行的技术路径。

Abstract: In high-order finite element analysis for elasticity, matrix-free (PA) methods are a key technology for overcoming the memory bottleneck of traditional Full Assembly (FA). However, existing implementations fail to fully exploit the special structure of modern CPU architectures and tensor-product elements, causing their performance "sweet spot" to anomalously remain at the low order of $p \approx 2$, which severely limits the potential of high-order methods. To address this challenge, we design and implement a highly optimized PA operator within the MFEM framework, deeply integrated with a Geometric Multigrid (GMG) preconditioner. Our multi-level optimization strategy includes replacing the original $O(p^6)$ generic algorithm with an efficient $O(p^4)$ one based on tensor factorization, exploiting Voigt symmetry to reduce redundant computations for the elasticity problem, and employing macro-kernel fusion to enhance data locality and break the memory bandwidth bottleneck. Extensive experiments on mainstream x86 and ARM architectures demonstrate that our method successfully shifts the performance "sweet spot" to the higher-order region of $p \ge 6$. Compared to the MFEM baseline, the optimized core operator (kernel) achieves speedups of 7x to 83x, which translates to a 3.6x to 16.8x end-to-end performance improvement in the complete solution process. This paper provides a validated and efficient practical path for conducting large-scale, high-order elasticity simulations on mainstream CPU hardware.

</details>


### [10] [MixServe: An Automatic Distributed Serving System for MoE Models with Hybrid Parallelism Based on Fused Communication Algorithm](https://arxiv.org/abs/2601.08800)
*Bowen Zhou,Jinrui Jia,Wenhao He,Yong Zhang,Fang Dong*

Main category: cs.DC

TL;DR: MixServe是一种新型自动分布式服务系统，通过融合AR-A2A通信算法的TP-EP混合并行策略，显著提升MoE模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有分布式MoE模型因通信瓶颈和负载不均导致的低效问题，尤其是在多节点部署场景下。

Method: 提出TP-EP混合并行策略，结合融合AR-A2A通信算法，并根据模型参数与硬件配置自动选择最优并行方案。

Result: 在DeepSeek-R1和Qwen3模型上，TTFT加速1.08~3.80倍，ITL加速1.03~1.66倍，吞吐量提升5.2%~50.3%。

Conclusion: MixServe有效优化了MoE模型的分布式推理性能，是当前高效部署大规模MoE模型的有力解决方案。

Abstract: The Mixture of Experts (MoE) models are emerging as the latest paradigm for Large Language Models (LLMs). However, due to memory constraints, MoE models with billions or even trillions of parameters can only be deployed in multi-GPU or even multi-node & multi-GPU based serving systems. Thus, communication has became a major bottleneck in distributed serving systems, especially inter-node communication. Contemporary distributed MoE models are primarily implemented using all-reduce (AR) based tensor parallelism (TP) and all-to-all (A2A) based expert parallelism (EP). However, TP generally exhibits low inter-node efficiency and is thus confined to high-speed intra-node bandwidth. In contrast, EP tends to suffer from load imbalance, especially when the parallel degree is high.
  In this work, we introduce MixServe, a novel automatic distributed serving system for efficient deployment of MoE models by a novel TP-EP hybrid parallelism based on fused AR-A2A communication algorithm. MixServe begins by evaluating the communication overhead associated with various parallel strategies, taking into account the model hyperparameters and the configurations of network and hardware resources, and then automatically selects the most efficient parallel strategy. Then, we propose the TP-EP hybrid parallelism based on fused AR-A2A communication algorithm that overlaps intra-node AR communication and inter-node A2A communication. Extensive experiments on DeepSeek-R1 and Qwen3 models demonstrate that MixServe achieves superior inference performance, with 1.08~3.80x acceleration in time to first token (TTFT), 1.03~1.66x acceleration in inter-token latency (ITL), and 5.2%~50.3% throughput improvement compared to existing approaches.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [SECite: Analyzing and Summarizing Citations in Software Engineering Literature](https://arxiv.org/abs/2601.07939)
*Shireesh Reddy Pyreddy,Khaja Valli Pathan,Hasan Masum,Tarannum Shaila Zaman*

Main category: cs.SE

TL;DR: SECite通过分析引用语境的情感，结合NLP与生成式AI，评估论文的学术影响力并总结其优缺点。


<details>
  <summary>Details</summary>
Motivation: 传统摘要仅反映作者视角，缺乏外部评价，需更全面理解论文贡献与不足。

Method: 构建半自动流程提取引用，用无监督机器学习分类情感，并利用生成式AI生成情感导向摘要。

Result: 发现学术界对目标论文的评价模式，揭示外部反馈与作者自述之间的异同。

Conclusion: 该方法为评估学术贡献提供了整合引用情感与LLM摘要的综合框架。

Abstract: Identifying the strengths and limitations of a research paper is a core component of any literature review. However, traditional summaries reflect only the authors' self-presented perspective. Analyzing how other researchers discuss and cite the paper can offer a deeper, more practical understanding of its contributions and shortcomings. In this research, we introduce SECite, a novel approach for evaluating scholarly impact through sentiment analysis of citation contexts. We develop a semi-automated pipeline to extract citations referencing nine research papers and apply advanced natural language processing (NLP) techniques with unsupervised machine learning to classify these citation statements as positive or negative. Beyond sentiment classification, we use generative AI to produce sentiment-specific summaries that capture the strengths and limitations of each target paper, derived both from clustered citation groups and from the full text. Our findings reveal meaningful patterns in how the academic community perceives these works, highlighting areas of alignment and divergence between external citation feedback and the authors' own presentation. By integrating citation sentiment analysis with LLM-based summarization, this study provides a comprehensive framework for assessing scholarly contributions.

</details>


### [12] [Towards Verifiably Safe Tool Use for LLM Agents](https://arxiv.org/abs/2601.08012)
*Aarya Doshi,Yining Hong,Congying Xu,Eunsuk Kang,Alexandros Kapravelos,Christian Kästner*

Main category: cs.SE

TL;DR: 提出结合STPA与增强版MCP框架，为LLM智能体构建具备形式化保障的主动安全机制。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体工具调用存在数据泄露与误操作风险，现有方法依赖人工标注且无法提供形式化安全保障。

Method: 采用系统理论过程分析（STPA）识别智能体工作流危害并导出安全需求，再通过增强型Model Context Protocol框架结构化标注能力、机密性与信任等级以强制执行。

Result: 实现从临时可靠性修补到具备形式化保障的主动防护机制转变，降低对用户确认的依赖，使自主性成为可设计选项。

Conclusion: 该方法有望在企业级场景中为LLM智能体提供可验证、低人工干预的安全运行保障。

Abstract: Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. While this empowers agents to perform complex tasks, LLMs may invoke unintended tool interactions and introduce risks, such as leaking sensitive data or overwriting critical records, which are unacceptable in enterprise contexts. Current approaches to mitigate these risks, such as model-based safeguards, enhance agents' reliability but cannot guarantee system safety. Methods like information flow control (IFC) and temporal constraints aim to provide guarantees but often require extensive human annotation. We propose a process that starts with applying System-Theoretic Process Analysis (STPA) to identify hazards in agent workflows, derive safety requirements, and formalize them as enforceable specifications on data flows and tool sequences. To enable this, we introduce a capability-enhanced Model Context Protocol (MCP) framework that requires structured labels on capabilities, confidentiality, and trust level. Together, these contributions aim to shift LLM-based agent safety from ad hoc reliability fixes to proactive guardrails with formal guarantees, while reducing dependence on user confirmation and making autonomy a deliberate design choice.

</details>


### [13] [Automating API Documentation from Crowdsourced Knowledge](https://arxiv.org/abs/2601.08036)
*Bonan Kou,Zijie Zhou,Muhao Chen,Tianyi Zhang*

Main category: cs.SE

TL;DR: AutoDoc利用Stack Overflow讨论和GPT-4o自动生成更准确、全面且简洁的API文档，有效缓解LLM幻觉与冗余问题。


<details>
  <summary>Details</summary>
Motivation: 官方API文档常过时或不完整，开发者难以高效学习和使用API。

Method: 结合微调稠密检索模型从SO提取七类API知识，并用GPT-4o总结；设计两个组件抑制LLM幻觉与冗余。

Result: 相比基线，AutoDoc生成文档准确率提升77.7%，重复减少9.5%，补充34.4%官方未覆盖知识；小模型经优化后表现接近大模型。

Conclusion: 合理设计可使LLM有效用于API文档生成，显著提升文档质量与实用性。

Abstract: API documentation is crucial for developers to learn and use APIs. However, it is known that many official API documents are obsolete and incomplete. To address this challenge, we propose a new approach called AutoDoc that generates API documents with API knowledge extracted from online discussions on Stack Overflow (SO). AutoDoc leverages a fine-tuned dense retrieval model to identify seven types of API knowledge from SO posts. Then, it uses GPT-4o to summarize the API knowledge in these posts into concise text. Meanwhile, we designed two specific components to handle LLM hallucination and redundancy in generated content. We evaluated AutoDoc against five comparison baselines on 48 APIs of different popularity levels. Our results indicate that the API documents generated by AutoDoc are up to 77.7% more accurate, 9.5% less duplicated, and contain 34.4% knowledge uncovered by the official documents. We also measured the sensitivity of AutoDoc to the choice of different LLMs. We found that while larger LLMs produce higher-quality API documents, AutoDoc enables smaller open-source models (e.g., Mistral-7B-v0.3) to achieve comparable results. Finally, we conducted a user study to evaluate the usefulness of the API documents generated by AutoDoc. All participants found API documents generated by AutoDoc to be more comprehensive, concise, and helpful than the comparison baselines. This highlights the feasibility of utilizing LLMs for API documentation with careful design to counter LLM hallucination and information redundancy.

</details>


### [14] [Cognitive Biases in LLM-Assisted Software Development](https://arxiv.org/abs/2601.08045)
*Xinyi Zhou,Zeinadsadat Saghi,Sadra Sabouri,Rahul Pandita,Mollie McGuire,Souti Chattopadhyay*

Main category: cs.SE

TL;DR: 本文首次系统研究了大语言模型（LLM）辅助开发中的认知偏差，通过混合方法发现56.4%的偏差行为源于人机交互，并提出15类偏差分类及缓解建议。


<details>
  <summary>Details</summary>
Motivation: LLM普及使编程从生成解决方案转向评估方案，引发新型认知偏差，亟需系统研究其影响与应对策略。

Method: 结合14名开发者观察研究与22名开发者问卷调查，定性对比传统与LLM工作流中的偏差类别，经认知心理学家验证构建分类体系。

Result: 发现48.8%程序员行为存在偏差，其中56.4%由LLM交互引发；建立含15类偏差的分类法，并提供开发者工具与LLM设计者改进建议。

Conclusion: LLM协作显著增加新型认知偏差风险，需通过工具优化与流程改进减轻其对决策质量的负面影响。

Abstract: The widespread adoption of Large Language Models (LLMs) in software development is transforming programming from a solution-generative to a solution-evaluative activity. This shift opens a pathway for new cognitive challenges that amplify existing decision-making biases or create entirely novel ones. One such type of challenge stems from cognitive biases, which are thinking patterns that lead people away from logical reasoning and result in sub-optimal decisions. How do cognitive biases manifest and impact decision-making in emerging AI-collaborative development? This paper presents the first comprehensive study of cognitive biases in LLM-assisted development. We employ a mixed-methods approach, combining observational studies with 14 student and professional developers, followed by surveys with 22 additional developers. We qualitatively compare categories of biases affecting developers against the traditional non-LLM workflows. Our findings suggest that LLM-related actions are more likely to be associated with novel biases. Through a systematic analysis of 90 cognitive biases specific to developer-LLM interactions, we develop a taxonomy of 15 bias categories validated by cognitive psychologists. We found that 48.8% of total programmer actions are biased, and developer-LLM interactions account for 56.4% of these biased actions. We discuss how these bias categories manifest, present tools and practices for developers, and recommendations for LLM tool builders to help mitigate cognitive biases in human-AI programming.

</details>


### [15] [Coverage-Guided Road Selection and Prioritization for Efficient Testing in Autonomous Driving Systems](https://arxiv.org/abs/2601.08609)
*Qurban Ali,Andrea Stocco,Leonardo Mariani,Oliviero Riganelli*

Main category: cs.SE

TL;DR: 提出一种减少冗余并保持多样性的自动驾驶测试优先框架，显著提升故障检测效率。


<details>
  <summary>Details</summary>
Motivation: 解决ADAS测试数据集中冗余案例拖慢测试速度的问题。

Method: 基于几何与动态特征聚类场景，按复杂度、难度和历史失败率优先排序测试用例。

Result: 在OPENCAT和Udacity数据集上平均减少89%测试规模，保留79%失败场景，早期故障检测效率提升达95倍。

Conclusion: 该框架有效平衡测试效率与覆盖率，显著优化ADAS系统验证流程。

Abstract: Autonomous Driving Assistance Systems (ADAS) rely on extensive testing to ensure safety and reliability, yet road scenario datasets often contain redundant cases that slow down the testing process without improving fault detection. To address this issue, we present a novel test prioritization framework that reduces redundancy while preserving geometric and behavioral diversity. Road scenarios are clustered based on geometric and dynamic features of the ADAS driving behavior, from which representative cases are selected to guarantee coverage. Roads are finally prioritized based on geometric complexity, driving difficulty, and historical failures, ensuring that the most critical and challenging tests are executed first. We evaluate our framework on the OPENCAT dataset and the Udacity self-driving car simulator using two ADAS models. On average, our approach achieves an 89% reduction in test suite size while retaining an average of 79% of failed road scenarios. The prioritization strategy improves early failure detection by up to 95x compared to random baselines.

</details>


### [16] [LLMs in Code Vulnerability Analysis: A Proof of Concept](https://arxiv.org/abs/2601.08691)
*Shaznin Sultana,Sadia Afreen,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本文探讨了代码专用和通用大语言模型在自动化软件安全任务中的潜力，如漏洞识别、严重性预测和修复生成。


<details>
  <summary>Details</summary>
Motivation: 传统软件安全分析方法难以应对现代代码库的规模与复杂性，需借助智能自动化提升效率与准确性。

Method: 评估五对近期LLM（含代码与通用开源模型）在Big-Vul与Vul-Repair数据集上的表现，并比较微调与提示方法。

Result: 微调在所有任务中均优于零样本与少样本；代码专用模型在复杂任务中表现更佳，但通用模型亦接近；现有修复质量评估指标存在不足。

Conclusion: 本研究为软件安全领域提供了利用先进LLM改进漏洞分析与修复的实证依据。

Abstract: Context: Traditional software security analysis methods struggle to keep pace with the scale and complexity of modern codebases, requiring intelligent automation to detect, assess, and remediate vulnerabilities more efficiently and accurately. Objective: This paper explores the incorporation of code-specific and general-purpose Large Language Models (LLMs) to automate critical software security tasks, such as identifying vulnerabilities, predicting severity and access complexity, and generating fixes as a proof of concept. Method: We evaluate five pairs of recent LLMs, including both code-based and general-purpose open-source models, on two recognized C/C++ vulnerability datasets, namely Big-Vul and Vul-Repair. Additionally, we compare fine-tuning and prompt-based approaches. Results: The results show that fine-tuning uniformly outperforms both zero-shot and few-shot approaches across all tasks and models. Notably, code-specialized models excel in zero-shot and few-shot settings on complex tasks, while general-purpose models remain nearly as effective. Discrepancies among CodeBLEU, CodeBERTScore, BLEU, and ChrF highlight the inadequacy of current metrics for measuring repair quality. Conclusions: This study contributes to the software security community by investigating the potential of advanced LLMs to improve vulnerability analysis and remediation.

</details>


### [17] ["Where is My Troubleshooting Procedure?": Studying the Potential of RAG in Assisting Failure Resolution of Large Cyber-Physical System](https://arxiv.org/abs/2601.08706)
*Maria Teresa Rossi,Leonardo Mariani,Oliviero Riganelli,Giuseppe Filomento,Danilo Giannone,Paolo Gavazzo*

Main category: cs.SE

TL;DR: 本文探讨了RAG技术在复杂工业环境中辅助操作员快速检索故障排除程序的潜力。


<details>
  <summary>Details</summary>
Motivation: 操作员在紧急情况下需要快速从大量技术手册中找到正确的故障排除步骤，传统方法效率低下。

Method: 通过分析Fincantieri公司提供的故障排除程序，进行了一系列实验评估RAG的效果。

Result: 实验证明RAG能有效帮助操作员快速响应故障症状，但需交叉验证推荐结果以确保准确性。

Conclusion: RAG可提升操作员应对故障的效率，但实际部署时需辅以验证机制保障安全可靠。

Abstract: In today's complex industrial environments, operators must often navigate through extensive technical manuals to identify troubleshooting procedures that may help react to some observed failure symptoms. These manuals, written in natural language, describe many steps in detail. Unfortunately, the number, magnitude, and articulation of these descriptions can significantly slow down and complicate the retrieval of the correct procedure during critical incidents. Interestingly, Retrieval Augmented Generation (RAG) enables the development of tools based on conversational interfaces that can assist operators in their retrieval tasks, improving their capability to respond to incidents. This paper presents the results of a set of experiments that derive from the analysis of the troubleshooting procedures available in Fincantieri, a large international company developing complex naval cyber-physical systems. Results show that RAG can assist operators in reacting promptly to failure symptoms, although specific measures have to be taken into consideration to cross-validate recommendations before actuating them.

</details>


### [18] [Revisiting "Revisiting Neuron Coverage for DNN Testing: A Layer-Wise and Distribution-Aware Criterion": A Critical Review and Implications on DNN Coverage Testing](https://arxiv.org/abs/2601.08729)
*Jinhan Kim,Nargiz Humbatova,Gunel Jahangirova,Shin Yoo,Paolo Tonella*

Main category: cs.SE

TL;DR: 本文对NLC这一DNN覆盖标准进行了批判性回顾，指出了其理论与实证假设的不足，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 质疑NLC在满足设计要求和实证表现背后的理论与经验假设是否充分合理。

Method: 通过分析NLC偏离覆盖准则核心原则的现象，并验证其实证研究中的有效性威胁，提出改进方案。

Result: 证实了NLC在单调性、测试套件顺序独立性及协方差矩阵特性方面的不足，并提出了未来度量改进方向。

Conclusion: 这些发现为未来DNN覆盖指标的设计提供了重要启示。

Abstract: We present a critical review of Neural Coverage (NLC), a state-of-the-art DNN coverage criterion by Yuan et al. at ICSE 2023. While NLC proposes to satisfy eight design requirements and demonstrates strong empirical performance, we question some of their theoretical and empirical assumptions. We observe that NLC deviates from core principles of coverage criteria, such as monotonicity and test suite order independence, and could more fully account for key properties of the covariance matrix. Additionally, we note threats to the validity of the empirical study, related to the ground truth ordering of test suites. Through our empirical validation, we substantiate our claims and propose improvements for future DNN coverage metrics. Finally, we conclude by discussing the implications of these insights.

</details>


### [19] [TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback](https://arxiv.org/abs/2601.08734)
*Prithwish Jana,Sam Davidson,Bhavana Bhasker,Andrey Kan,Anoop Deoras,Laurent Callot*

Main category: cs.SE

TL;DR: TerraFormer是一个结合监督微调与验证器引导强化学习的神经符号框架，用于提升基础设施即代码（IaC）生成与变异的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在从自然语言生成IaC时经常产生错误配置，亟需更可靠的方法。

Method: 采用形式化验证工具提供语法、可部署性及策略合规性反馈，并构建高质量数据集TF-Gen和TF-Mutn支持训练与评估。

Result: 在多个基准测试中显著超越基线模型及更大规模LLM，尤其在安全合规性和最佳实践方面表现优异。

Conclusion: TerraFormer有效提升了IaC自动化生成的正确率与可靠性，具备实际部署价值。

Abstract: Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.

</details>


### [20] [Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs](https://arxiv.org/abs/2601.08773)
*Manideep Reddy Chinthareddy*

Main category: cs.SE

TL;DR: 本文比较了三种检索增强生成方法在Java代码库中的表现，发现基于AST的确定性知识图谱方法（DKB）在成本、覆盖率和多跳推理准确性上优于LLM生成图谱和纯向量检索。


<details>
  <summary>Details</summary>
Motivation: 解决现有向量检索在软件工程中难以处理多跳架构推理的问题，如控制器到服务再到仓库的链式调用、接口驱动连接和继承关系。

Method: 构建并对比三种检索管道：纯向量检索（No-Graph）、LLM生成知识图谱（LLM-KB）和基于AST的确定性知识图谱（DKB），在多个Java项目上评估索引时间、查询延迟、覆盖率、成本和答案正确性。

Result: DKB索引速度快、成本低、覆盖率高，在Shopizer数据集上正确率最高；LLM-KB接近但成本高、索引不完整；纯向量检索在架构类问题上表现最差且幻觉风险最高。

Conclusion: 相比LLM提取的知识图谱，基于AST的确定性图谱能以更低成本提供更可靠的多跳推理能力和语料覆盖，更适合软件工程场景。

Abstract: Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal.
  Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.

</details>


### [21] [APEX-SWE](https://arxiv.org/abs/2601.08806)
*Abhi Kottamasu,Akul Datta,Aakash Barthwal,Chirag Mahapatra,Ajay Arun,Adarsh Hiremath,Brendan Foody,Bertie Vidgen*

Main category: cs.SE

TL;DR: 本文提出了APEX-SWE基准，用于评估前沿AI模型在真实软件工程任务中的经济价值表现，包含集成与可观测性两类任务，并发现Gemini 3 Pro表现最佳，关键能力为认知推理与不确定性处理。


<details>
  <summary>Details</summary>
Motivation: 现有评估多聚焦狭窄明确任务，缺乏对真实复杂软件工程工作的衡量，故提出更贴近实际生产场景的评估基准。

Method: 构建包含100个集成任务与100个可观测性任务的基准，评估8个前沿模型，以Pass@1为指标，并分析性能驱动因素。

Result: Gemini 3 Pro（高思考模式）表现最优，得分为25%，其优势源于认知推理能力与主动解决不确定性的行为。

Conclusion: 真实软件工程任务需模型具备区分假设与事实、主动消解不确定性的能力，未来模型设计应强化此类认知与执行能力。

Abstract: We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [22] [A New Tool to Find Lightweight (And, Xor) Implementations of Quadratic Vectorial Boolean Functions up to Dimension 9](https://arxiv.org/abs/2601.08368)
*Marie Bolzer,Sébastien Duval,Marine Minier*

Main category: cs.AR

TL;DR: 提出了一种新工具，用于在AND深度为1的情况下高效实现最多9位的二次函数，并最小化AND门数量。


<details>
  <summary>Details</summary>
Motivation: 现有工具受限于计算时间，仅能处理5位或6位以下的小函数，无法满足更大规模需求。

Method: 开发了一种时间效率更高的专用工具，优化AND门数量并支持最多9位的二次函数实现。

Result: 该工具相比以往方法更高效，可处理6位及以下函数，并扩展至9位规模。

Conclusion: 新工具突破了小函数实现的位数限制，显著提升了计算效率和适用范围。

Abstract: The problem of finding a minimal circuit to implement a given function is one of the oldest in electronics. It is known to be NP-hard. Still, many tools exist to find sub-optimal circuits to implement a function. In electronics, such tools are known as synthesisers. However, these synthesisers aim to implement very large functions (a whole electronic chip). In cryptography, the focus is on small functions, hence the necessity for new dedicated tools for small functions. Several tools exist to implement small functions. They differ by their algorithmic approach (some are based on Depth-First-Search as introduced by Ullrich in 2011, some are based on SAT-solvers like the tool desgined by Stoffelen in 2016, some non-generic tools use subfield decomposition) and by their optimisation criteria (some optimise for circuit size, others for circuit depth, and some for side-channel-protected implementations). However, these tools are limited to functions operating on less than 5 bits, sometimes 6 bits for quadratic functions, or to very simple functions. The limitation lies in a high computing time. We propose a new tool (The tool is provided alongside the IEEE article with CodeOcean and at https://github.com/seduval/implem-quad-sbox) to implement quadratic functions up to 9 bits within AND-depth 1, minimising the number of AND gates. This tool is more time-efficient than previous ones, allowing to explore larger implementations than others on 6 bits or less and allows to reach larger sizes, up to 9 bits.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [23] [Emergent Coordination in Multi-Agent Systems via Pressure Fields and Temporal Decay](https://arxiv.org/abs/2601.08129)
*Roland Rodriguez*

Main category: cs.MA

TL;DR: 提出一种受自然协调机制启发的多智能体框架，通过共享工件上的压力梯度实现隐式协调，在拉丁方约束满足任务中与显式分层控制效果相当且显著优于对话式方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类组织结构的显式协调模式存在随智能体数量和任务复杂度增长而恶化的协调开销问题。

Method: 智能体在共享工件上依据质量信号导出的压力梯度局部操作，并引入时间衰减防止过早收敛，形式化为压力景观优化并证明其收敛性。

Result: 在1078次拉丁方实验中，压力场方法求解率38.2%，与分层控制38.8%统计等价；显著优于顺序、随机及对话方法；时间衰减关键，禁用后压力上升49倍；2至32智能体性能稳定。

Conclusion: 通过共享压力梯度的隐式协调可达到与显式分层控制同等效能，同时大幅超越对话式协调，表明约束驱动的涌现机制是构建多智能体AI更简洁有效的基础。

Abstract: Current multi-agent LLM frameworks rely on explicit orchestration patterns borrowed from human organizational structures: planners delegate to executors, managers coordinate workers, and hierarchical control flow governs agent interactions. These approaches suffer from coordination overhead that scales poorly with agent count and task complexity. We propose a fundamentally different paradigm inspired by natural coordination mechanisms: agents operate locally on a shared artifact, guided only by pressure gradients derived from measurable quality signals, with temporal decay preventing premature convergence. We formalize this as optimization over a pressure landscape and prove convergence guarantees under mild conditions.
  Empirically, on Latin Square constraint satisfaction across 1,078 trials, pressure-field coordination matches hierarchical control (38.2% vs 38.8% aggregate solve rate, p=0.94, indicating statistical equivalence). Both significantly outperform sequential (23.3%), random (11.7%), and conversation-based multi-agent dialogue (8.6%, p<0.00001). Temporal decay is essential: disabling it increases final pressure 49-fold (d=4.15). On easy problems, pressure-field achieves 87% solve rate. The approach maintains consistent performance from 2 to 32 agents. Our key finding: implicit coordination through shared pressure gradients achieves parity with explicit hierarchical control while dramatically outperforming explicit dialogue-based coordination. This suggests that constraint-driven emergence offers a simpler, equally effective foundation for multi-agent AI.

</details>


### [24] [Agent Contracts: A Formal Framework for Resource-Bounded Autonomous AI Systems](https://arxiv.org/abs/2601.08815)
*Qing Ye,Jing Tan*

Main category: cs.MA

TL;DR: 提出了一种名为Agent Contracts的形式化框架，用于在多智能体系统中实现资源受限的执行与协调。


<details>
  <summary>Details</summary>
Motivation: 现有智能体协议缺乏对资源消耗和运行时长的形式化约束机制。

Method: 通过扩展合同隐喻，整合输入/输出规范、多维资源约束、时间边界和成功标准，建立具有明确生命周期语义的治理机制，并支持通过合同委托实现分层协调。

Result: 实验证明该框架在迭代工作流中减少90%令牌使用且方差降低525倍，多智能体委托中零违反守恒定律，并可通过合同模式衡量质量-资源权衡。

Conclusion: Agent Contracts为可预测、可审计和资源受限的自主AI部署提供了形式化基础。

Abstract: The Contract Net Protocol (1980) introduced coordination through contracts in multi-agent systems. Modern agent protocols standardize connectivity and interoperability; yet, none provide formal, resource governance-normative mechanisms to bound how much agents may consume or how long they may operate. We introduce Agent Contracts, a formal framework that extends the contract metaphor from task allocation to resource-bounded execution. An Agent Contract unifies input/output specifications, multi-dimensional resource constraints, temporal boundaries, and success criteria into a coherent governance mechanism with explicit lifecycle semantics. For multi-agent coordination, we establish conservation laws ensuring delegated budgets respect parent constraints, enabling hierarchical coordination through contract delegation. Empirical validation across four experiments demonstrates 90% token reduction with 525x lower variance in iterative workflows, zero conservation violations in multi-agent delegation, and measurable quality-resource tradeoffs through contract modes. Agent Contracts provide formal foundations for predictable, auditable, and resource-bounded autonomous AI deployment.

</details>
